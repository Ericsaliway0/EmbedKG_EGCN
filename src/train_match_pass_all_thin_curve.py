import os
import math
import csv
from collections import Counter
import time
from pathlib import Path
from collections import defaultdict
from itertools import combinations
from tqdm import tqdm
import psutil
import numpy as np
import pandas as pd
import scipy.stats
from scipy.stats import entropy, ttest_ind, fisher_exact
from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list
from sklearn.cluster import KMeans, SpectralBiclustering
from sklearn.preprocessing import normalize
from scipy.ndimage import gaussian_filter
#from sklearn_extra.cluster import KMeansConstrained
import umap
from matplotlib import rcParams
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from matplotlib.ticker import MaxNLocator
from sklearn.preprocessing import StandardScaler, minmax_scale
from sklearn.metrics import (
    roc_curve, auc, precision_recall_curve,
    silhouette_score, adjusted_rand_score,
    normalized_mutual_info_score, confusion_matrix
)
from networkx.algorithms.community import greedy_modularity_communities
from matplotlib import ticker
import torch
import torch.nn as nn
import torch.nn.functional as F
from matplotlib.cm import get_cmap
import dgl
import networkx as nx
from dgl.nn import GNNExplainer
from torch_geometric.nn import GCNConv
from .models import EGCN, HGDC, EMOGI, MTGCN, GCN, GAT, GraphSAGE, GIN, ChebNet, ChebNetII, FocalLoss
from src.utils import (
    choose_model, plot_roc_curve, plot_pr_curve, load_graph_data,
    load_oncokb_genes, plot_and_analyze, save_and_plot_results
)
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.lines import Line2D
from matplotlib.patches import Patch, Rectangle
from matplotlib.gridspec import GridSpec
from matplotlib.colors import Normalize, ListedColormap, to_rgba, to_rgb
import matplotlib.cm as cm
import matplotlib.colors as mcolors
from matplotlib_venn import venn2, venn3, venn2_circles, venn3_circles
from venn import venn
import holoviews as hv
hv.extension('bokeh')
from holoviews import opts
from bokeh.io import output_notebook, export_png
from bokeh.plotting import show
from bokeh.io.export import get_screenshot_as_png
import plotly.graph_objects as go
from gprofiler import GProfiler
import glob
from pathlib import Path   
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse.csgraph import laplacian
from scipy.linalg import eigh
import numpy as np  # required for -log10
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from pathlib import Path
from itertools import combinations
from collections import defaultdict
from matplotlib.colors import LinearSegmentedColormap
from sklearn.metrics.pairwise import rbf_kernel
import random
from matplotlib.ticker import FormatStrFormatter
from matplotlib.ticker import ScalarFormatter
from sklearn.metrics import mean_squared_error
from matplotlib import gridspec


 
CLUSTER_COLORS = {
    0: '#0077B6',  1: '#0000FF',  2: '#00B4D8',  3: '#48EAC4',
    4: '#F1C0E8',  5: '#B9FBC0',  6: '#32CD32',  7: '#bee1e6',
    8: '#8A2BE2',  9: '#E377C2'
}
'''CLUSTER_COLORS = {
    0: '#0077B6',  1: '#0000FF',  2: '#00B4D8',  3: '#48EAC4',
    4: '#F1C0E8',  5: '#B9FBC0',  6: '#32CD32',  7: '#bee1e6',
    8: '#8A2BE2',  9: '#E377C2', 10: '#8EECF5', 11: '#A3C4F3',
    12: '#FFB347', 13: '#FFD700', 14: '#FF69B4', 15: '#CD5C5C',
    16: '#7FFFD4', 17: '#FF7F50'
}'''
CLUSTER_COLORS = {
    0: '#0077B6',   1: '#0000FF',   2: '#00B4D8',   3: '#48EAC4',
    4: '#F1C0E8',   5: '#B9FBC0',   6: '#32CD32',   7: '#bee1e6',
    8: '#8A2BE2',   9: '#E377C2',  10: '#8EECF5',  11: '#A3C4F3',
    12: '#FFB347', 13: '#FFD700',  14: '#FF69B4',  15: '#CD5C5C',
    16: '#7FFFD4', 17: '#FF7F50',  18: '#C71585',  19: '#20B2AA',
    20: '#6A5ACD', 21: '#40E0D0',  22: '#FF8C00',  23: '#DC143C',
    24: '#9ACD32'
}



CLUSTER_COLORS_OMICS = {
    0: '#D62728',  1: '#1F77B4',  2: '#2CA02C',  3: '#9467BD'
}



def save_confirmed_predictions_to_csv(confirmed_predictions, output_dir, model_type, net_type, num_epochs):
    """
    Save confirmed predicted genes to a CSV file.
    
    Args:
    - confirmed_predictions: List of tuples (gene, score, sources).
    - output_dir: Directory to save the CSV file.
    - model_type, net_type, num_epochs: For naming the output file.
    """
    confirmed_predictions_csv_path = os.path.join(output_dir, f'{model_type}_{net_type}_confirmed_predicted_genes_epo{num_epochs}_2048.csv')
    df_confirmed = pd.DataFrame(confirmed_predictions, columns=["Gene", "Score", "Source"])
    df_confirmed.to_csv(confirmed_predictions_csv_path, index=False)
    print(f"Confirmed predicted genes saved to {confirmed_predictions_csv_path}")

def find_optimal_k(node_features, k_range=(2, 16), plot_path='silhouette_score_plot.png', save_n_clusters_row_path='n_clusters_row.txt'):
    """
    Find the optimal number of clusters k using silhouette score.
    
    Parameters:
        node_features (numpy.ndarray): The features of the nodes (embeddings).
        k_range (tuple): A range (min, max) for the values of k to evaluate.
        plot_path (str): Path to save the silhouette score plot.
        save_n_clusters_row_path (str): Path to save the best k value.
        
    Returns:
        int: The optimal number of clusters based on the highest silhouette score.
    """
    silhouette_scores = []
    K_range = range(k_range[0], k_range[1] + 1)

    # Compute silhouette score for each k in the range
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        row_labels = kmeans.fit_predict(node_features)
        score = silhouette_score(node_features, row_labels)
        silhouette_scores.append(score)
        print(f"Silhouette score for k={k}: {score:.4f}")
    
    # Find the best k (highest silhouette score)
    n_clusters_row = K_range[np.argmax(silhouette_scores)]
    print(f"Optimal number of clusters (k) based on Silhouette Score: {n_clusters_row}")

    # Plot silhouette scores
    plt.plot(K_range, silhouette_scores, marker='o')
    plt.xlabel('Number of clusters (k)')
    plt.ylabel('Silhouette Score')
    plt.title('Silhouette Score for Different k')

    # Save the plot to the specified path
    plt.savefig(plot_path)
    plt.close()  # Close the plot after saving

    # Save the best k value to a text file
    with open(save_n_clusters_row_path, 'w') as f:
        f.write(f"Best k: {n_clusters_row}\n")

    return n_clusters_row

def analyze_sankey_structure(
    source,
    target,
    value,
    label_to_idx,
    node_names,
    cluster_to_genes,
    gene_to_neighbors,
    row_labels,
    name_to_index,
    relevance_scores
):
    # === Build Directed Graph
    G = nx.DiGraph()
    for s, t, v in zip(source, target, value):
        G.add_edge(s, t, weight=v)

    node_id_to_name = {v: k for k, v in label_to_idx.items()}

    # === 1. Degree Centrality of Genes
    gene_nodes = [label_to_idx[g] for g in node_names if g in label_to_idx]
    degree_centrality = nx.degree_centrality(G)
    gene_centrality_scores = {
        node_id_to_name[n]: degree_centrality[n]
        for n in gene_nodes if n in degree_centrality
    }

    # === 2. Gene-to-Cluster Participation Count
    gene_cluster_participation = defaultdict(set)
    for cluster_label, genes in cluster_to_genes.items():
        for gene in genes:
            gene_cluster_participation[gene].add(cluster_label)
    gene_cluster_counts = {gene: len(clusters) for gene, clusters in gene_cluster_participation.items()}

    # === 3. Entropy of Flow Distributions (per Confirmed Cluster)
    # cluster_entropy = {}
    # for cluster_label, genes in cluster_to_genes.items():
    #     scores_arr = np.array([relevance_scores[name_to_index[g]] for g in genes])
    #     probs = scores_arr / scores_arr.sum() if scores_arr.sum() > 0 else np.ones_like(scores_arr) / len(scores_arr)
    #     cluster_entropy[cluster_label] = entropy(probs)

    # === 3. Entropy of Flow Distributions (per Confirmed Cluster)
    cluster_entropy = {}
    for cluster_label, genes in cluster_to_genes.items():
        scores_arr = np.array([np.linalg.norm(relevance_scores[name_to_index[g]]) for g in genes])  # ensure 1D
        if scores_arr.sum() > 0:
            probs = scores_arr / scores_arr.sum()
        else:
            probs = np.ones_like(scores_arr) / len(scores_arr)
        cluster_entropy[cluster_label] = float(entropy(probs))  # ensure scalar

    # === 4. Jaccard Similarity Between Clusters Based on Shared Downstream Clusters
    cluster_to_downstream = defaultdict(set)
    for gene, neighbors in gene_to_neighbors.items():
        if gene not in name_to_index:
            continue
        gene_idx = name_to_index[gene]
        cluster = f"Confirmed Cluster {row_labels[gene_idx]}"
        for neighbor_idx in neighbors:
            neighbor_cluster = f"Cluster {row_labels[neighbor_idx]}"
            cluster_to_downstream[cluster].add(neighbor_cluster)

    cluster_jaccard = {}
    for c1, c2 in combinations(cluster_to_downstream.keys(), 2):
        s1 = cluster_to_downstream[c1]
        s2 = cluster_to_downstream[c2]
        intersection = len(s1 & s2)
        union = len(s1 | s2)
        if union > 0:
            cluster_jaccard[(c1, c2)] = intersection / union

    return {
        "gene_degree_centrality": gene_centrality_scores,
        "gene_cluster_counts": gene_cluster_counts,
        "cluster_entropy": cluster_entropy,
        "cluster_jaccard": cluster_jaccard,  # <- changed from cluster_jaccard_similarity
        "cluster_participation": gene_cluster_counts
    }

def save_metrics_to_csv(metrics_dict, output_dir='sankey_metrics'):
    os.makedirs(output_dir, exist_ok=True)

    # 1. Degree Centrality
    pd.DataFrame.from_dict(metrics_dict['gene_degree_centrality'], orient='index', columns=['degree_centrality'])\
        .sort_values('degree_centrality', ascending=False)\
        .to_csv(f"{output_dir}/gene_degree_centrality.csv")

    # 2. Cluster Participation Count
    pd.DataFrame.from_dict(metrics_dict['gene_cluster_counts'], orient='index', columns=['cluster_count'])\
        .sort_values('cluster_count', ascending=False)\
        .to_csv(f"{output_dir}/gene_cluster_counts.csv")

    # 3. Entropy per Cluster
    pd.DataFrame.from_dict(metrics_dict['cluster_entropy'], orient='index', columns=['entropy'])\
        .sort_values('entropy', ascending=False)\
        .to_csv(f"{output_dir}/cluster_entropy.csv")

    # 4. Jaccard Similarity between Clusters
    # Jaccard Similarity between Clusters
    jaccard_df = pd.DataFrame([
        {'Cluster 1': k[0], 'Cluster 2': k[1], 'Jaccard Similarity': v}
        for k, v in metrics_dict['cluster_jaccard'].items()
    ])

    jaccard_df.sort_values(by='Jaccard Similarity', ascending=False)\
        .to_csv(f"{output_dir}/cluster_jaccard_similarity.csv", index=False)

def save_predictions_to_csv(predicted_genes, output_dir, model_type, net_type, num_epochs):
    """
    Save the predicted genes with their sources to a CSV file.
    
    Args:
    - predicted_genes: List of tuples (gene, score, sources) to save.
    - output_dir: Directory to save the CSV file.
    - model_type, net_type, num_epochs: For naming the output file.
    """
    os.makedirs(output_dir, exist_ok=True)
    predicted_genes_csv_path = os.path.join(output_dir, f'{model_type}_{net_type}_predicted_driver_genes_epo{num_epochs}_2048.csv')
    df_predictions = pd.DataFrame(predicted_genes, columns=["Gene", "Score", "Confirmed Sources"])
    df_predictions.to_csv(predicted_genes_csv_path, index=False)
    print(f"Predicted driver genes with confirmed sources saved to {predicted_genes_csv_path}")

def save_predicted_known_drivers(predicted_driver_genes, output_dir, model_type, net_type, num_epochs):
    """
    Save predicted known cancer driver genes to a CSV file.
    
    Args:
    - predicted_driver_genes: List of predicted cancer driver genes.
    - output_dir: Directory to save the CSV file.
    - model_type, net_type, num_epochs: For naming the output file.
    """
    predicted_drivers_csv_path = os.path.join(output_dir, f'{model_type}_{net_type}_predicted_known_drivers_epo{num_epochs}_2048.csv')
    df = pd.DataFrame(predicted_driver_genes, columns=["Gene"])
    df.to_csv(predicted_drivers_csv_path, index=False)
    print(f"Predicted known driver genes saved to {predicted_drivers_csv_path}")
        
def visualize_feature_relevance_heatmaps(relevance_df, clusters, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    # Merge relevance with cluster labels
    merged = relevance_df.copy()
    merged['gene'] = merged.index
    merged = pd.merge(merged, clusters, on='gene')

    for cluster_type in clusters.columns[1:]:
        for view in ['cancer', 'omics']:
            cluster_groups = merged.groupby(cluster_type)
            all_cluster_heatmaps = []
            fig, axes = plt.subplots(len(cluster_groups), 1, figsize=(12, 4 * len(cluster_groups)))
            if len(cluster_groups) == 1:
                axes = [axes]

            for ax, (label, group) in zip(axes, cluster_groups):
                data = group.drop(columns=['gene'] + list(clusters.columns[1:]))

                if view == 'cancer':
                    # Collapse omics features per cancer (max-over-omics per cancer)
                    cancer_names = list(set([col.split('_')[0] for col in data.columns]))
                    cancer_view = pd.DataFrame(index=data.index, columns=cancer_names)
                    for ct in cancer_names:
                        cols = [col for col in data.columns if col.startswith(ct + '_')]
                        cancer_view[ct] = data[cols].max(axis=1)
                    plot_data = cancer_view
                    title = f"{cluster_type.capitalize()} cluster {label} — Cancer view"
                    fname = f"{cluster_type.capitalize()}_cluster_{label}_cancer_heatmap.png"

                else:
                    # Collapse across cancer types for each omics type (max-over-cancers per omics)
                    omics_types = list(set([col.split('_')[-1] for col in data.columns]))
                    omics_view = pd.DataFrame(index=data.index, columns=omics_types)
                    for om in omics_types:
                        cols = [col for col in data.columns if col.endswith('_' + om)]
                        omics_view[om] = data[cols].max(axis=1)
                    plot_data = omics_view
                    title = f"{cluster_type.capitalize()} cluster {label} — Omics view"
                    fname = f"{cluster_type.capitalize()}_cluster_{label}_omics_heatmap.png"

                # Normalize per row for better heatmap contrast
                plot_data = pd.DataFrame(StandardScaler().fit_transform(plot_data.T).T,
                                         index=plot_data.index, columns=plot_data.columns)
                sns.heatmap(plot_data, cmap='viridis', ax=ax, cbar=True)
                ax.set_title(title)
                ax.set_xlabel('Features')
                ax.set_ylabel('Genes')

                ##fig.tight_layout()
                fig.subplots_adjust(hspace=0.0, top=0.98, bottom=0.01)

                fig.savefig(os.path.join(output_dir, fname))
                plt.close(fig)

def cluster_and_visualize_predicted_genes(graph, predicted_cancer_genes, node_names, 
                                          output_path_genes_clusters, num_clusters=12):
    """
    Clusters gene embeddings into groups using KMeans, visualizes them with t-SNE, 
    and marks predicted cancer genes with red circles (half the size of non-cancer dots).

    Returns:
        row_labels (np.ndarray): Cluster assignments for each gene.
        total_genes_per_cluster (dict): Total number of genes per cluster.
        pred_counts (dict): Number of predicted cancer genes per cluster.
    """
    # Extract embeddings
    embeddings = graph.ndata['feat'].cpu().numpy()

    # Run KMeans clustering
    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=12)
    row_labels = kmeans.fit_predict(embeddings)

    # Store cluster labels in graph
    graph.ndata['cluster'] = torch.tensor(row_labels, dtype=torch.long, device=graph.device)

    # Calculate the total number of genes per cluster
    total_genes_per_cluster = {i: np.sum(row_labels == i) for i in range(num_clusters)}

    # Calculate the number of predicted cancer genes per cluster
    pred_counts = {i: 0 for i in range(num_clusters)}
    '''for gene_idx in predicted_cancer_genes:
        print(f"gene_idx type: {type(gene_idx)}, value: {gene_idx}")
        print(f"row_labels type: {type(row_labels)}, length: {len(row_labels)}")

        cluster_id = row_labels[gene_idx]
        pred_counts[cluster_id] += 1'''

    # Convert gene names to their corresponding indices
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    predicted_cancer_gene_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]

    # Ensure valid indices before using them
    for gene_idx in predicted_cancer_gene_indices:
        if 0 <= gene_idx < len(row_labels):  
            cluster_id = row_labels[gene_idx]
            pred_counts[cluster_id] += 1
        else:
            print(f"Skipping invalid index: {gene_idx}")


    # Reduce dimensions with t-SNE for visualization
    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    reduced_embeddings = tsne.fit_transform(embeddings)

    plt.figure(figsize=(12, 10))

    # Plot clusters (Non-cancer genes)
    non_cancer_dot_size = 100  # Default dot size
    red_circle_size = non_cancer_dot_size / 2  # Half the size

    for cluster_id in range(num_clusters):
        idx = np.where(row_labels == cluster_id)[0]  # Get indices of this cluster
        plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1], 
                    color=CLUSTER_COLORS[cluster_id], 
                    edgecolor='k', s=non_cancer_dot_size, alpha=0.8)

    # Mark predicted cancer genes with red circles (⚪, half the size)
    for gene_idx in predicted_cancer_gene_indices:
        x, y = reduced_embeddings[gene_idx]
        plt.scatter(x, y, facecolors='none', edgecolors='red', s=red_circle_size, linewidths=2)


    # Labels and title
    ##plt.title("Gene clustering with predicted cancer genes")
    plt.xlabel("t-SNE Dimension 1", fontsize=18)
    plt.ylabel("t-SNE Dimension 2", fontsize=18)

    # Save plot
    plt.savefig(output_path_genes_clusters, bbox_inches="tight")  # Ensure proper cropping
    plt.close()

    print(f"Cluster visualization saved to {output_path_genes_clusters}")

    return row_labels, total_genes_per_cluster, pred_counts

def compute_lrp_scores(model, graph, features, node_indices=None):
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features.requires_grad_(True)


    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        # Select the nodes to analyze (e.g., predicted cancer genes)
        if node_indices is None:
            node_indices = torch.nonzero((probs > 0.0)).squeeze()

        relevance_scores = torch.zeros_like(features)

        for idx in node_indices:
            model.zero_grad()
            ##probs[idx].backward(retain_graph=True)
            probs[idx].backward(retain_graph=(idx != node_indices[-1]))

            relevance_scores[idx] = features.grad[idx].detach()

    return relevance_scores

def load_gene_set(file_path):
    """
    Load a gene list from a file and return as a set.
    
    Args:
    - file_path: Path to the file containing genes, one per line.
    
    Returns:
    - Set of gene names.
    """
    with open(file_path, 'r') as f:
        return set(line.strip() for line in f)

def save_row_labels(row_labels, save_path):
    ##row_labels_path = os.path.join(os.path.dirname(save_path), "row_labels.npy")
    np.save(save_path, row_labels)
    print(f"Cluster labels saved to {save_path}")

# Save total genes per cluster
def save_total_genes_per_cluster(total_genes_per_cluster, save_path):
    ##total_genes_path = os.path.join(os.path.dirname(save_path), "total_genes_per_cluster.npy")
    np.save(save_path, total_genes_per_cluster)
    print(f"Total genes per cluster saved to {save_path}")

# Save predicted cancer genes per cluster
def save_predicted_counts(pred_counts, save_path):
    ##pred_counts_path = os.path.join(os.path.dirname(save_path), "predicted_counts.npy")
    np.save(save_path, pred_counts)
    print(f"Predicted cancer genes per cluster saved to {save_path}")

'''# Function to reload the saved graph data
def reload_spectral_biclustering_data(save_path):
    print(f"Loading graph data from {save_path}...")
    
    # Load the saved data
    checkpoint = torch.load(save_path)

    # Extract graph-related data
    edges = checkpoint['edges']
    features = checkpoint['features']
    labels = checkpoint['labels']
    row_labels = checkpoint['cluster']

    print("Graph data loaded successfully.")

    return edges, features, labels, row_labels
'''
# Function to reload cluster labels

def reload_row_labels(save_path):
    ##row_labels_path = os.path.join(os.path.dirname(save_path), "row_labels.npy")
    row_labels = np.load(save_path)
    print(f"Cluster labels loaded from {save_path}")
    return row_labels

# Function to reload total genes per cluster
def reload_total_genes_per_cluster(save_path):
    ##total_genes_path = os.path.join(os.path.dirname(save_path), "total_genes_per_cluster.npy")
    total_genes_per_cluster = np.load(save_path, allow_pickle=True).item()
    print(f"Total genes per cluster loaded from {save_path}")
    return total_genes_per_cluster

# Function to reload predicted cancer genes per cluster
def reload_predicted_counts(save_path):
    ##pred_counts_path = os.path.join(os.path.dirname(save_path), "predicted_counts.npy")
    pred_counts = np.load(save_path, allow_pickle=True).item()
    print(f"Predicted cancer genes per cluster loaded from {save_path}")
    return pred_counts

def load_bioclustered_graph(save_path):
    """
    Loads a previously saved DGL graph that includes features, labels, and cluster assignments.

    Args:
        save_path (str): Path to the saved graph file (.pth)

    Returns:
        dgl.DGLGraph: The reconstructed graph with restored node data.
    """
    data = torch.load(save_path)

    graph = dgl.graph(data['edges'])
    graph.ndata['feat'] = data['features']

    if data.get('label') is not None:
        graph.ndata['label'] = data['label']

    if data.get('cluster') is not None:
        graph.ndata['cluster'] = data['cluster']

    if 'degree' in data:
        graph.ndata['degree'] = data['degree']
    if 'train_mask' in data:
        graph.ndata['train_mask'] = data['train_mask']
    if 'test_mask' in data:
        graph.ndata['test_mask'] = data['test_mask']

    print("✅ Clustered graph loaded successfully.")
    return graph

def save_bioclustered_gene_info_csv(
    row_labels,
    total_genes_per_cluster,
    pred_counts,
    node_names,
    predicted_gene_indices,
    output_csv_path
):
    """
    Saves clustered gene information to a CSV file.

    Args:
        row_labels (np.ndarray): Cluster assignments.
        total_genes_per_cluster (dict): Total genes in each cluster.
        pred_counts (dict): Number of predicted cancer genes per cluster.
        node_names (list): List of all gene names by index.
        predicted_gene_indices (list): Indices of predicted cancer genes.
        output_csv_path (str): Path to save the CSV file.
    """
    gene_data = []
    for idx, name in enumerate(node_names):
        cluster = row_labels[idx]
        is_predicted = 1 if idx in predicted_gene_indices else 0
        gene_data.append({
            "Gene": name,
            "Cluster": cluster,
            "IsPredictedCancerGene": is_predicted,
            "TotalGenesInCluster": total_genes_per_cluster[cluster],
            "PredictedInCluster": pred_counts[cluster]
        })

    df = pd.DataFrame(gene_data)
    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)
    df.to_csv(output_csv_path, index=False)
    print(f"Clustered gene information saved to {output_csv_path}")

def save_reduced_feature_relevance_scores(
    relevance_scores,      # Original 2048D matrix, shape [N, 2048]
    gene_names,            # List of gene symbols, length N
    output_csv_path        # File to save the reduced 64D matrix
):
    """
    Reduce 2048D LRP scores to 64D using max-over-16 logic,
    and save to CSV with columns like 'BRCA_mf', ..., 'KIRP_meth'.
    """
    import numpy as np
    import pandas as pd
    import os

    # -- Step 1: Reduce features to 64D
    reduced_scores = extract_summary_features_np_skip(relevance_scores)  # shape [N, 64]

    # -- Step 2: Define feature names in cancer x omics order
    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]
    omics_types = ['cna', 'ge', 'meth', 'mf']
    column_labels = [f"{cancer}_{omics}" for omics in omics_types for cancer in cancer_names]

    # -- Step 3: Build DataFrame and save
    df = pd.DataFrame(reduced_scores, index=gene_names, columns=column_labels)
    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)
    df.to_csv(output_csv_path)
    print(f"✅ Saved reduced feature relevance matrix to {output_csv_path}")

def plot_top_predicted_genes_tsne(graph, node_names, scores, output_path, top_k=1000):
    cluster_ids = graph.ndata['cluster_bio'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores##.cpu().numpy()

    # Get top K predicted genes
    top_indices = np.argsort(scores)[-top_k:]
    top_embeddings = embeddings[top_indices]
    top_clusters = cluster_ids[top_indices]
    top_scores = scores[top_indices]
    top_names = [node_names[i] for i in top_indices]

    # t-SNE projection
    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(top_embeddings)

    # Plot
    plt.figure(figsize=(10, 8))
    rcParams['pdf.fonttype'] = 42  # prevent font issues in vector graphics

    for c in np.unique(top_clusters):
        mask = top_clusters == c
        coords = tsne_coords[mask]
        plt.scatter(
            coords[:, 0], coords[:, 1],
            color=CLUSTER_COLORS.get(c, "#555555"),
            edgecolors='k',
            s=60,
            alpha=0.8
        )

        # Top 1 in this cluster (within top_k)
        cluster_scores = top_scores[mask]
        if cluster_scores.size > 0:
            top_idx_in_cluster = np.argmax(cluster_scores)
            name = np.array(top_names)[mask][top_idx_in_cluster]
            x, y = coords[top_idx_in_cluster]
            # Highlight the top 1 with a yellow circle (half the size of the original dot)
            plt.scatter(x, y, s=60, edgecolors='yellow', alpha=0.6, linewidth=1, marker='o', color='red')
            # Change label text color to red
            plt.text(x, y, name, fontsize=9, fontweight='bold', ha='center', va='center', color='black')

    plt.title("t-SNE of Top 1000 Predicted Genes by Cluster", fontsize=14)
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    
    # Remove legend
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"✅ Top predicted gene t-SNE plot saved to:\n{output_path}")

def plot_tsne_predicted_genes(graph, node_names, scores, output_path, args):
    cluster_ids = graph.ndata['cluster_bio'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores##.cpu().numpy()

    predicted_mask = scores >= args.score_threshold
    predicted_indices = np.where(predicted_mask)[0]
    predicted_scores = scores[predicted_indices]
    predicted_clusters = cluster_ids[predicted_indices]
    predicted_embeddings = embeddings[predicted_indices]

    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(predicted_embeddings)

    # Gather top 2 genes per cluster
    top_genes = []
    for c in np.unique(predicted_clusters):
        cluster_mask = predicted_clusters == c
        cluster_indices = np.where(cluster_mask)[0]
        if len(cluster_indices) == 0:
            continue
        top_indices = cluster_indices[np.argsort(predicted_scores[cluster_indices])[-2:]]  # top 2
        for idx in top_indices:
            top_genes.append((predicted_indices[idx], tsne_coords[idx], c))

    # Plot
    plt.figure(figsize=(10, 7))
    for idx, (node_idx, coord, cluster_id) in enumerate(top_genes):
        color = CLUSTER_COLORS.get(cluster_id, "#333333")
        plt.scatter(coord[0], coord[1], color=color, s=120, edgecolor='k')
        plt.text(coord[0]+1.5, coord[1], node_names[node_idx], fontsize=9, color=color)

    # Legend
    unique_predicted_clusters = np.unique(predicted_clusters)
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=f"Cluster {c}", markersize=10)
        for c, color in CLUSTER_COLORS.items()
        if c in unique_predicted_clusters
    ]
    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.title("t-SNE of Top 2 Predicted Genes per Cluster")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

def plot_feature_importance(
    relevance_vector,
    feature_names=None,
    node_name=None,
    top_k=9,
    gene_names=None,
    output_path="plots/feature_importance.png"):
    # Convert to NumPy array
    if isinstance(relevance_vector, torch.Tensor):
        relevance_vector = relevance_vector.detach().cpu().numpy()
    else:
        relevance_vector = np.array(relevance_vector)

    # --- 🔄 Min-Max Normalization ---
    min_val = relevance_vector.min()
    max_val = relevance_vector.max()
    norm_scores = (relevance_vector - min_val) / (max_val - min_val + 1e-8)

    # --- 🔝 Top-K selection ---
    top_indices = np.argsort(norm_scores)[-top_k:]
    top_scores = norm_scores[top_indices]

    if feature_names is None:
        feature_names = [f"Feature {i}" for i in range(len(relevance_vector))]

    # --- 🧬 Labeling ---
    if gene_names is not None:
        top_labels = [gene_names[i].capitalize() if i < len(gene_names) else f"Unknown {i}" for i in top_indices]
    else:
        top_labels = [feature_names[i] for i in top_indices]

    # Construct DataFrame
    df = pd.DataFrame({
        "feature": top_labels,
        "relevance": top_scores
    })

    # --- 📊 Plot ---
    plt.figure(figsize=(2.5, 2.5))
    sns.set_style("white")
    sns.barplot(
        data=df,
        x="feature",
        y="relevance",
        palette="Blues_d",
        dodge=False,
        legend=False,
        width=0.6
    )

    plt.ylabel("Relevance score", fontsize=12)
    plt.xlabel("", fontsize=12)
    plt.title(f"{node_name}", fontsize=13)

    plt.xticks(rotation=90, ha='center', fontsize=11)
    plt.yticks(fontsize=11)

    sns.despine()
    plt.tight_layout()

    # --- 💾 Ensure directory exists and save ---
    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"✅ Saved barplot to {output_path}")
    else:
        plt.close()

def _plot_bar(omics_relevance, omics_colors, omics_order, output_path):
    plt.figure(figsize=(1.4, 2))
    sns.barplot(
        x=omics_relevance.index,
        y=omics_relevance.values,
        palette=[omics_colors[o] for o in omics_order],
        width=0.6
    )

    # Capitalize x-axis labels
    plt.xticks(
        ticks=range(len(omics_relevance.index)),
        labels=[label.upper() for label in omics_relevance.index],
        rotation=90,
        fontsize=9
    )
    plt.yticks(fontsize=10)
    plt.xlabel('', fontsize=10)
    plt.ylabel('', fontsize=10)
    plt.tick_params(axis='both', which='both', length=0)
    sns.despine()

    # Use scientific notation on y-axis
    '''ax = plt.gca()
    ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))
    ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))'''
    ax = plt.gca()

    # Set scientific formatter (plain 'e' format)
    formatter = ScalarFormatter(useMathText=False)
    formatter.set_scientific(True)
    formatter.set_powerlimits((0, 0))
    ax.yaxis.set_major_formatter(formatter)

    # Reduce the font size of the offset (e.g., '1e-3' label)
    ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))
    ax.yaxis.offsetText.set_fontsize(6)
    ax.yaxis.offsetText.set_horizontalalignment('left')
        
    plt.tight_layout()

    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"✅ Saved barplot to {output_path}")
    else:
        plt.close()

def extract_summary_features_np_skip(features_np):
    num_nodes = features_np.shape[0]
    total_dim = features_np.shape[1]
    summary_features = []

    for o_idx in range(4):  # omics
        for c_idx in range(16):  # cancer
            base = o_idx * 16 * 16 + c_idx * 16
            if base + 16 > total_dim:
                continue  # skip invalid slice
            group = features_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)

def compute_integrated_gradients(
    model, graph, features, node_indices=None, baseline=None, steps=50):
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    features = features.clone().detach()

    if baseline is None:
        baseline = torch.zeros_like(features)

    assert baseline.shape == features.shape, "Baseline must match feature shape"

    # Scale inputs and compute gradients
    scaled_inputs = [
        baseline + (float(i) / steps) * (features - baseline)
        for i in range(1, steps + 1)
    ]
    scaled_inputs = torch.stack(scaled_inputs)  # Shape: (steps, num_nodes, num_features)

    # Integrated gradients trainedization
    integrated_grads = torch.zeros_like(features)

    for step_input in scaled_inputs:
        step_input.requires_grad_(True)
        logits = model(graph, step_input)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        grads = torch.autograd.grad(
            outputs=probs[node_indices],
            inputs=step_input,
            grad_outputs=torch.ones_like(probs[node_indices]),
            retain_graph=True,
            create_graph=False,
        )[0]

        integrated_grads += grads.detach()

    # Average the gradients and scale by the input difference
    avg_grads = integrated_grads / steps
    ig_attributions = (features - baseline) * avg_grads

    return ig_attributions

def get_two_hop_neighbors(graph, node_id):
    one_hop = set(graph.neighbors(node_id)) if node_id in graph else set()
    two_hop = set()
    
    for neighbor in one_hop:
        two_hop.update(graph.neighbors(neighbor))
    
    # Remove the original node and one-hop neighbors from two-hop set
    two_hop.difference_update(one_hop)
    two_hop.discard(node_id)
    
    return sorted(two_hop)

def save_cluster_legend(output_path_legend, cluster_colors, num_clusters=12):
    """
    Creates and saves a separate legend image for cluster colors in a single row.

    Args:
        output_path_legend (str): Path to save the legend image.
        cluster_colors (list): List of colors for each cluster.
        num_clusters (int): Number of clusters.
    """
    fig, ax = plt.subplots(figsize=(12, 1.5))  # Wider and shorter for single-row legend

    # Create legend handles labeled from Cluster 0 to Cluster 11
    legend_patches = [mpatches.Patch(color=cluster_colors[i], label=f"Cluster {i}") 
                      for i in range(num_clusters)]

    # Display legend with one row
    ax.legend(handles=legend_patches, loc='center', ncol=num_clusters,
              frameon=False, fontsize=14)

    # Remove axes
    ax.set_xticks([])
    ax.set_yticks([])
    ax.axis("off")

    # Save legend image
    plt.savefig(output_path_legend, bbox_inches="tight", dpi=300)
    plt.close()

    print(f"Legend saved to {output_path_legend}")

def compute_saliency_all_nodes(model, g, features, target_classes=None):
    """
    Compute saliency (relevance scores) for all nodes using gradients.

    Args:
        model: Trained GNN model
        g: DGL graph
        features: Input node features (torch.Tensor or np.ndarray)
        target_classes: Optional tensor/list of target classes per node. If None, use predicted class.

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features] with saliency values.
    """
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    
    features = features.clone().detach().requires_grad_(True)
    logits = model(g, features)

    num_nodes = features.shape[0]
    relevance_scores = torch.zeros_like(features)

    if target_classes is None:
        target_classes = torch.argmax(logits, dim=1)

    for node_id in range(num_nodes):
        model.zero_grad()
        if features.grad is not None:
            features.grad.zero_()

        score = logits[node_id, target_classes[node_id]]
        score.backward(retain_graph=True)

        relevance_scores[node_id] = features.grad[node_id].abs().detach()

    return relevance_scores

def plot_gene_feature_contributions_topo_(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_topo(df, barplot_path)

    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Omics', columns='Cancer', values='Relevance')
    omics_order = ['cna', 'ge', 'meth', 'mf']
    heatmap_data = heatmap_data.reindex(omics_order)

    plt.figure(figsize=(8, 2.8))
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')
    ##plt.title(f"{gene_name} ({score:.3f})", fontsize=14)
    ##plt.title(f"{gene_name} ({float(score):.3f})", fontsize=14)
    if isinstance(score, np.ndarray):
        score = score.item()

    ##plt.title(f"{gene_name} ({score.item():.3f})", fontsize=14)
    plt.title(f"{gene_name}", fontsize=14)


    plt.yticks(rotation=0)
    plt.xticks(rotation=90, ha='right')
    plt.xlabel('')
    plt.ylabel('')
    
    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_bio_(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_bio(df, barplot_path)

    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Omics', columns='Cancer', values='Relevance')
    omics_order = ['cna', 'ge', 'meth', 'mf']
    heatmap_data = heatmap_data.reindex(omics_order)

    plt.figure(figsize=(8, 2.8))
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')
    ##plt.title(f"{gene_name} ({score:.3f})", fontsize=14)
    ##plt.title(f"{gene_name} ({float(score):.3f})", fontsize=14)
    if isinstance(score, np.ndarray):
        score = score.item()
    ##plt.title(f"{gene_name} ({score.item():.3f})", fontsize=14)
    plt.title(f"{gene_name}", fontsize=14)


    plt.yticks(rotation=0)
    plt.xticks(rotation=90, ha='right')
    plt.xlabel('')
    plt.ylabel('')
    
    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_confirmed_neighbors_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    # scores,
    row_labels,
    total_clusters,
    relevance_scores):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        # gene_score = relevance_scores[node_idx]
        gene_score = relevance_scores[node_idx].sum().item()  # OR .mean().item()

        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()
        print(f"{gene} → Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])

        # Filter only those neighbors present in top-k
        neighbor_scores_dict = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    #if rel_score > 0.1:
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/bio_neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )


def plot_confirmed_neighbors_topo(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    # scores,
    row_labels,
    total_clusters,
    relevance_scores):
    # Only top-k names are passed in node_names
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= relevance_scores.shape[0]:
            print(f"⚠️ Skipping {gene}: index {node_idx} out of bounds for relevance_scores with shape {relevance_scores.shape}")
            continue

        gene_score = relevance_scores[node_idx].sum().item()


        # ✅ Get cluster from graph
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()
        print(f"{gene} → Node {node_idx} | Topo score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores_dict = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:  # bounds check
                    rel_score = relevance_scores[rel_idx].sum().item()
                    # if rel_score > 0.1:
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/topo_neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_topo_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/topo_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_topo_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )


def plot_confirmed_neighbor_relevance(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    scores,
    relevance_scores,
    mode="bio"):  # or "topo"):
    """
    Plots top-10 neighbor relevance scores for confirmed genes.
    Mode can be 'bio' or 'topo'.
    """


    assert mode in ("bio", "topo"), "Mode must be 'bio' or 'topo'"

    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in name_to_index:
            print(f"⚠️ Gene {gene} not found in the graph.")
            continue

        node_idx = name_to_index[gene]
        gene_score = scores[node_idx]
        print(f"{gene} → Node {node_idx} | {mode.capitalize()} score: {gene_score:.4f}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_indices = [name_to_index[n] for n in neighbors if n in name_to_index]

        # Get relevance scores for neighbors (filtering by score > 0.1)
        neighbor_scores_dict = {
            i: relevance_scores[i].sum().item()
            for i in neighbor_indices
            if relevance_scores[i].sum().item() > 0.1
        }

        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors found for {gene}.")
            continue

        # Sort and select top 10
        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_{mode}_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )


def plot_topo_clusterwise_feature_contributions(
    args,
    relevance_scores,           # 2D array (samples x features)
    row_labels,             # 1D array of cluster assignments
    feature_names,              # List of feature names (e.g., TOPO: BRCA, ...)
    per_cluster_feature_contributions_output_dir):  # Output folder
    ##omics_colors                # Dict of omics type colors (e.g., 'topo': '#1F77B4')):
    os.makedirs(per_cluster_feature_contributions_output_dir, exist_ok=True)

    '''def get_omics_color(feature_name):
        prefix = feature_name.split(":")[0].lower()
        return omics_colors.get(prefix, "#AAAAAA")'''

    unique_clusters = np.unique(row_labels)

    for cluster_id in sorted(unique_clusters):
        indices = np.where(row_labels == cluster_id)[0]
        cluster_scores = relevance_scores[indices]
        avg_contribution = np.mean(cluster_scores, axis=0)
        total_score = np.sum(avg_contribution)

        fig, ax = plt.subplots(figsize=(10, 2.5))

        x = np.linspace(0, 1, len(feature_names))
        bar_width = 1 / len(feature_names) * 0.95

        bars = ax.bar(
            x,
            avg_contribution,
            width=bar_width,
            ##color=[get_omics_color(name) for name in feature_names],
            align='center'
        )

        ax.set_title(
            fr"Cluster {cluster_id} $\mathregular{{({len(indices)}\ genes,\ avg = {total_score:.2f})}}$",
            fontsize=14
        )

        clean_labels = [name.split(":")[1].strip() if ":" in name else name for name in feature_names]
        ax.set_xticks(x)
        ax.set_xticklabels(clean_labels, rotation=90)

        '''for label, feature_name in zip(ax.get_xticklabels(), feature_names):
            label.set_color(get_omics_color(feature_name))'''

        ax.tick_params(axis='x', labelsize=9)
        ax.set_xlim(-bar_width, 1 + bar_width)

        plt.tight_layout()
        save_path = os.path.join(
            per_cluster_feature_contributions_output_dir,
            f"{args.model_type}_{args.net_type}_TOPO_cluster_{cluster_id}_feature_contributions_epo{args.num_epochs}.png"
        )
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"✅ Saved TOPO feature contribution barplot for Cluster {cluster_id} to {save_path}")

def plot_feature_importance_topo(
    relevance_vector,
    feature_names,
    node_name=None,
    output_path="plots"):


    if len(relevance_vector) != len(feature_names):
        raise ValueError("Length mismatch between relevance vector and feature names.")

    ##pretty_labels = [f"emb_{i}" for i in range(len(relevance_vector))]
    pretty_labels = [f"{i}" for i in range(len(relevance_vector))]

    df = pd.DataFrame({
        "feature": pretty_labels,
        "relevance": relevance_vector
    })

    plt.figure(figsize=(24, 5))
    sns.set_style("white")
    bars = plt.bar(df["feature"], df["relevance"], color="#607D8B")  # bluish-gray

    # Title (no mean)
    if node_name:
        plt.title(f"{node_name}", fontsize=16)

    # Axis labels and formatting
    plt.xlabel("Topology Embedding Dimension", fontsize=14)
    plt.ylabel("Relevance", fontsize=14)
    plt.xticks(rotation=90, fontsize=12)
    plt.yticks(fontsize=12)
    plt.margins(x=0)

    num_bars = len(df)
    margin = 0.75
    ax = plt.gca()
    ax.set_xlim(-margin, num_bars - 1 + margin)

    ##ax.set_xlim(-bar_width, 1 + bar_width) 

    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved TOPO feature importance plot to {output_path}")

def plot_bio_topo_saliency(bio_scores, topo_scores, title="", save_path=None):
    """
    Plot saliency relevance for bio and topo features with mean values in a color-patched legend.

    Parameters:
        bio_scores (np.ndarray): Relevance scores for bio features (length 1024).
        topo_scores (np.ndarray): Relevance scores for topo features (length 1024).
        title (str): Plot title.
        save_path (str, optional): Path to save the figure.
    """

    # 🔹 Normalize
    bio_scores_norm = (bio_scores - bio_scores.min()) / (bio_scores.max() - bio_scores.min() + 1e-8)
    topo_scores_norm = (topo_scores - topo_scores.min()) / (topo_scores.max() - topo_scores.min() + 1e-8)
    #x = np.arange(0, 1024)
    x = np.arange(0, 64)

    # 🔹 Means
    mean_bio = bio_scores_norm.mean()
    mean_topo = topo_scores_norm.mean()

    # 🔹 Plot
    plt.figure(figsize=(12, 6))
    plt.fill_between(x, bio_scores_norm, alpha=0.4, color="royalblue")
    plt.fill_between(x, topo_scores_norm, alpha=0.4, color="darkorange")

    # 🔹 Mean lines
    plt.axhline(mean_bio, color="royalblue", linestyle="--", linewidth=1.5)
    plt.axhline(mean_topo, color="darkorange", linestyle="--", linewidth=1.5)

    # 🔹 Custom legend with mean values
    legend_handles = [
        Patch(facecolor='royalblue', label=f'Bio Mean: {mean_bio:.2f}'),
        Patch(facecolor='darkorange', label=f'Topo Mean: {mean_topo:.2f}')
    ]
    plt.legend(handles=legend_handles, loc='upper left', frameon=False, fontsize=10)

    # 🔹 Formatting
    #plt.xlim(0, 1023)
    plt.xlim(0, 63)
    plt.ylim(0, 1)
    plt.xlabel("Feature Index (0 - 63)", fontsize=12)
    plt.ylabel("Normalized Relevance Score", fontsize=12)
    plt.title(title, fontsize=14)
    sns.despine()
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"✅ Saved bio-topo saliency plot to {save_path}")
    else:
        plt.close()

def plot_bio_topo_saliency_cuberoot(bio_scores, topo_scores, title="", save_path=None):
    """
    Plot saliency relevance for bio and topo features using cube root transformed normalized values.
    This enhances low scores and compresses high spikes.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.patches import Patch
    import seaborn as sns

    # 🔹 Normalize to [0, 1]
    bio_scores_norm = (bio_scores - bio_scores.min()) / (bio_scores.max() - bio_scores.min() + 1e-8)
    topo_scores_norm = (topo_scores - topo_scores.min()) / (topo_scores.max() - topo_scores.min() + 1e-8)

    # 🔹 Apply cube root transform
    bio_scores_scaled = np.cbrt(bio_scores_norm)
    topo_scores_scaled = np.cbrt(topo_scores_norm)

    x = np.arange(0, len(bio_scores))

    # 🔹 Means (after transform)
    mean_bio = bio_scores_scaled.mean()
    mean_topo = topo_scores_scaled.mean()

    # 🔹 Plot
    plt.figure(figsize=(12, 6))
    plt.fill_between(x, bio_scores_scaled, alpha=0.4, color="royalblue")
    plt.fill_between(x, topo_scores_scaled, alpha=0.4, color="darkorange")

    # 🔹 Mean lines
    plt.axhline(mean_bio, color="royalblue", linestyle="--", linewidth=1.5)
    plt.axhline(mean_topo, color="darkorange", linestyle="--", linewidth=1.5)

    # 🔹 Legend
    legend_handles = [
        Patch(facecolor='royalblue', label=f'Bio Mean: {mean_bio:.2f}'),
        Patch(facecolor='darkorange', label=f'Topo Mean: {mean_topo:.2f}')
    ]
    plt.legend(handles=legend_handles, loc='upper left', frameon=False, fontsize=10)

    # 🔹 Formatting
    plt.xlim(0, len(bio_scores) - 1)
    plt.ylim(0, 1)
    plt.xlabel("Feature Index (0 – 63)", fontsize=12)
    plt.ylabel("Cube Root Transformed Score", fontsize=12)
    plt.title(title, fontsize=14)
    sns.despine()
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"✅ Saved cube root bio-topo saliency plot to {save_path}")
    else:
        plt.close()

def plot_bio_topo_saliency_log(bio_scores, topo_scores, title="", save_path=None):
    """
    Plot saliency relevance for bio and topo features with log-transformed normalized values.
    Enhances small values and compresses high peaks.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.patches import Patch
    import seaborn as sns

    # 🔹 Normalize
    bio_scores_norm = (bio_scores - bio_scores.min()) / (bio_scores.max() - bio_scores.min() + 1e-8)
    topo_scores_norm = (topo_scores - topo_scores.min()) / (topo_scores.max() - topo_scores.min() + 1e-8)

    # 🔹 Apply log transform
    bio_scores_scaled = np.log1p(bio_scores_norm) / np.log1p(1)  # log1p(x)/log1p(1) keeps range in [0,1]
    topo_scores_scaled = np.log1p(topo_scores_norm) / np.log1p(1)

    x = np.arange(0, len(bio_scores))

    # 🔹 Means (after transform)
    mean_bio = bio_scores_scaled.mean()
    mean_topo = topo_scores_scaled.mean()

    # 🔹 Plot
    plt.figure(figsize=(12, 6))
    plt.fill_between(x, bio_scores_scaled, alpha=0.4, color="royalblue")
    plt.fill_between(x, topo_scores_scaled, alpha=0.4, color="darkorange")

    # 🔹 Mean lines
    plt.axhline(mean_bio, color="royalblue", linestyle="--", linewidth=1.5)
    plt.axhline(mean_topo, color="darkorange", linestyle="--", linewidth=1.5)

    # 🔹 Legend
    legend_handles = [
        Patch(facecolor='royalblue', label=f'Bio Mean: {mean_bio:.2f}'),
        Patch(facecolor='darkorange', label=f'Topo Mean: {mean_topo:.2f}')
    ]
    plt.legend(handles=legend_handles, loc='upper right', frameon=False, fontsize=10)

    # 🔹 Formatting
    plt.xlim(0, len(bio_scores) - 1)
    plt.ylim(0, 1)
    plt.xlabel("Feature Index (0–1023)", fontsize=12)
    plt.ylabel("Transformed Relevance Score (log1p)", fontsize=12)
    plt.title(title, fontsize=14)
    sns.despine()
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"✅ Saved log-transformed bio-topo saliency plot to {save_path}")
    else:
        plt.close()

def extract_bio_summary_features_np(features_np):
    num_nodes = features_np.shape[0]
    total_dim = features_np.shape[1]
    summary_features = []

    num_omics = 4
    num_cancers = 16
    features_per_pair = 16

    for o_idx in range(num_omics):
        for c_idx in range(num_cancers):
            base = o_idx * num_cancers * features_per_pair + c_idx * features_per_pair
            if base + features_per_pair > total_dim:
                continue
            group = features_np[:, base:base + features_per_pair]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # ➜ (N, 64)

def extract_topo_summary_features_np(features_np):
    num_nodes = features_np.shape[0]
    total_dim = features_np.shape[1]
    summary_features = []

    num_topo_blocks = 4
    num_cancers = 16
    features_per_block = 16

    for t_idx in range(num_topo_blocks):
        for c_idx in range(num_cancers):
            base = t_idx * num_cancers * features_per_block + c_idx * features_per_block
            if base + features_per_block > total_dim:
                continue
            group = features_np[:, base:base + features_per_block]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # ➜ (N, 64)

def extract_summary_features_np(features_np):
    """
    Extracts summary features by computing the max of 16-dimensional segments across all (omics, cancer) pairs.
    This version only works with the first 1024 biological features.

    Args:
        features_np (np.ndarray): shape [num_nodes, 2048]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = features_np.shape
    summary_features = []

    assert num_features == 2048, f"Expected 2048 features, got {num_features}"

    # First 1024 for biological features (omics and cancer)
    for o_idx in range(4):  # 4 omics types
        for c_idx in range(16):  # 16 cancer types
            base = o_idx * 16 * 16 + c_idx * 16
            group = features_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)  # [num_nodes, 1]
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # shape: [num_nodes, 64]

def plot_neighbor_relevance_by_mode(
    gene,
    relevance_scores,
    mode,
    neighbor_scores,
    neighbors_dict,
    name_to_index,
    node_id_to_name,
    graph,
    row_labels,
    total_clusters,
    args,
    save_dir="results/gene_prediction/neighbor_feature_contributions/"):
    os.makedirs(save_dir, exist_ok=True)

    if gene not in name_to_index:
        print(f"⚠️ Gene {gene} not found in the graph.")
        return

    node_idx = name_to_index[gene]
    gene_score = neighbor_scores[node_idx]

    # ✅ Get gene cluster based on mode
    if mode == "bio":
        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()
    elif mode == "topo":
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()
    else:
        gene_cluster = -1  # fallback if cluster type is unknown

    print(f"[{mode}] {gene} → Node {node_idx} | Predicted score: {gene_score:.4f} | Cluster: {gene_cluster}")

    neighbors = neighbors_dict.get(gene, [])
    neighbor_indices = [name_to_index[n] for n in neighbors if n in name_to_index]

    relevance_vals = [relevance_scores[i].sum().item() for i in neighbor_indices]
    scores_dict = dict(zip(neighbor_indices, relevance_vals))

    output_path = os.path.join(
        save_dir,
        f"{args.model_type}_{args.net_type}_{gene}_{mode}_neighbor_relevance_epo{args.num_epochs}.png"
    )

    plot_neighbor_relevance(
        neighbor_scores=scores_dict,
        gene_name=f"{gene} (Cluster {gene_cluster})",
        node_id_to_name=node_id_to_name,
        output_path=output_path,
        row_labels=row_labels,
        total_clusters=total_clusters,
        add_legend=False
    )

def plot_saliency_for_gene(
    gene,
    relevance_scores,
    node_idx,
    save_dir,
    args,
    bio_feat_names,
    topo_feat_names):
    bio_1024 = relevance_scores[node_idx]["bio"].cpu().numpy().reshape(1, -1)
    topo_1024 = relevance_scores[node_idx]["topo"].cpu().numpy().reshape(1, -1)

    bio_64 = extract_summary_features_np_skip(bio_1024).squeeze()
    topo_64 = extract_summary_features_np_skip(topo_1024).squeeze()

    # BIO plot
    plot_feature_importance_bio(
        relevance_vector=bio_64,
        feature_names=bio_feat_names,
        node_name=gene,
        output_path=os.path.join(
            save_dir,
            f"{args.model_type}_{args.net_type}_{gene}_bio_feature_importance_epo{args.num_epochs}.png"
        )
    )

    # TOPO plot
    plot_feature_importance_topo(
        relevance_vector=topo_64,
        feature_names=topo_feat_names,
        node_name=gene,
        output_path=os.path.join(
            save_dir,
            f"{args.model_type}_{args.net_type}_{gene}_topo_feature_importance_epo{args.num_epochs}.png"
        )
    )

def plot_feature_importance_bio_ori(relevance_vector, feature_names, node_name=None, output_path="plots"):
    """
    Plot biological feature importance in OMICS:CANCER format using alphabetical order of cancer names.

    Parameters:
        relevance_vector (array-like): Relevance scores.
        feature_names (list of str): Feature names in the format Cancer_Omics.
        node_name (str, optional): Name of the node (used for title).
        output_path (str): Path to save the plot.
    """
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    import os

    # Define mapping from TCGA codes to cancer names
    cancer_map = {
        'BLCA': 'Bladder', 'BRCA': 'Breast', 'CESC': 'Cervix', 'COAD': 'Colon',
        'ESCA': 'Esophagus', 'HNSC': 'HeadNeck', 'KIRC': 'KidneyCC', 'KIRP': 'KidneyPC',
        'LIHC': 'Liver', 'LUAD': 'LungAD', 'LUSC': 'LungSC', 'PRAD': 'Prostate',
        'READ': 'Rectum', 'STAD': 'Stomach', 'THCA': 'Thyroid', 'UCEC': 'Uterus'
    }

    # Sort cancer codes by alphabetical order of their full names
    sorted_cancer_codes = sorted(cancer_map.keys(), key=lambda k: cancer_map[k])
    omics_order = ['cna', 'ge', 'meth', 'mf']
    column_labels = [f"{omics.upper()}:{cancer}" for omics in omics_order for cancer in sorted_cancer_codes]

    # Validate input
    if len(relevance_vector) != len(feature_names):
        raise ValueError(f"Mismatch: {len(relevance_vector)} values vs {len(feature_names)} names")

    # Build DataFrame
    df = pd.DataFrame({
        "feature": feature_names,
        "relevance": relevance_vector
    })

    df["omics_type"] = df["feature"].apply(lambda x: x.split("_")[1].lower())
    df["cancer_type"] = df["feature"].apply(lambda x: x.split("_")[0])
    df["formatted_label"] = df.apply(lambda row: f"{row['omics_type'].upper()}:{row['cancer_type']}", axis=1)

    # Reorder using column_labels
    df["order"] = df["formatted_label"].apply(lambda x: column_labels.index(x) if x in column_labels else -1)
    df = df[df["order"] != -1].sort_values("order")

    # Color mapping
    omics_color = {
        'cna': '#1F77B4',
        'ge': '#9467BD',
        'meth': '#2CA02C',
        'mf': '#D62728'
    }
    df["bar_color"] = df["omics_type"].map(omics_color)

    # Plotting
    plt.figure(figsize=(24, 5))
    ax = sns.barplot(x="formatted_label", y="relevance", data=df, palette=df["bar_color"].tolist())

    num_bars = len(df)
    margin = 0.75
    ax.set_xlim(-margin, num_bars - 1 + margin)

    ax.set_title(node_name if node_name else "", fontsize=16)
    ax.set_ylabel("Saliency score", fontsize=14)
    ax.set_xlabel("Feature (Omics: Cancer)", fontsize=14)

    # Color tick labels
    for tick, omics in zip(ax.get_xticklabels(), df["omics_type"]):
        tick.set_color(omics_color.get(omics, "black"))

    plt.xticks(rotation=90, fontsize=14)
    plt.yticks(fontsize=14)
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved BIO plot to {output_path}")

def plot_clusterwise_sorted_heatmaps(
    args,
    relevance_scores,          # [N, F] relevance scores for top genes
    row_labels,            # [N] cluster assignments for top genes
    gene_names,                # List[str], top-k gene names
    omics_splits,              # Dict[str, Tuple[int, int]], omics feature ranges
    output_dir,                # str, directory to save the heatmaps
    model_type,                # str, for filename
    net_type,                  # str, for filename
    epoch,                     # int, for filename
    tag="bio"                  # 'bio' or 'topo'
):
    os.makedirs(output_dir, exist_ok=True)
    num_clusters = len(set(row_labels))

    for cluster_id in range(num_clusters):
        # === Subset ===
        cluster_indices = np.where(row_labels == cluster_id)[0]
        if len(cluster_indices) == 0:
            continue

        cluster_scores = relevance_scores[cluster_indices]  # [n_genes_cluster, F]
        cluster_gene_names = [gene_names[i] for i in cluster_indices]

        # === Sort genes (rows) by total relevance ===
        gene_totals = cluster_scores.sum(axis=1)
        sorted_gene_idx = np.argsort(-gene_totals)
        cluster_scores_sorted = cluster_scores[sorted_gene_idx]
        sorted_gene_names = [cluster_gene_names[i] for i in sorted_gene_idx]

        # === Sort columns within each omics group ===
        sorted_columns = []
        for _, (start, end) in omics_splits.items():
            group_scores = cluster_scores_sorted[:, start:end]
            col_totals = group_scores.sum(axis=0)
            sorted_group_columns = np.argsort(-col_totals) + start
            sorted_columns.extend(sorted_group_columns)

        # === Apply column sorting ===
        final_scores = cluster_scores_sorted[:, sorted_columns]

        # === Plot ===
        plt.figure(figsize=(12, max(4, 0.25 * len(sorted_gene_names))))
        sns.heatmap(
            final_scores,
            cmap='YlGnBu',
            yticklabels=sorted_gene_names,
            xticklabels=False  # You can set True and relabel if you have feature names
        )
        plt.title(f"Cluster {cluster_id} ({tag}) - Relevance Heatmap")
        plt.xlabel("Sorted Features")
        plt.ylabel("Genes")

        save_path = os.path.join(
            output_dir,
            f"{model_type}_{net_type}_cluster_{cluster_id}_{tag}_sorted_heatmap_epo{epoch}.png"
        )
        plt.tight_layout()
        plt.savefig(save_path, dpi=300)
        plt.close()

def get_cluster_color(cluster_id, total_clusters=10):
    """
    Get a color from a colormap based on the cluster ID.
    """
    cmap = cm.get_cmap('tab10') if total_clusters <= 10 else cm.get_cmap('tab20')
    return cmap(cluster_id % total_clusters)

def plot_neighbor_relevance_ori(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False):
    """
    Plots the relevance score of neighbors for a confirmed gene, with optional coloring by cluster.
    Filters neighbors with relevance > 0.05, selects top 10, and normalizes scores.
    """
    # Filter and get top-10
    filtered = {k: v for k, v in neighbor_scores.items() if v > 0.05}
    if len(filtered) < 10:
        print(f"⚠️ Less than 10 neighbors > 0.05 for {gene_name}")
        return

    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])
    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"Node {nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores to [0.025, 0.975]
    norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
    norm_scores = norm_scores * 0.95 + 0.025

    # Assign bar colors based on clusters
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None:
            cluster_id = int(row_labels[nid])
            cluster_ids.append(cluster_id)
            colors.append(CLUSTER_COLORS.get(cluster_id % total_clusters, "gray"))
        else:
            colors.append("gray")

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved neighbor relevance plot to {output_path}")

def process_predictions(ranking, args, drivers_file_path, oncokb_file_path, ongene_file_path, ncg_file_path, intogen_file_path, node_names, non_labeled_nodes):
    """
    Process and save the predicted driver genes, confirmed sources, and known drivers.
    
    Args:
    - ranking: List of tuples (gene, score) representing ranked predictions.
    - args: Argument object containing model and network type, and score threshold.
    - drivers_file_path, oncokb_file_path, ongene_file_path, ncg_file_path, intogen_file_path: Paths to the confirmation gene files.
    - node_names, non_labeled_nodes: Information about node names and indices for matching.
    """
    # Load data from the confirmation files
    oncokb_genes = load_gene_set(oncokb_file_path)
    ongene_genes = load_gene_set(ongene_file_path)
    ncg_genes = load_gene_set(ncg_file_path)
    intogen_genes = load_gene_set(intogen_file_path)

    # Threshold for the score
    score_threshold = args.score_threshold

    confirmed_predictions = []
    predicted_genes = []

    for node, score in ranking:
        if score >= score_threshold:
            sources = []  # Accumulate sources confirming the gene
            if node in oncokb_genes:
                sources.append("OncoKB")
            if node in ongene_genes:
                sources.append("OnGene")
            if node in ncg_genes:
                sources.append("NCG")
            if node in intogen_genes:
                sources.append("IntOGen")
            if sources:  # If the gene is confirmed by at least one source
                confirmed_predictions.append((node, score, ", ".join(sources)))
            predicted_genes.append((node, score, ", ".join(sources) if sources else ""))

    # Save predictions to a CSV file
    save_predictions_to_csv(predicted_genes, 'results/gene_prediction/', args.model_type, args.net_type, args.num_epochs)
    save_confirmed_predictions_to_csv(confirmed_predictions, 'results/gene_prediction/', args.model_type, args.net_type, args.num_epochs)

    # Load known cancer driver genes
    with open(drivers_file_path, 'r') as f:
        known_drivers = set(line.strip() for line in f)

    # Collect predicted cancer driver genes that match the known drivers
    predicted_driver_genes = [node_names[i] for i in non_labeled_nodes if node_names[i] in known_drivers]

    # Save the predicted known cancer driver genes to a CSV file
    save_predicted_known_drivers(predicted_driver_genes, 'results/gene_prediction/', args.model_type, args.net_type, args.num_epochs)


def save_overall_metrics(total_time, average_time_per_epoch, average_cpu_usage, average_gpu_usage, args, output_dir):
    """
    Save the overall performance metrics to a CSV file.

    Args:
    - total_time: Total training time in seconds.
    - average_time_per_epoch: Average time per epoch in seconds.
    - average_cpu_usage: Average CPU usage in MB.
    - average_gpu_usage: Average GPU usage in MB.
    - args: Argument object containing model and network type.
    - output_dir: The directory where the results will be saved.
    """
    # Save overall metrics
    df_overall_metrics = pd.DataFrame([{
        "Model Type": args.model_type,
        "Total Time": f"{total_time:.4f}s",
        "Average Time per Epoch": f"{average_time_per_epoch:.4f}s",
        "Average CPU Usage (MB)": f"{average_cpu_usage:.2f}",
        "Average GPU Usage (MB)": f"{average_gpu_usage:.2f}"
    }])
    
    # Define path to save the CSV
    overall_metrics_csv_path = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_overall_performance_epo{args.num_epochs}_2048.csv')
    
    # Save to CSV
    df_overall_metrics.to_csv(overall_metrics_csv_path, index=False)
    print(f"Overall performance metrics saved to {overall_metrics_csv_path}")

def calculate_and_save_prediction_stats(non_labeled_nodes, labels, node_names, scores, args):
    """
    Calculate prediction statistics and save them to a CSV file.

    Parameters:
    - non_labeled_nodes: List of nodes without labels
    - labels: List of ground truth labels for the nodes
    - node_names: List of node names corresponding to the nodes
    - scores: List of predicted scores for the nodes
    - args: Arguments containing model and network type, score threshold, and number of epochs
    """
    # Calculate statistics
    non_labeled_nodes_count = len(non_labeled_nodes)
    ground_truth_driver_nodes = [i for i, label in enumerate(labels) if label == 1]
    ground_truth_non_driver_nodes = [i for i, label in enumerate(labels) if label == 0]

    predicted_driver_nodes = [node_names[i] for i in non_labeled_nodes if scores[i] >= args.score_threshold]

    # Prepare data to save to CSV
    stats_output_file = os.path.join('results/gene_prediction/', f'{args.model_type}_{args.net_type}_prediction_stats_{args.num_epochs}.csv')
    
    with open(stats_output_file, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(['Non-Labeled Nodes Count', 'Driver Genes', 'Non-Driver Genes', 'Total Testing Nodes', 'Predicted Driver Genes'])
        csvwriter.writerow([
            non_labeled_nodes_count,
            len(ground_truth_driver_nodes),
            len(ground_truth_non_driver_nodes),
            len(ground_truth_driver_nodes) + len(ground_truth_non_driver_nodes),
            len(predicted_driver_nodes)
        ])

    print(f"Prediction statistics saved to {stats_output_file}")

def plot_degree_distributions(sorted_degree_counts_above, sorted_degree_counts_below, args, output_dir):
    """
    Generates a box plot comparing interaction degrees of PCGs vs. Other Genes with KCGs.
    
    Parameters:
    - sorted_degree_counts_above: List of degrees for PCGs.
    - sorted_degree_counts_below: List of degrees for other genes.
    - args: Arguments containing model and training configuration.
    - output_dir: Directory to save the plot.
    """

    print("Generating box plot for degree distributions...")

    degree_data = [sorted_degree_counts_above, sorted_degree_counts_below]

    plt.figure(figsize=(3, 4))

    # Create the box plot
    boxplot = plt.boxplot(
        degree_data,
        vert=True,
        patch_artist=True,  # Allows customization of box color
        flierprops=dict(marker='o', markerfacecolor='grey', markeredgecolor='grey', markersize=5, alpha=0.2),  # Outliers
        boxprops=dict(color='black'),  # Box border color
        medianprops=dict(color='blue', linewidth=2),  # Median line style
        whiskerprops=dict(color='black', linewidth=1.5),  # Whiskers
        capprops=dict(color='black', linewidth=1.5)  # Caps
    )

    # Customize frame
    ax = plt.gca()
    ax.spines['top'].set_visible(False)  # Remove top frame line
    ax.spines['right'].set_visible(False)  # Remove right frame line

    # X-axis labels
    plt.xticks([1, 2], ['PCGs', 'Other'], fontsize=8)  
    plt.yticks(fontsize=8)
    plt.ylabel('Interaction Degrees with KCGs', fontsize=10, labelpad=10) 

    # Assign different colors to box plots
    colors = ['green', 'skyblue']
    for patch, color in zip(boxplot['boxes'], colors):
        patch.set_facecolor(color)

    # Save the plot
    os.makedirs(output_dir, exist_ok=True)
    output_plot_path = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_degree_distributions_epo{args.num_epochs}_2048.png')
    plt.savefig(output_plot_path, bbox_inches='tight')

    plt.tight_layout()
    plt.show()
    print(f"Box plot saved to {output_plot_path}")

def generate_kde_and_curves(logits, node_names, degree_counts_above, degree_counts_below, labels, train_mask, args):
    """
    Generates KDE plot comparing ACGNN score ranks with KCG interaction ranks, 
    computes Spearman correlation, and saves the KDE plot.
    Also computes and saves ROC and PR curves.

    Parameters:
    - logits: Tensor of model outputs before applying sigmoid.
    - node_names: List of node names.
    - degree_counts_above, degree_counts_below: Dictionaries mapping nodes to degree counts.
    - labels: Ground truth labels.
    - train_mask: Boolean mask indicating training samples.
    - args: Arguments containing model and training configuration.
    - output_dir: Directory to save plots.
    """

    print("Preparing data for KDE plot...")
    
    # Convert logits to probabilities
    scores = torch.sigmoid(logits).cpu().numpy()

    # Compute degree ranks
    degrees = [
        degree_counts_above.get(node_names[i], 0) +
        degree_counts_below.get(node_names[i], 0)
        for i in range(len(node_names))
    ]

    # Create DataFrame
    plot_data = pd.DataFrame({
        "Prob_pos_ranked": pd.Series(scores).rank(),
        "Degree_ranked": pd.Series(degrees).rank()
    })

    # KDE Plot
    print("Generating KDE plot...")
    plt.figure(figsize=(4, 4))
    sns.kdeplot(
        x=plot_data["Prob_pos_ranked"],
        y=plot_data["Degree_ranked"],
        cmap="Reds", fill=True,
        alpha=0.7, levels=50, thresh=0.05
    )

    # Spearman correlation
    correlation, p_value = scipy.stats.spearmanr(
        plot_data["Prob_pos_ranked"], plot_data["Degree_ranked"]
    )

    # Labels and formatting
    plt.xticks(fontsize=8)
    plt.yticks(fontsize=8)
    plt.xlabel('ACGNN score rank', fontsize=10, labelpad=10)
    plt.ylabel('KCG interaction rank', fontsize=12, labelpad=10)

    # Add correlation text
    legend_text = f"Spearman R: {correlation:.4f}\nP-value: {p_value:.3e}"
    plt.text(
        0.05, 0.95, legend_text,
        fontsize=8, transform=plt.gca().transAxes,
        verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8, edgecolor='none')
    )

    kde_output_path = os.path.join('results/gene_prediction/', f'{args.model_type}_{args.net_type}_kde_plot_epo{args.num_epochs}_2048.png')
    plt.savefig(kde_output_path, bbox_inches='tight')
    print(f"KDE plot saved to {kde_output_path}")

    plt.tight_layout()
    plt.show()

    # Extract labeled scores and labels
    labeled_scores = scores[train_mask.cpu().numpy()]
    labeled_labels = labels[train_mask.cpu().numpy()]

    # Convert to NumPy arrays if necessary
    labeled_scores_np = labeled_scores.cpu().detach().numpy() if isinstance(labeled_scores, torch.Tensor) else labeled_scores
    labeled_labels_np = labeled_labels.cpu().detach().numpy() if isinstance(labeled_labels, torch.Tensor) else labeled_labels

    # Save ROC and PR curves
    output_file_roc = os.path.join('results/gene_prediction/', f'{args.model_type}_{args.net_type}_epo{args.num_epochs}_2048_roc_curves.png')
    output_file_pr = os.path.join('results/gene_prediction/', f'{args.model_type}_{args.net_type}_epo{args.num_epochs}_2048_pr_curves.png')

    plot_roc_curve(labeled_labels_np, labeled_scores_np, output_file_roc)
    plot_pr_curve(labeled_labels_np, labeled_scores_np, output_file_pr)

    print(f"ROC curve saved to {output_file_roc}")
    print(f"PR curve saved to {output_file_pr}")

def plot_model_performance(args):
    """
    Generates and saves a scatter plot comparing AUROC and AUPRC values 
    for different models across multiple networks.

    Parameters:
    - models: List of model names.
    - networks: List of network names.
    - auroc: 2D list of AUROC scores (rows: models, cols: networks).
    - auprc: 2D list of AUPRC scores (rows: models, cols: networks).
    - args: Arguments containing model and training configuration.
    - output_dir: Directory to save the plot.
    """


    # Define models and networks
    models = ["ACGNN", "HGDC", "EMOGI", "MTGCN", "GCN", "GAT", "GraphSAGE", "GIN", "ChebNet"]
    networks = ["CPDB", "STRING", "HIPPIE"]

    # AUPRC values for ONGene and OncoKB for each model (rows: models, cols: networks)
    auroc = [
        [0.9652, 0.9578, 0.9297],  # ACGNN ACGNN & 0.9652 & 0.9783 & 0.9578 & 0.9738 & 0.9297 & 0.9597 \\
        [0.6776, 0.7133, 0.6525],  # HGDC
        [0.6735, 0.8184, 0.6672],  # EMOGI
        [0.6862, 0.7130, 0.6762],  # MTGCN
        [0.6915, 0.6688, 0.6708],  # GCN
        [0.6670, 0.8166, 0.6478],  # GAT
        [0.6664, 0.6166, 0.6571],  # GraphSAGE
        [0.5836, 0.5173, 0.5844],  # GIN
        [0.8017, 0.8777, 0.7409]   # ChebNet
    ]

    auprc = [
        [0.9783, 0.9738, 0.9597],  # ACGNN
        [0.7288, 0.7740, 0.7634],  # HGDC
        [0.7230, 0.8737, 0.7960],  # EMOGI
        [0.7712, 0.7878, 0.7785],  # MTGCN
        [0.7730, 0.7681, 0.7675],  # GCN
        [0.7086, 0.8791, 0.7496],  # GAT
        [0.7522, 0.7182, 0.7624],  # GraphSAGE
        [0.6405, 0.5918, 0.6791],  # GIN
        [0.8622, 0.9159, 0.8443]   # ChebNet
    ]

    # Compute averages for each model
    avg_auroc = np.mean(auroc, axis=1)
    avg_auprc = np.mean(auprc, axis=1)

    # Define colors for models and unique shapes for networks
    colors = ['red', 'grey', 'blue', 'green', 'purple', 'orange', 'cyan', 'brown', 'pink']
    network_markers = ['P', '^', 's']  # One shape for each network
    avg_marker = 'o'  # Marker for average points

    # Create the plot
    plt.figure(figsize=(8, 7))

    # Plot individual points for each model and network
    for i, model in enumerate(models):
        for j, network in enumerate(networks):
            plt.scatter(auprc[i][j], auroc[i][j], color=colors[i], 
                        marker=network_markers[j], s=90, alpha=0.6)

    # Add average points for each model
    for i, model in enumerate(models):
        plt.scatter(avg_auprc[i], avg_auroc[i], color=colors[i], marker=avg_marker, 
                    s=240, edgecolor='none', alpha=0.5)

    # Create legends for models (colors) and networks (shapes)
    model_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], 
                            markersize=14, label=models[i], alpha=0.5) for i in range(len(models))]
    network_legend = [Line2D([0], [0], marker=network_markers[i], color='k', linestyle='None', 
                            markersize=8, label=networks[i]) for i in range(len(networks))]

    # Add legends
    network_legend_artist = plt.legend(handles=network_legend, loc='lower right', title="Networks", fontsize=12, title_fontsize=14, frameon=True)
    plt.gca().add_artist(network_legend_artist)
    plt.legend(handles=model_legend, loc='upper left', fontsize=12, frameon=True)

    # Labels and title
    plt.ylabel("AUPRC", fontsize=14)
    plt.xlabel("AUROC", fontsize=14)

    # Save the plot
    comp_output_path = os.path.join('results/gene_prediction/', f'{args.model_type}_{args.net_type}_comp_plot_epo{args.num_epochs}_2048.png')
    plt.savefig(comp_output_path, bbox_inches='tight')
    
    print(f"Comparison plot saved to {comp_output_path}")

    # Show plot
    plt.tight_layout()
    plt.show()

def save_model_details(model, args, model_csv_path, in_feats, hidden_feats, out_feats):
    """
    Extracts model details and saves them to a CSV file.

    Parameters:
    - model: The neural network model.
    - args: Arguments containing model configuration.
    - model_csv_path: File path to save the model details.
    - in_feats: Number of input features.
    - hidden_feats: Number of hidden layer features.
    - out_feats: Number of output features.
    """
    # Count layers and parameters
    num_layers = sum(1 for _ in model.children())  # Count layers
    total_params = sum(p.numel() for p in model.parameters())  # Count parameters

    # Detect attention layers
    attention_layer_nodes = None
    for layer in model.children():
        if hasattr(layer, 'heads'):  # Assuming attention layers have 'heads' attribute
            attention_layer_nodes = layer.heads

    # Detect residual connections
    has_residual = any(isinstance(layer, nn.Identity) for layer in model.modules())

    # Prepare data for CSV
    model_data = {
        "Method": [args.model_type],
        "Number of Layers": [num_layers],
        "Input Layer Nodes": [in_feats],
        "Hidden Layer Nodes": [hidden_feats],
        "Attention Layer Nodes": [attention_layer_nodes if attention_layer_nodes else "N/A"],
        "Output Layer Nodes": [out_feats],
        "Total Parameters": [total_params],
        "Residual Connection": ["Yes" if has_residual else "No"]
    }

    # Convert to DataFrame and save as CSV
    df = pd.DataFrame(model_data)
    df.to_csv(model_csv_path, index=False)
    print(f"Model architecture saved to {model_csv_path}")

def save_predicted_scores(scores, labels, nodes, args):
    """
    Saves predicted scores and labels to a CSV file.

    Parameters:
    - scores: List of predicted scores.
    - labels: List of ground-truth labels.
    - nodes: Dictionary of node names.
    - args: Arguments containing model configuration.
    """
    # Initialize variables to calculate average scores and standard deviations
    label_scores = {0: [], 1: [], 2: [], 3: []}  # Groups for each label

    # Define CSV file path
    csv_file_path = os.path.join(
        'results/gene_prediction/',
        f'{args.model_type}_{args.net_type}_predicted_scores_threshold{args.score_threshold}_epo{args.num_epochs}.csv'
    )

    # Ensure directory exists
    os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)

    # Save results to CSV
    with open(csv_file_path, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Node Name', 'Score', 'Label'])  # Header

        for i, score in enumerate(scores):
            label = int(labels[i].item())  # Ensure label is an integer
            
            if label in [1, 0]:  # Ground-truth labels
                writer.writerow([list(nodes.keys())[i], score, label])
                label_scores[label].append(score)
            elif label == -1 and score >= args.score_threshold:  # Predicted driver genes
                writer.writerow([list(nodes.keys())[i], score, 2])
                label_scores[2].append(score)
            else:  # Non-labeled nodes or other
                writer.writerow([list(nodes.keys())[i], score, 3])
                label_scores[3].append(score)

    print(f"Predicted scores and labels saved to {csv_file_path}")

    return label_scores  # Returning for further analysis if needed

def save_average_scores(label_scores, args):
    """
    Calculates and saves the average score, standard deviation, and number of nodes per label.

    Parameters:
    - label_scores: Dictionary with labels as keys and lists of scores as values.
    - args: Arguments containing model configuration.
    """
    # Define CSV file path
    average_scores_file = os.path.join(
        'results/gene_prediction/',
        f'{args.model_type}_{args.net_type}_average_scores_by_label_threshold{args.score_threshold}_epo{args.num_epochs}.csv'
    )

    # Ensure directory exists
    os.makedirs(os.path.dirname(average_scores_file), exist_ok=True)

    # Save average scores to CSV
    with open(average_scores_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Label', 'Average Score', 'Standard Deviation', 'Number of Nodes'])  # Header

        for label, scores_list in label_scores.items():
            if scores_list:  # Check if the list is not empty
                avg_score = np.mean(scores_list)
                std_dev = np.std(scores_list)
                num_nodes = len(scores_list)
            else:
                avg_score = 0.0  # Default if no nodes in the label group
                std_dev = 0.0
                num_nodes = 0

            writer.writerow([label, avg_score, std_dev, num_nodes])

    print(f"Average scores by label saved to {average_scores_file}")

def plot_average_scores(label_scores, args):
    """
    Plots average scores with error bars and saves the figure.

    Parameters:
    - label_scores: Dictionary with labels as keys and lists of scores as values.
    - args: Arguments containing model configuration.
    """
    labels_list = []
    avg_scores = []
    std_devs = []

    for label, scores_list in label_scores.items():
        if scores_list:
            labels_list.append(label)
            avg_scores.append(np.mean(scores_list))
            std_devs.append(np.std(scores_list))

    if not labels_list:
        print("No valid scores to plot.")
        return

    # Define plot save path
    plot_path = os.path.join(
        'results/gene_prediction/',
        f'{args.model_type}_{args.net_type}_average_scores_with_error_bars_threshold{args.score_threshold}_epo{args.num_epochs}.png'
    )

    # Ensure directory exists
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)

    # Create plot
    plt.figure(figsize=(8, 6))
    plt.bar(labels_list, avg_scores, yerr=std_devs, capsize=5, color='skyblue', alpha=0.7)
    plt.xlabel('Label')
    plt.ylabel('Average Score')
    plt.title('Average Scores by Label with Error Bars')
    plt.grid(axis='y', linestyle='--', alpha=0.6)

    # Save and close plot
    plt.savefig(plot_path)
    plt.close()
    
    print(f"Error bar plot saved to {plot_path}")

def plot_score_distributions(label_scores, args):
    """
    Plots score distributions for each label and saves the figures.

    Parameters:
    - label_scores: Dictionary with labels as keys and lists of scores as values.
    - args: Arguments containing model configuration.
    """
    for label, scores_list in label_scores.items():
        if scores_list:
            plt.figure(figsize=(8, 6))
            plt.hist(scores_list, bins=20, alpha=0.7, color='#98f5e1', edgecolor='black')

            # Set labels and tick sizes
            plt.xlabel('Score', fontsize=16)
            plt.ylabel('Frequency', fontsize=16)
            plt.xticks(fontsize=14)
            plt.yticks(fontsize=14)

            # Customize grid and tick appearance
            plt.tick_params(axis='both', which='major', length=6, width=2, direction='inout', grid_color='gray', grid_alpha=0.5)
            plt.grid(axis='y', linestyle='--', alpha=0.6)

            # Define plot save path
            plot_path = os.path.join(
                'results/gene_prediction/',
                f'{args.model_type}_{args.net_type}_score_distribution_label{label}_threshold{args.score_threshold}_epo{args.num_epochs}.png'
            )

            # Ensure directory exists
            os.makedirs(os.path.dirname(plot_path), exist_ok=True)

            # Save and close plot
            plt.savefig(plot_path)
            plt.close()

            print(f"Score distribution for label {label} saved to {plot_path}")

def save_performance_metrics(epoch_times, cpu_usages, gpu_usages, args):
    """
    Saves performance metrics per epoch, including time per epoch, CPU, and GPU usage.

    Parameters:
    - epoch_times: List of epoch durations (in seconds).
    - cpu_usages: List of CPU memory usage per epoch (in MB).
    - gpu_usages: List of GPU memory usage per epoch (in MB).
    - args: Arguments containing model and training configuration.
    - output_dir: Directory to save the metrics CSV file.
    """

    # Compute total and average performance metrics
    total_time = sum(epoch_times)
    avg_time_per_epoch = total_time / args.num_epochs
    avg_cpu_usage = sum(cpu_usages) / args.num_epochs
    avg_gpu_usage = sum(gpu_usages) / args.num_epochs

    # Create DataFrame with per-epoch metrics
    df_metrics = pd.DataFrame({
        "Epoch": range(1, args.num_epochs + 1),
        "Time per Epoch (s)": epoch_times,
        "CPU Usage (MB)": cpu_usages,
        "GPU Usage (MB)": gpu_usages
    })

    # Define CSV path
    metrics_csv_path = os.path.join(
        'results/gene_prediction/',
        f'{args.model_type}_{args.net_type}_performance_metrics_epo{args.num_epochs}_2048.csv'
    )

    # Save to CSV
    df_metrics.to_csv(metrics_csv_path, index=False)

    print(f"Epoch performance metrics saved to {metrics_csv_path}")

    # Print summary statistics
    print(f"Total Training Time: {total_time:.2f} seconds")
    print(f"Average Time per Epoch: {avg_time_per_epoch:.2f} seconds")
    print(f"Average CPU Usage: {avg_cpu_usage:.2f} MB")
    print(f"Average GPU Usage: {avg_gpu_usage:.2f} MB")


def plot_pcg_cancer_genes(
    clusters,
    predicted_cancer_genes_count,
    total_genes_per_cluster,
    node_names,
    row_labels,
    output_path):
    """
    Plots the percentage of predicted cancer genes per cluster.
    """

    # Convert to sorted array
    clusters = np.array(sorted(total_genes_per_cluster.keys()))
    total_genes_array = np.array([total_genes_per_cluster[c] for c in clusters])
    predicted_counts = np.array([predicted_cancer_genes_count.get(c, 0) for c in clusters])

    # Compute percentages
    percent_predicted = np.divide(predicted_counts, total_genes_array, where=total_genes_array > 0)

    # Prepare bar colors
    colors = [CLUSTER_COLORS.get(c, '#333333') for c in clusters]

    # Plot
    fig, ax = plt.subplots(figsize=(8, 5))
    bars = ax.bar(clusters, percent_predicted, color=colors, edgecolor='black')

    # Annotate with raw count
    for bar, cluster_id in zip(bars, clusters):
        height = bar.get_height()
        count = predicted_cancer_genes_count.get(cluster_id, 0)
        ax.text(bar.get_x() + bar.get_width() / 2, height, str(count),
                ha='center', va='bottom', fontsize=16, fontweight='bold')

    # Left margin space
    num_clusters = len(clusters)
    ax.set_xlim(-0.55, num_clusters - 0.65)

    # Labels and formatting
    ax.set_ylabel("Percent of PCGs", fontsize=20)
    plt.xlabel("")  # No xlabel here
    plt.xticks(clusters, fontsize=16)
    plt.yticks(fontsize=16)
    ax.set_ylim(0, max(percent_predicted) + 0.1)

    sns.despine()
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"✅ Plot saved to {output_path}")

def plot_kcg_cancer_genes(clusters, kcg_count, total_genes_per_cluster, node_names, row_labels, output_path):

    cluster_ids = sorted(clusters)
    total = [total_genes_per_cluster[c] for c in cluster_ids]
    kcgs = [kcg_count.get(c, 0) for c in cluster_ids]
    proportions = [k / t if t > 0 else 0 for k, t in zip(kcgs, total)]

    plt.figure(figsize=(8, 5))
    bars = plt.bar(cluster_ids, proportions, 
                   color=[CLUSTER_COLORS.get(c, '#333333') for c in cluster_ids],
                   edgecolor='black')

    # Annotate each bar with the raw KCG count
    for bar, cluster_id in zip(bars, cluster_ids):
        height = bar.get_height()
        count = kcg_count.get(cluster_id, 0)
        plt.text(bar.get_x() + bar.get_width() / 2, height, str(count), 
                 ha='center', va='bottom', fontsize=16, fontweight='bold')

    ax = plt.gca()
    num_clusters = len(cluster_ids)
    ax.set_xlim(-0.55, num_clusters - 0.65)

    # Formatting
    ##plt.xlabel("Cluster ID", fontsize=16)
    plt.ylabel("Percent of KCGs", fontsize=20)
    plt.xlabel("")
    plt.xticks(cluster_ids, fontsize=16)
    plt.yticks(fontsize=16)
    plt.ylim(0, max(proportions) + 0.1)

    sns.despine()  # 🔻 Remove top/right spines

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

def plot_interactions_with_pcgs(data, output_path):
    """
    Creates a box plot with individual data points showing 
    the number of interactions with predicted cancer genes (PCGs).
    """
    import seaborn as sns
    import matplotlib.pyplot as plt

    plt.figure(figsize=(8, 5))
    sns.set_style("white")

    unique_clusters = sorted(data['Cluster'].unique())
    cluster_color_map = {cluster_id: CLUSTER_COLORS[cluster_id] for cluster_id in unique_clusters}

    ax = sns.boxplot(
        x='Cluster', y='Interactions', data=data, 
        hue='Cluster', palette=cluster_color_map, showfliers=False
    )

    sns.stripplot(
        x='Cluster', y='Interactions', data=data, 
        color='black', alpha=0.2, jitter=True, size=1.5
    )

    if ax.get_legend() is not None:
        ax.get_legend().remove()

    # 👉 Adjust x-axis limits for more space on the left
    num_clusters = len(unique_clusters)
    ax.set_xlim(-0.55, num_clusters - 0.65)  # Or adjust -0.4, -0.5, etc. for more space

    plt.ylabel("Number of interactions with PCGs", fontsize=20)
    plt.xlabel("")
    plt.xticks(rotation=0, ha="right", fontsize=16)
    plt.yticks(fontsize=16)
    plt.ylim(0, 50)

    sns.despine()  # 🔻 Remove top/right spines

    plt.savefig(output_path, dpi=300, bbox_inches="tight")  
    plt.close()

    print(f"✅ Plot saved to {output_path}")

def plot_interactions_with_kcgs(data, output_path):
    """
    Creates a box plot with individual data points showing 
    the number of interactions with known cancer genes (KCGs).
    """
    import seaborn as sns
    import matplotlib.pyplot as plt

    plt.figure(figsize=(8, 5))
    sns.set_style("white")

    unique_clusters = sorted(data['Cluster'].unique())
    cluster_color_map = {cluster_id: CLUSTER_COLORS[cluster_id] for cluster_id in unique_clusters}

    ax = sns.boxplot(
        x='Cluster', y='Interactions', data=data, 
        hue='Cluster', palette=cluster_color_map, showfliers=False
    )

    sns.stripplot(
        x='Cluster', y='Interactions', data=data, 
        color='black', alpha=0.2, jitter=True, size=1.5
    )

    if ax.get_legend() is not None:
        ax.get_legend().remove()

    # 👉 Adjust x-axis limits for more space on the left
    num_clusters = len(unique_clusters)
    ax.set_xlim(-0.55, num_clusters - 0.65)  # Or adjust -0.4, -0.5, etc. for more space

    plt.ylabel("Number of interactions with KCGs", fontsize=20)
    plt.xlabel("")
    plt.xticks(rotation=0, ha="right", fontsize=16)
    plt.yticks(fontsize=16)
    plt.ylim(0, 50)

    sns.despine()  # 🔻 Remove top/right spines

    plt.savefig(output_path, dpi=300, bbox_inches="tight")  
    plt.close()

    print(f"✅ Plot saved to {output_path}")



def plot_enriched_term_counts(enrichment_results, output_path, model_type, net_type, num_epochs, bio_color='#1f77b4', topo_color='#ff7f0e'):
    """
    Plot bar chart of the number of enriched terms per cluster for bio and topo clusters,
    with x-axis labels colored according to their type.

    Parameters:
        enrichment_results (dict): Dictionary with enrichment results for 'bio' and 'topo' clusters.
        output_path (str): Path to save the output plot.
        model_type (str): Model type for naming the file.
        net_type (str): Network type for naming the file.
        num_epochs (int): Number of epochs for naming the file.
        bio_color (str): Color for bio bars and labels.
        topo_color (str): Color for topo bars and labels.
        
        plt.plot(bio_scores_rel, label='Bio', color='#1f77b4')
        plt.plot(topo_scores_rel, label='Topo', color='#ff7f0e')
    """
    fig, ax = plt.subplots(figsize=(10, 6))

    bars = []
    xtick_colors = []
    xtick_labels = []

    for cluster_type in ['bio', 'topo']:
        cluster_ids = list(enrichment_results[cluster_type].keys())
        term_counts = [len(res) for res in enrichment_results[cluster_type].values()]
        labels = [f"{cluster_type.capitalize()}_{i}" for i in cluster_ids]

        color = bio_color if cluster_type == 'bio' else topo_color
        bar_container = ax.bar(labels, term_counts, label=cluster_type, color=color)
        bars.extend(bar_container)

        xtick_labels.extend(labels)
        xtick_colors.extend([color] * len(labels))

    # Adjust x-axis limits
    ax.set_xlim(-0.65, len(bars) - 0.5)

    # Labeling
    ax.set_ylabel("Number of enriched terms", fontsize=28)
    ##ax.set_title("Functional Coherence: Enriched Term Counts per Cluster", fontsize=28)

    # Set custom x-tick labels and colors
    ax.set_xticks(range(len(xtick_labels)))
    ax.set_xticklabels(xtick_labels, rotation=90, fontsize=20)
    for tick_label, color in zip(ax.get_xticklabels(), xtick_colors):
        tick_label.set_color(color)

    # Set y-tick font size
    ax.tick_params(axis='y', labelsize=20)

    # Clean up spines
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    ##ax.legend(frameon=False, fontsize=24)

    plt.tight_layout()
    filename = f"{model_type}_{net_type}_term_counts_barplot_epo{num_epochs}.png"
    plt.savefig(os.path.join(output_path, filename), dpi=300)
    plt.close()

    print(f"Bar plot saved to {os.path.join(output_path, filename)}")

def plot_shared_enriched_pathways_venn(enrichment_results, output_path, model_type, net_type, num_epochs, bio_color='#1f77b4', topo_color='#ff7f0e'):
    """
    Plots a Venn diagram showing the overlap of enriched pathways between bio and topo clusters.

    Parameters:
        enrichment_results (dict): Dictionary with enrichment DataFrames under 'bio' and 'topo'.
        output_path (str): Path to save the output plot.
        model_type (str): Model type for naming the file.
        net_type (str): Network type for naming the file.
        num_epochs (int): Number of epochs for naming the file.
        bio_color (str): Color for the bio set in the Venn diagram.
        topo_color (str): Color for the topo set in the Venn diagram.
    """
    bio_terms = set(
        sum([df['name'].tolist() for df in enrichment_results['bio'].values() if not df.empty], [])
    )
    topo_terms = set(
        sum([df['name'].tolist() for df in enrichment_results['topo'].values() if not df.empty], [])
    )

    plt.figure(figsize=(8, 8))
    venn = venn2(
        [bio_terms, topo_terms],
        set_labels=('Bio', 'Topo'),
        set_colors=(bio_color, topo_color),
        alpha=0.7
    )

    # Set font size for all Venn labels and subset counts
    for text in venn.set_labels:
        if text:
            text.set_fontsize(28)
    for text in venn.subset_labels:
        if text:
            text.set_fontsize(28)

    plt.title("Overlap of enriched pathways", fontsize=16)

    filename = f"{model_type}_{net_type}_shared_pathways_venn_epo{num_epochs}.png"
    venn_path = os.path.join(output_path, filename)
    plt.savefig(venn_path, dpi=300)
    plt.close()

    print(f"Venn diagram saved to {venn_path}")

def plot_contingency_matrix(row_labels_bio_topk, row_labels_topo_topk, ari_score, nmi_score, output_dir, args):
    """
    Plots a contingency matrix comparing bio and topo cluster labels.

    Parameters:
    - row_labels_bio_topk: Cluster labels from bio features.
    - row_labels_topo_topk: Cluster labels from topo features.
    - ari_score: Adjusted Rand Index between the clusterings.
    - nmi_score: Normalized Mutual Information score.
    - output_dir: Directory to save the plot.
    - args: Arguments containing model type, net type, and epoch count.
    """
    from sklearn.metrics import confusion_matrix
    import seaborn as sns
    import matplotlib.pyplot as plt
    import os

    # === Contingency matrix ===
    contingency = confusion_matrix(row_labels_bio_topk, row_labels_topo_topk)

    # Plot heatmap of confusion matrix
    plt.figure(figsize=(8, 8))
    sns.heatmap(contingency, annot=True, fmt='d', cmap='BuPu', cbar=False, annot_kws={"fontsize": 20})

    plt.title(f"Contingency Matrix: Bio vs Topo\n(ARI={ari_score:.2f}, NMI={nmi_score:.2f})", fontsize=30)
    plt.xlabel("Topo clusters", fontsize=28)
    plt.ylabel("Bio clusters", fontsize=28)

    plt.xticks([])
    plt.yticks([])

    # Save plot
    contingency_plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_contingency_matrix_epo{args.num_epochs}.png"
    )
    plt.tight_layout()
    plt.savefig(contingency_plot_path, dpi=300)
    plt.close()

    print(f"✅ Contingency matrix saved to {contingency_plot_path}")

def plot_omics_barplot_bio(df, output_path=None):
    """
    Plot omics relevance for biological features with format like 'MF:BRCA'.
    """
    omics_order = ['cna', 'ge', 'meth', 'mf']
    omics_colors = {
        'cna': '#9370DB',    # purple
        'ge': '#228B22',      # dark green
        'meth': '#00008B',   # dark blue
        'mf': '#b22222',     # dark red
    }

    # Extract 'Omics' and 'Cancer' from features like 'MF:BRCA'
    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    omics_relevance = df.groupby('Omics')['Relevance'].sum().reindex(omics_order)

    _plot_bar(omics_relevance, omics_colors, omics_order, output_path)

def plot_omics_barplot_topo(df, output_path=None):
    """
    Plot omics relevance for topological features with format like 'BRCA_mf'.
    """
    omics_order = ['cna', 'ge', 'meth', 'mf']
    omics_colors = {
        'cna': '#9370DB',    # purple
        'ge': '#228B22',      # dark green
        'meth': '#00008B',   # dark blue
        'mf': '#b22222',     # dark red
    }

    # Extract 'Cancer' and 'Omics' from features like 'BRCA_mf'
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    omics_relevance = df.groupby('Omics')['Relevance'].sum().reindex(omics_order)

    _plot_bar(omics_relevance, omics_colors, omics_order, output_path)

def plot_gene_feature_contributions_topo_(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    # Barplot of all 64 topo features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_topo(df, barplot_path)

    # Prepare for heatmap
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def save_and_plot_confirmed_genes_topo_(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    confirmed_genes_save_path,
    row_labels_topk,
    tag="topo",
    confirmed_gene_path="data/ncg_8886.txt"):
    """
    Finds confirmed cancer genes and plots their topological feature contributions.
    """

    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{cancer}_{omics}" for cancer in cancer_names for omics in omics_order]

    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    confirmed_genes = [g for g in node_names_topk if g in known_cancer_genes]

    with open(confirmed_genes_save_path, "w") as f:
        for gene in confirmed_genes:
            f.write(f"{gene}\n")

    ##summary_feature_relevance = extract_summary_features_np_topo(summary_feature_relevance)

    plot_dir = os.path.join(output_dir, f"{tag}_confirmed_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    for gene_name in confirmed_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        # plot_path = os.path.join(
        #     plot_dir,
        #     f"{args.model_type}_{args.net_type}_{gene_name}_{tag}_confirmed_feature_contributions_epo{args.num_epochs}.png"
        # )
        # cluster_id = row_labels_topk[idx].item()
        # plot_gene_feature_contributions_topo(
        #     #gene_name=gene_name,
        #     gene_name=f"{gene_name} (Cluster {cluster_id})",
        #     relevance_vector=relevance_vector,
        #     feature_names=feature_names,
        #     output_path=plot_path,
        #     score=score
        # )
        output_path = os.path.join(
            "results/gene_prediction/topo_confirmed_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_topo_confirmed_feature_contributions_epo{args.num_epochs}.png"
        )

        plot_gene_feature_contributions_topo(
            gene_name=gene,
            relevance_vector=importance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=gene_cluster,
            output_path=output_path
        )

def load_ground_truth_cancer_genes(file_path):
    """
    Load ground truth cancer genes from a file.
    
    Args:
        file_path (str): Path to the ground truth cancer gene file.
    
    Returns:
        set: A set containing ground truth cancer gene names.
    """
    with open(file_path, 'r') as f:
        return set(line.strip() for line in f)

def compute_node_saliency(model, graph, features, node_indices=None, use_abs=True, normalize=True):
    """
    Computes a fast gradient-based LRP approximation for selected nodes.

    Args:
        model: Trained GNN model
        graph: DGLGraph
        features: Node features (Tensor or numpy array)
        node_indices: List/Tensor of node indices to compute relevance for. If None, auto-select (probs > 0.0)
        use_abs: Whether to take absolute value of gradients (recommended)
        normalize: Whether to normalize relevance scores (per node)

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features]
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(node_indices):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            probs[idx].backward(retain_graph=(i != len(node_indices) - 1))

            grads = features.grad[idx]
            relevance = grads.abs() if use_abs else grads

            if normalize:
                norm = relevance.norm(p=1)  # L1 norm
                if norm > 0:
                    relevance = relevance / norm

            relevance_scores[idx] = relevance.detach()

    return relevance_scores

def compute_neighbor_saliency(model, graph, features, node_idx):
    features = features.clone().detach().requires_grad_(True)
    output = model(graph, features)
    score = output[node_idx].max()  # pick the score you want
    model.zero_grad()
    score.backward()
    
    # Find neighbors
    neighbors = graph.successors(node_idx)  # if directed, or .neighbors(node_idx) for undirected

    neighbor_saliencies = {}
    for n in neighbors:
        neighbor_saliencies[n] = features.grad[n].abs().sum().item()
    
    return neighbor_saliencies

def plot_neighbor_saliency_heatmap(
    graph,
    confirmed_genes,
    node_names,
    name_to_index,
    relevance_scores,
    omics_splits,
    output_path="neighbor_saliency_heatmap.png"
    ):
    heatmap_data = []
    heatmap_labels = []

    for gene in confirmed_genes:
        if gene not in name_to_index:
            continue
        idx = name_to_index[gene]
        
        # Get relevance for the confirmed gene
        gene_relevance = relevance_scores[idx].cpu().numpy()
        heatmap_data.append(gene_relevance)
        heatmap_labels.append(f"{gene} (self)")
        
        # Get neighbors
        neighbors = graph.successors(idx).tolist()
        for neighbor_idx in neighbors:
            neighbor_name = node_names[neighbor_idx]
            neighbor_relevance = relevance_scores[neighbor_idx].cpu().numpy()
            heatmap_data.append(neighbor_relevance)
            heatmap_labels.append(f"{neighbor_name} (nbr)")
    
    # Stack into matrix
    heatmap_data = np.vstack(heatmap_data)
    
    # Optional: min-max normalization per row
    heatmap_data_norm = (heatmap_data - heatmap_data.min(axis=1, keepdims=True)) / \
                        (heatmap_data.max(axis=1, keepdims=True) - heatmap_data.min(axis=1, keepdims=True) + 1e-8)
    
    # Limit number of neighbors plotted
    MAX_NEIGHBORS = 100  # or whatever you want

    if heatmap_data_norm.shape[0] > MAX_NEIGHBORS:
        neighbor_importance = np.abs(heatmap_data_norm).mean(axis=1)
        top_indices = np.argsort(-neighbor_importance)[:MAX_NEIGHBORS]
        heatmap_data_norm = heatmap_data_norm[top_indices]
        heatmap_labels = [heatmap_labels[i] for i in top_indices]

    # ✅ NOW set figure size based on the reduced number of labels
    plt.figure(figsize=(14, max(8, len(heatmap_labels) * 0.3)))

    # Create heatmap
    sns.heatmap(heatmap_data_norm, cmap="viridis", yticklabels=heatmap_labels, xticklabels=False)
    plt.title("Neighbor Saliency Heatmap")
    plt.xlabel("Omics Feature Dimension")
    plt.ylabel("Confirmed Genes + Neighbors")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"Neighbor saliency heatmap saved at {output_path}")

def build_subgraph(graph, target_idx, neighbors):
    G = nx.Graph()
    G.add_node(target_idx)

    for neighbor_idx in neighbors:
        G.add_node(neighbor_idx)
        G.add_edge(target_idx, neighbor_idx)

    return G

def plot_saliency_graph_multiomics(G, target_idx, bio_saliencies, topo_saliencies, node_names, save_path=None):
    pos = nx.spring_layout(G, seed=42)

    edges = list(G.edges())
    bio_scores = np.array([bio_saliencies.get(v, 0) for u, v in edges])
    topo_scores = np.array([topo_saliencies.get(v, 0) for u, v in edges])
    
    total_scores = bio_scores + topo_scores
    total_scores = np.clip(total_scores, 1e-6, None)  # Avoid zero division

    # Calculate color mixing
    bio_ratio = bio_scores / total_scores
    topo_ratio = topo_scores / total_scores

    edge_colors = [(topo_ratio[i], 0, bio_ratio[i]) for i in range(len(edges))]  # RGB: (red, 0, blue)
    edge_widths = total_scores / total_scores.max() * 5  # Max width = 5

    # Draw nodes
    node_colors = []
    for node in G.nodes():
        if node == target_idx:
            node_colors.append('gold')
        else:
            node_colors.append('lightgray')

    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=600)

    # Draw edges
    for (u, v), color, width in zip(edges, edge_colors, edge_widths):
        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=width, edge_color=[color])

    # Draw labels
    labels = {node: node_names[node] for node in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels=labels, font_size=10)

    plt.title(f"Neighbor Bio vs Topo Saliency for {node_names[target_idx]}", fontsize=14)
    plt.axis('off')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"✅ Saved multi-omics saliency graph to {save_path}")
    else:
        plt.close()

def plot_dynamic_sankey_topo_clusterlevel_each_gene(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    relevance_scores,
    row_labels,
    total_clusters,
    ):
    # Mapping node IDs and names
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    # Prepare folder to save
    output_dir = os.path.join("results/gene_prediction/topo_dynamic_sankey_clusterlevel")
    os.makedirs(output_dir, exist_ok=True)

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        gene_score = scores[node_idx]

        # ✅ Get cluster label from graph
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()
        print(f"{gene} → Node {node_idx} | Topo score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores_dict = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:  # bounds check
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors found for {gene}.")
            continue

        # Top neighbors (limit to 10)
        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # Sankey nodes
        labels = [f"{gene} (Cluster {gene_cluster})"]
        colors = [f"rgba({(gene_cluster/total_clusters)*255},100,150,0.8)"]

        for idx in top_neighbors.keys():
            neighbor_name = node_id_to_name[idx]
            neighbor_cluster = row_labels[idx]
            labels.append(f"{neighbor_name} (Cluster {neighbor_cluster})")
            colors.append(f"rgba({(neighbor_cluster/total_clusters)*255},180,100,0.8)")

        # Sankey links
        source = [0] * len(top_neighbors)  # source = gene
        target = list(range(1, len(top_neighbors)+1))  # targets = neighbors
        value = list(top_neighbors.values())

        # Build Sankey figure
        fig = go.Figure(data=[go.Sankey(
            node=dict(
                pad=15,
                thickness=20,
                line=dict(color="black", width=0.5),
                label=labels,
                color=colors
            ),
            link=dict(
                source=source,
                target=target,
                value=value,
            ))])

        fig.update_layout(
            title_text=f"Topo Dynamic Sankey for {gene}",
            font_size=10,
            width=800,
            height=600
        )

        # Save HTML
        output_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_{gene}_topo_confirmed_neighbor_sankey_epo{args.num_epochs}.html"
        )
        fig.write_html(output_path)
        fig.write_image(output_path.replace('.html', '.png'))

        print(f"✅ Sankey saved: {output_path}")

def plot_dynamic_sankey_bio_clusterlevel_not_fixed_colors(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores
):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    # Mapping cluster-to-cluster scores
    cluster_to_cluster_score = {}

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list. Skipping.")
            continue

        node_idx = topk_name_to_index[gene]
        gene_cluster = row_labels[node_idx].item()

        neighbors = neighbors_dict.get(gene, [])

        neighbor_scores_dict = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        for rel_idx, rel_score in top_neighbors.items():
            neighbor_cluster = row_labels[rel_idx].item()

            key = (gene_cluster, neighbor_cluster)
            cluster_to_cluster_score[key] = cluster_to_cluster_score.get(key, 0) + rel_score

    if not cluster_to_cluster_score:
        print("⚠️ No cluster-to-cluster links to plot.")
        return

    # Now prepare Sankey inputs
    clusters_involved = set()
    for (src_c, tgt_c) in cluster_to_cluster_score.keys():
        clusters_involved.add(src_c)
        clusters_involved.add(tgt_c)
    clusters_involved = sorted(list(clusters_involved))

    cluster_id_to_label = {c: f"C{c}" for c in clusters_involved}
    label_to_index = {f"C{c}": i for i, c in enumerate(clusters_involved)}

    source = []
    target = []
    value = []
    label = [f"C{c}" for c in clusters_involved]
    color = [f"rgba({(c*37)%255}, {(c*83)%255}, {(c*131)%255}, 0.8)" for c in clusters_involved]

    for (src_c, tgt_c), score in cluster_to_cluster_score.items():
        source.append(label_to_index[f"C{src_c}"])
        target.append(label_to_index[f"C{tgt_c}"])
        value.append(score)

    sankey_fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=label,
            color=color
        ),
        link=dict(
            source=source,
            target=target,
            value=value
        )
    )])

    sankey_fig.update_layout(
        title_text=f"Confirmed Cluster → Neighbor Cluster (Bio) - {args.model_type}_{args.net_type}",
        font_size=10,
        width=1200,
        height=800
    )

    output_dir = "results/gene_prediction/bio_dynamic_sankey_clusterlevel/"
    os.makedirs(output_dir, exist_ok=True)
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_confirmed_dynamic_sankey_clusterlevel_epo{args.num_epochs}.html"
    )
    sankey_fig.write_html(plot_path)
    print(f"✅ Cluster-level Sankey diagram saved to {plot_path}")

def get_neighbors_gene_names(graph, node_names, name_to_index, genes):
    neighbors_dict = {}
    for gene in genes:
        if gene in name_to_index:
            idx = name_to_index[gene]
            neighbors = graph.successors(idx).tolist()
            neighbor_names = [node_names[n] for n in neighbors]
            neighbors_dict[gene] = neighbor_names
    return neighbors_dict

def plot_dynamic_sankey_bio_clusterlevel(
    args,
    graph,
    node_names_topk,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores
):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names_topk, name_to_index, confirmed_genes)

    # Mapping cluster-to-cluster scores
    cluster_to_cluster_score = {}

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list. Skipping.")
            continue

        node_idx = topk_name_to_index[gene]
        gene_cluster = row_labels[node_idx].item()

        neighbors = neighbors_dict.get(gene, [])

        neighbor_scores_dict = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        for rel_idx, rel_score in top_neighbors.items():
            neighbor_cluster = row_labels[rel_idx].item()

            key = (gene_cluster, neighbor_cluster)
            cluster_to_cluster_score[key] = cluster_to_cluster_score.get(key, 0) + rel_score

    if not cluster_to_cluster_score:
        print("⚠️ No cluster-to-cluster links to plot.")
        return

    # Now prepare Sankey inputs
    clusters_involved = set()
    for (src_c, tgt_c) in cluster_to_cluster_score.keys():
        clusters_involved.add(src_c)
        clusters_involved.add(tgt_c)
    clusters_involved = sorted(list(clusters_involved))

    cluster_id_to_label = {c: f"C{c}" for c in clusters_involved}
    label_to_index = {f"C{c}": i for i, c in enumerate(clusters_involved)}

    source = []
    target = []
    value = []
    label = [f"C{c}" for c in clusters_involved]
    color = [CLUSTER_COLORS.get(c, "#CCCCCC") for c in clusters_involved]  # 🛠 fixed color

    for (src_c, tgt_c), score in cluster_to_cluster_score.items():
        source.append(label_to_index[f"C{src_c}"])
        target.append(label_to_index[f"C{tgt_c}"])
        value.append(score)

    sankey_fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=label,
            color=color
        ),
        link=dict(
            source=source,
            target=target,
            value=value
        )
    )])

    sankey_fig.update_layout(
        title_text=f"Confirmed Cluster → Neighbor Cluster (Bio) - {args.model_type}_{args.net_type}",
        font_size=10,
        width=1200,
        height=800
    )

    output_dir = "results/gene_prediction/bio_dynamic_sankey_clusterlevel/"
    os.makedirs(output_dir, exist_ok=True)
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_confirmed_dynamic_sankey_clusterlevel_epo{args.num_epochs}.html"
    )
    sankey_fig.write_html(plot_path)
    print(f"✅ Cluster-level Sankey diagram saved to {plot_path}")

def plot_dynamic_sankey_topo_clusterlevel(
    args,
    graph,
    node_names_topk,
    name_to_index,
    confirmed_genes,
    scores,
    relevance_scores,
    row_labels,
    total_clusters
):
    # Mapping node IDs and names
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names_topk, name_to_index, confirmed_genes)

    # Mapping cluster-to-cluster scores
    cluster_to_cluster_score = {}

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list. Skipping.")
            continue

        node_idx = topk_name_to_index[gene]
        gene_score = scores[node_idx]

        # ✅ Get topo cluster from graph
        gene_cluster = graph.ndata["cluster_topo"][node_idx].item()

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores_dict = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:  # bounds check
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score

        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors for {gene}.")
            continue

        # Top neighbors (limit to 10)
        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        for rel_idx, rel_score in top_neighbors.items():
            neighbor_cluster = row_labels[rel_idx].item()

            key = (gene_cluster, neighbor_cluster)
            cluster_to_cluster_score[key] = cluster_to_cluster_score.get(key, 0) + rel_score

    if not cluster_to_cluster_score:
        print("⚠️ No cluster-to-cluster links to plot.")
        return

    # Prepare Sankey inputs
    clusters_involved = set()
    for (src_c, tgt_c) in cluster_to_cluster_score.keys():
        clusters_involved.add(src_c)
        clusters_involved.add(tgt_c)
    clusters_involved = sorted(list(clusters_involved))

    cluster_id_to_label = {c: f"C{c}" for c in clusters_involved}
    label_to_index = {f"C{c}": i for i, c in enumerate(clusters_involved)}

    source = []
    target = []
    value = []
    label = [f"C{c}" for c in clusters_involved]
    color = [CLUSTER_COLORS.get(c, "#CCCCCC") for c in clusters_involved]  # 🛠 fixed color

    for (src_c, tgt_c), score in cluster_to_cluster_score.items():
        source.append(label_to_index[f"C{src_c}"])
        target.append(label_to_index[f"C{tgt_c}"])
        value.append(score)

    # Build Sankey figure
    sankey_fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=15,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=label,
            color=color
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
        ))])

    sankey_fig.update_layout(
        title_text=f"Confirmed Cluster → Neighbor Cluster (Topo) - {args.model_type}_{args.net_type}",
        font_size=10,
        width=1200,
        height=800
    )

    # Save
    output_dir = "results/gene_prediction/topo_dynamic_sankey_clusterlevel/"
    os.makedirs(output_dir, exist_ok=True)
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_confirmed_dynamic_sankey_clusterlevel_epo{args.num_epochs}.html"
    )
    sankey_fig.write_html(plot_path)
    print(f"✅ Cluster-level Sankey diagram saved to {plot_path}")

def plot_multilevel_sankey_bio_clusterlevel(
    args,
    graph,
    node_names_topk,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names_topk, name_to_index, confirmed_genes)

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list. Skipping.")
            continue

        node_idx = topk_name_to_index[gene]
        neighbors = neighbors_dict.get(gene, [])
        
        if not neighbors:
            print(f"⚠️ No neighbors found for {gene}.")
            continue

        neighbor_scores = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if not neighbor_scores:
            print(f"⚠️ No valid neighbor relevance for {gene}.")
            continue

        # Limit to top 10 neighbors
        neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:10])

        # Now build node list and mapping
        all_labels = []
        all_colors = []
        label_to_idx = {}

        # 1. Confirmed gene node
        confirmed_label = f"{gene}"
        all_labels.append(confirmed_label)
        all_colors.append("gray")
        label_to_idx[confirmed_label] = 0

        # 2. Neighbor nodes
        for neighbor_idx in neighbor_scores.keys():
            neighbor_name = node_id_to_name[neighbor_idx]
            neighbor_label = f"{neighbor_name}"
            if neighbor_label not in label_to_idx:
                label_to_idx[neighbor_label] = len(all_labels)
                all_labels.append(neighbor_label)
                all_colors.append("lightgray")

        # 3. Cluster nodes
        neighbor_clusters = set()
        for neighbor_idx in neighbor_scores.keys():
            neighbor_cluster = row_labels[neighbor_idx]
            neighbor_clusters.add(neighbor_cluster)

        for cluster_id in sorted(neighbor_clusters):
            cluster_label = f"Cluster {cluster_id}"
            if cluster_label not in label_to_idx:
                label_to_idx[cluster_label] = len(all_labels)
                all_labels.append(cluster_label)
                all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))

        # Build Sankey source-target-value-linkcolor
        source = []
        target = []
        value = []
        link_colors = []

        # Gene -> Neighbor
        for neighbor_idx, score in neighbor_scores.items():
            neighbor_name = node_id_to_name[neighbor_idx]
            source.append(label_to_idx[confirmed_label])
            target.append(label_to_idx[neighbor_name])
            value.append(score)
            link_colors.append("rgba(128,128,128,0.4)")  # gray links

        # Neighbor -> Cluster
        for neighbor_idx, score in neighbor_scores.items():
            neighbor_name = node_id_to_name[neighbor_idx]
            neighbor_cluster = row_labels[neighbor_idx]
            cluster_label = f"Cluster {neighbor_cluster}"

            source.append(label_to_idx[neighbor_name])
            target.append(label_to_idx[cluster_label])
            value.append(score)
            # Link colored by cluster
            cluster_color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
            link_colors.append(cluster_color.replace("#", "rgba(") + ",0.6)")  # lighter

        # Build figure
        fig = go.Figure(data=[go.Sankey(
            arrangement="snap",
            node=dict(
                pad=15,
                thickness=20,
                line=dict(color="black", width=0.5),
                label=all_labels,
                color=all_colors
            ),
            link=dict(
                source=source,
                target=target,
                value=value,
                color=link_colors
            ))])

        fig.update_layout(
            title_text=f"Multi-Level Sankey: {gene} → Neighbors → Clusters (Bio)",
            font_size=10,
            width=1000,
            height=700
        )

        # Save
        output_dir = "results/gene_prediction/bio_multilevel_sankey/"
        os.makedirs(output_dir, exist_ok=True)

        save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_{gene}_bio_multilevel_sankey_epo{args.num_epochs}.html"
        )
        fig.write_html(save_path)
        print(f"✅ Multi-level Sankey saved: {save_path}")

def compute_combined_relevance_scores(
    model,
    graph,
    features,
    node_indices=None,
    use_abs=True,
    normalize=False,
    prob_threshold=0.5
):
    """
    Computes gradient-based relevance (saliency) scores for selected nodes.

    Args:
        model: Trained GNN model
        graph: DGLGraph
        features: Node features (Tensor or numpy array)
        node_indices: List/Tensor of node indices to compute relevance for. If None, select using prob_threshold
        use_abs: Whether to use absolute value of gradients
        normalize: Whether to normalize relevance scores per node (optional)
        prob_threshold: Probability threshold for auto-selecting nodes

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features]
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > prob_threshold, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(node_indices):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            probs[idx].backward(retain_graph=(i != len(node_indices) - 1))

            grads = features.grad[idx]
            relevance = grads.abs() if use_abs else grads

            if normalize:
                norm = relevance.norm(p=1)  # L1 norm
                if norm > 0:
                    relevance = relevance / norm

            relevance_scores[idx] = relevance.detach()

    return relevance_scores

def saliency_to_color(saliency, min_saliency=0.0, max_saliency=1.0):
    saliency = np.clip((saliency - min_saliency) / (max_saliency - min_saliency), 0, 1)

    # Interpolate from blue (low) to red (high)
    r = int(0 + saliency * (255 - 0))    # Red from 0 to 255
    g = int(0)                           # Green stays 0
    b = int(255 - saliency * (255 - 0))  # Blue from 255 to 0

    return f"rgb({r},{g},{b})"

def hex_to_rgba(hex_color, alpha):
    hex_color = hex_color.lstrip('#')
    r, g, b = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
    return f'rgba({r},{g},{b},{alpha})'

def saliency_to_grayscale(saliency, min_saliency=0.0, max_saliency=1.0):
    saliency = np.clip((saliency - min_saliency) / (max_saliency - min_saliency), 0, 1)
    gray_value = int(238 - saliency * (238 - 17))  # 238: #eeeeee (light gray), 17: #111111 (dark gray)
    return f"rgb({gray_value},{gray_value},{gray_value})"

def plot_top_confirmed_gene_neighbors_chord_(
    graph,
    node_names,
    name_to_index,
    scores,
    confirmed_genes,
    output_html="top10_confirmed_gene_neighbors_chord.html",
    output_png="top10_confirmed_gene_neighbors_chord.png",
    top_k_genes=10,
    top_k_neighbors=10,
    min_edge_score=0.05
):
    """
    Plots a chord diagram of top confirmed genes and their neighbors.
    Saves both interactive HTML and static PNG versions.

    Parameters:
        graph: DGLGraph
        node_names: list mapping node indices to gene names
        name_to_index: dict mapping gene names to indices
        scores: numpy array of predicted scores
        confirmed_genes: list of confirmed gene names
        output_html: HTML path to save the interactive chord diagram
        output_png: PNG path to save the static image
        top_k_genes: number of confirmed genes to include
        top_k_neighbors: number of top neighbors per gene
        min_edge_score: minimum score threshold for edges
    """

    # Step 1: Filter top confirmed genes
    confirmed_gene_scores = [
        (gene, scores[name_to_index[gene]])
        for gene in confirmed_genes if gene in name_to_index
    ]
    top_confirmed_genes = sorted(confirmed_gene_scores, key=lambda x: x[1], reverse=True)[:top_k_genes]

    # Step 2: Build edges to top neighbors
    chord_links = []
    seen_edges = set()

    for gene, _ in top_confirmed_genes:
        idx = name_to_index[gene]
        neighbors = graph.successors(idx).tolist()
        neighbor_scores = [
            (node_names[n], scores[n]) for n in neighbors if node_names[n] != gene
        ]
        top_neighbors = sorted(neighbor_scores, key=lambda x: x[1], reverse=True)[:top_k_neighbors]

        for neighbor, score in top_neighbors:
            if score >= min_edge_score:
                edge_key = tuple(sorted((gene, neighbor)))
                if edge_key not in seen_edges:
                    chord_links.append((gene, neighbor, score))
                    seen_edges.add(edge_key)

    if not chord_links:
        print("[Chord Diagram] No valid edges found. Try lowering `min_edge_score`.")
        return

    # Step 3: Create Chord diagram
    chord = hv.Chord(chord_links).select(value=(min_edge_score, None))
    chord.opts(
        opts.Chord(
            cmap='Category20',
            edge_color='source',
            node_color='index',
            labels='name',
            edge_alpha=0.7,
            edge_line_width=hv.dim('value') * 5,
            width=900,
            height=900,
            title="Top Confirmed Genes and Their Neighbors"
        )
    )

    # Step 4: Save HTML
    hv.save(chord, output_html)
    print(f"[✔] HTML saved to: {output_html}")

    # Step 5: Save PNG using Bokeh backend
    try:
        from bokeh.io.export import export_png
        from bokeh.io import curdoc
        from holoviews.plotting.bokeh import render

        plot = render(chord)
        export_png(plot, filename=output_png)
        print(f"[✔] PNG saved to: {output_png}")
    except Exception as e:
        print(f"[⚠] PNG export failed: {e}")
        print("To enable PNG export, make sure you have installed: selenium, pillow, and a compatible web driver like chromedriver.")

def plot_top_confirmed_gene_neighbors_chord_not_gene_name(
    graph,
    node_names,
    name_to_index,
    scores,
    confirmed_genes,
    output_path="top10_confirmed_gene_neighbors_chord.html",
    top_k_genes=10,
    top_k_neighbors=10,
    min_edge_score=0.05  # filter out weak edges for clarity
):
    """
    Plots a chord diagram of top K confirmed genes and their top K neighbors by predicted cancer score.

    Parameters:
        graph: DGL graph
        node_names: list of all node names
        name_to_index: dict mapping names to indices
        scores: numpy array of cancer scores
        confirmed_genes: list of confirmed gene names
        output_path: where to save the HTML file
        top_k_genes: how many confirmed genes to show
        top_k_neighbors: how many neighbors per gene
        min_edge_score: minimum score threshold for showing edges
    """

    # Step 1: Rank confirmed genes by model score
    confirmed_gene_scores = [
        (gene, scores[name_to_index[gene]])
        for gene in confirmed_genes if gene in name_to_index
    ]
    top_confirmed_genes = sorted(confirmed_gene_scores, key=lambda x: x[1], reverse=True)[:top_k_genes]

    # Step 2: For each gene, get top neighbors by score
    chord_links = []
    seen_edges = set()

    for gene, _ in top_confirmed_genes:
        idx = name_to_index[gene]
        neighbors = graph.successors(idx).tolist()
        neighbor_scores = [
            (node_names[n], scores[n]) for n in neighbors if node_names[n] != gene
        ]
        top_neighbors = sorted(neighbor_scores, key=lambda x: x[1], reverse=True)[:top_k_neighbors]

        for neighbor, score in top_neighbors:
            if score >= min_edge_score:
                edge_key = tuple(sorted((gene, neighbor)))
                if edge_key not in seen_edges:
                    chord_links.append((gene, neighbor, score))
                    seen_edges.add(edge_key)

    if not chord_links:
        print("[Chord Diagram] No valid edges found. Try lowering `min_edge_score`.")
        return

    # Step 3: Create Chord Diagram
    chord = hv.Chord(chord_links).select(value=(min_edge_score, None))
    chord.opts(
        opts.Chord(
            cmap='Category20',
            edge_color='source',
            node_color='index',
            labels='name',
            edge_alpha=0.7,
            edge_line_width=hv.dim('value') * 5,
            width=900,
            height=900,
            title="Top Confirmed Genes and Neighbors"
        )
    )

    # Save
    hv.save(chord, output_path)
    print(f"[✔] Chord diagram saved to: {output_path}")

def plot_chord_diagram_topo(
    args,
    source_labels,
    target_labels,
    matrix,
    CLUSTER_COLORS
):
    """
    A stylized approximation of a chord diagram using Plotly's Sankey layout.
    """

    all_labels = list(set(source_labels) | set(target_labels))
    label_to_index = {label: idx for idx, label in enumerate(all_labels)}
    
    source = []
    target = []
    value = []
    link_colors = []

    for i, src in enumerate(source_labels):
        for j, tgt in enumerate(target_labels):
            if matrix[i][j] > 0:
                source_idx = label_to_index[src]
                target_idx = label_to_index[tgt]
                source.append(source_idx)
                target.append(target_idx)
                value.append(matrix[i][j])

                # Get color from source if available
                color = CLUSTER_COLORS.get(i, "#999999")
                link_colors.append(hex_to_rgba(color, 0.5))

    node_colors = [CLUSTER_COLORS.get(i, "#888888") for i in range(len(all_labels))]

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=20,
            thickness=20,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=node_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        title_text="Chord Diagram (Approx) - Topological Gene Interactions",
        font_size=12,
        margin=dict(l=200, r=200, t=100, b=100),
        width=1200,
        height=1000,
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )

    # Save output
    output_dir = "results/gene_prediction/topo_chord_diagram/"
    os.makedirs(output_dir, exist_ok=True)

    html_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_chord_epo{args.num_epochs}.html"
    )
    fig.write_html(html_path)
    print(f"✅ Chord diagram saved as HTML: {html_path}")

    # Save PNG
    try:
        import plotly.io as pio
        png_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_topo_chord_epo{args.num_epochs}.png"
        )
        fig.write_image(png_path, format="png", scale=2, width=1200, height=1000)
        print(f"🖼️ PNG also saved: {png_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' to enable image export:\n  pip install kaleido")

def plot_top_confirmed_gene_neighbors_chord(
    graph,
    node_names,
    name_to_index,
    scores,
    confirmed_genes,
    row_labels=None,
    cluster_colors=None,
    output_html="results/chord/top10_confirmed_gene_neighbors_chord.html",
    output_png="results/chord/top10_confirmed_gene_neighbors_chord.png",
    top_k_genes=10,
    top_k_neighbors=10,
    min_edge_score=0.05
):
    """
    Plots a chord diagram of top confirmed genes and their neighbors.
    Saves both interactive HTML and static PNG versions.

    Parameters:
        graph: DGLGraph
        node_names: list mapping node indices to gene names
        name_to_index: dict mapping gene names to indices
        scores: numpy array of predicted scores
        confirmed_genes: list of confirmed gene names
        row_labels: list or array of cluster labels for each node (optional)
        cluster_colors: dict mapping cluster label to hex color (optional)
        output_html: path to save interactive HTML
        output_png: path to save static PNG
        top_k_genes: number of confirmed genes to include
        top_k_neighbors: number of top neighbors per gene
        min_edge_score: minimum score threshold for edges
    """
    # Ensure output dirs exist
    os.makedirs(os.path.dirname(output_html), exist_ok=True)
    os.makedirs(os.path.dirname(output_png), exist_ok=True)

    # Step 1: Filter top confirmed genes
    confirmed_gene_scores = [
        (gene, scores[name_to_index[gene]])
        for gene in confirmed_genes if gene in name_to_index
    ]
    top_confirmed_genes = sorted(confirmed_gene_scores, key=lambda x: x[1], reverse=True)[:top_k_genes]

    # Step 2: Build edges to top neighbors
    chord_links = []
    seen_edges = set()

    for gene, _ in top_confirmed_genes:
        idx = name_to_index[gene]
        neighbors = graph.successors(idx).tolist()
        neighbor_scores = [
            (node_names[n], scores[n]) for n in neighbors if node_names[n] != gene
        ]
        top_neighbors = sorted(neighbor_scores, key=lambda x: x[1], reverse=True)[:top_k_neighbors]

        for neighbor, score in top_neighbors:
            if score >= min_edge_score:
                edge_key = tuple(sorted((gene, neighbor)))
                if edge_key not in seen_edges:
                    chord_links.append((gene, neighbor, score))
                    seen_edges.add(edge_key)

    if not chord_links:
        print("[Chord Diagram] No valid edges found. Try lowering `min_edge_score`.")
        return

    # Step 3: Create node color mapping if cluster info is provided
    node_set = set([g for edge in chord_links for g in edge[:2]])
    df_nodes = pd.DataFrame({'name': list(node_set)})

    if row_labels is not None and cluster_colors is not None:
        #df_nodes['cluster'] = df_nodes['name'].map(lambda g: row_labels[name_to_index[g]])
        df_nodes['cluster'] = df_nodes['name'].map(
            lambda g: row_labels[name_to_index[g]] if g in name_to_index else -1
        )

        df_nodes['color'] = df_nodes['cluster'].map(lambda c: cluster_colors.get(c, "#cccccc"))
    else:
        df_nodes['color'] = "#cccccc"

    name_to_color = dict(zip(df_nodes['name'], df_nodes['color']))

    # Step 4: Create Chord diagram
    chord = hv.Chord(chord_links).select(value=(min_edge_score, None))
    chord.opts(
        opts.Chord(
            cmap='Category20',
            edge_color='source',
            node_color=hv.dim('name').categorize(name_to_color, default="#cccccc"),
            labels='name',
            edge_alpha=0.7,
            edge_line_width=hv.dim('value') * 5,
            width=900,
            height=900,
            title="Top Confirmed Genes and Their Neighbors"
        )
    )

    # Step 5: Save HTML
    hv.save(chord, output_html)
    print(f"[✔] HTML saved to: {output_html}")

    # Step 6: Save PNG using Bokeh backend
    try:
        from bokeh.io.export import export_png
        from holoviews.plotting.bokeh import render
        plot = render(chord)
        export_png(plot, filename=output_png)
        print(f"[✔] PNG saved to: {output_png}")
    except Exception as e:
        print(f"[⚠] PNG export failed: {e}")
        print("To enable PNG export, install dependencies: `pip install selenium pillow` and configure a headless browser like `chromedriver`.")

def plot_enriched_pathways_heatmap(
    enrichment_results,
    output_dir,
    model_type,
    net_type,
    num_epochs,
    max_term_len=40,
    max_topo_term_len=60,
    max_rows=50,
    top_n_terms_per_cluster=None,
    return_data=False
):

    heatmap_data = pd.DataFrame()

    for cluster_type in ['bio', 'topo']:
        for cid, df in enrichment_results[cluster_type].items():
            if top_n_terms_per_cluster:
                df = df[df['p_value'] < 0.05].sort_values(by='p_value').head(top_n_terms_per_cluster)
            colname = f"{cluster_type.capitalize()}_{cid}"
            vals = {}
            for _, row in df.iterrows():
                p = row['p_value']
                name = row['name']
                if p < 0.05 and len(name) <= max_term_len:
                    term = f"{name} ({row['source']})"
                    vals[term] = -np.log10(p)
            heatmap_data[colname] = pd.Series(vals)

    heatmap_data = heatmap_data.fillna(0)
    heatmap_data = heatmap_data[heatmap_data.max(axis=1) > 1]

    enrichment_csv_path = os.path.join(
        output_dir,
        f"{model_type}_{net_type}_enrichment_matrix_epo{num_epochs}.csv"
    )
    heatmap_data.to_csv(enrichment_csv_path, index_label='Enriched Pathway')

    # Topo terms export
    topo_terms = []
    for cid, df in enrichment_results['topo'].items():
        if top_n_terms_per_cluster:
            df = df[df['p_value'] < 0.05].sort_values(by='p_value').head(top_n_terms_per_cluster)
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= max_topo_term_len:
                topo_terms.append({
                    "Cluster": f"Topo_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })

    topo_terms_df = pd.DataFrame(topo_terms)
    topo_terms_path = os.path.join(
        output_dir,
        f"{model_type}_{net_type}_topo_cluster_top_terms_epo{num_epochs}.csv"
    )
    topo_terms_df.to_csv(topo_terms_path, index=False)

    if heatmap_data.shape[0] > max_rows:
        step = max(1, heatmap_data.shape[0] // max_rows)
        selected_indices = heatmap_data.index[::step][:max_rows]
        heatmap_data = heatmap_data.loc[selected_indices]

    norm_data = heatmap_data / heatmap_data.max().replace(0, 1)

    colormaps = {
        'bio': get_cmap('Blues'),
        'topo': get_cmap('YlOrRd'),
    }

    colors = np.zeros((heatmap_data.shape[0], heatmap_data.shape[1], 4))
    col_types = []

    for i, col in enumerate(norm_data.columns):
        group = 'bio' if col.lower().startswith("bio") else 'topo'
        col_types.append(group)
        cmap = colormaps[group]
        colors[:, i, :] = cmap(norm_data[col].values)

    fig, ax = plt.subplots(figsize=(0.5 * len(norm_data.columns), 0.2 * len(norm_data)))

    ax.imshow(colors, aspect='auto')

    ax.set_xticks(np.arange(len(norm_data.columns)))
    ax.set_xticklabels(norm_data.columns, rotation=90, fontsize=14)
    ax.set_yticks(np.arange(len(norm_data.index)))
    ax.set_yticklabels(norm_data.index, fontsize=14)

    ax.set_ylabel("Enriched Pathway", fontsize=16, labelpad=20)

    for xtick, col in zip(ax.get_xticklabels(), col_types):
        xtick.set_color('darkblue' if col == 'bio' else 'darkred')

    ax.set_title("Top Enriched Pathways per Cluster (p < 0.05)", fontsize=15, pad=16)
    ax.set_xlabel("Cluster", fontsize=14)

    legend_patches = [
        Patch(color='cornflowerblue', label='Bio'),
        Patch(color='salmon', label='Topo')
    ]
    fig.legend(handles=legend_patches, loc='lower center', ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.08))

    sns.despine(ax=ax, trim=True)
    ax.tick_params(axis='both', which='both', length=0)
    plt.tight_layout(rect=[0, 0, 0.95, 0.93])

    enriched_terms_heatmap_path = os.path.join(
        output_dir,
        f"{model_type}_{net_type}_enriched_terms_heatmap_epo{num_epochs}.png"
    )
    plt.savefig(enriched_terms_heatmap_path, dpi=300)
    plt.close()

    if return_data:
        return heatmap_data, topo_terms_df

def plot_enriched_term_counts(enrichment_results, output_path, model_type, net_type, num_epochs):
    """
    Plot bar chart of the number of enriched terms per cluster for bio and topo clusters,
    with x-axis labels colored according to their type.
    """
    fig, ax = plt.subplots(figsize=(10, 6))

    bars = []
    xtick_colors = []
    xtick_labels = []

    colormaps = {
        'bio': get_cmap('Blues')(0.6),     # medium blue
        'topo': get_cmap('YlOrRd')(0.6),   # medium orange-red
    }
        
    for cluster_type in ['bio', 'topo']:
        cluster_ids = list(enrichment_results[cluster_type].keys())
        term_counts = [len(res) for res in enrichment_results[cluster_type].values()]
        labels = [f"{cluster_type.capitalize()}_{i}" for i in cluster_ids]

        color = colormaps[cluster_type]
        bar_container = ax.bar(labels, term_counts, label=cluster_type, color=color)
        bars.extend(bar_container)

        xtick_labels.extend(labels)
        xtick_colors.extend([color] * len(labels))

    ax.set_xlim(-0.65, len(bars) - 0.5)
    ax.set_ylabel("Number of enriched terms", fontsize=28)

    ax.set_xticks(range(len(xtick_labels)))
    ax.set_xticklabels(xtick_labels, rotation=90, fontsize=20)
    for tick_label, color in zip(ax.get_xticklabels(), xtick_colors):
        tick_label.set_color(color)

    ax.tick_params(axis='y', labelsize=20)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    plt.tight_layout()
    filename = f"{model_type}_{net_type}_term_counts_barplot_epo{num_epochs}.png"
    plt.savefig(os.path.join(output_path, filename), dpi=300)
    plt.close()

    print(f"Bar plot saved to {os.path.join(output_path, filename)}")

def plot_shared_enriched_pathways_venn(enrichment_results, output_path, model_type, net_type, num_epochs):
    """
    Plots a Venn diagram showing the overlap of enriched pathways between bio and topo clusters.
    """
    bio_terms = set(
        sum([df['name'].tolist() for df in enrichment_results['bio'].values() if not df.empty], [])
    )
    topo_terms = set(
        sum([df['name'].tolist() for df in enrichment_results['topo'].values() if not df.empty], [])
    )

    colormaps = {
        'bio': get_cmap('Blues')(0.6),     # medium blue
        'topo': get_cmap('YlOrRd')(0.6),   # medium orange-red
    }
    
    plt.figure(figsize=(8, 8))
    venn = venn2(
        [bio_terms, topo_terms],
        set_labels=('Bio', 'Topo'),
        set_colors=(colormaps['bio'], colormaps['topo']),
        alpha=0.7
    )

    for text in venn.set_labels:
        if text:
            text.set_fontsize(28)
    for text in venn.subset_labels:
        if text:
            text.set_fontsize(28)

    plt.title("Overlap of enriched pathways", fontsize=16)

    filename = f"{model_type}_{net_type}_shared_pathways_venn_epo{num_epochs}.png"
    venn_path = os.path.join(output_path, filename)
    plt.savefig(venn_path, dpi=300)
    plt.close()

    print(f"Venn diagram saved to {venn_path}")

def save_and_plot_enriched_pathways(enrichment_results, args, output_dir):
    # === Prepare Data ===
    heatmap_data = pd.DataFrame()

    for cluster_type in ['bio', 'topo']:
        for cid, df in enrichment_results[cluster_type].items():
            colname = f"{cluster_type.capitalize()}_{cid}"
            vals = {}
            for _, row in df.iterrows():
                p = row['p_value']
                name = row['name']
                if p < 0.05 and len(name) <= 60:
                    term = f"{name} ({row['source']})"
                    vals[term] = -np.log10(p)
            heatmap_data[colname] = pd.Series(vals)

    # Clean and filter
    heatmap_data = heatmap_data.fillna(0)
    heatmap_data = heatmap_data[heatmap_data.max(axis=1) > 1]

    # Save full enrichment data to CSV
    enrichment_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_enrichment_matrix_epo{args.num_epochs}.csv"
    )
    heatmap_data.to_csv(enrichment_csv_path, index_label='Enriched Pathway')

    # === Save Topo Cluster → Top Enriched Terms to CSV ===
    topo_terms = []
    for cid, df in enrichment_results['topo'].items():
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= 60:
                topo_terms.append({
                    "Cluster": f"Topo_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })
    topo_terms_df = pd.DataFrame(topo_terms)
    topo_terms_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_cluster_top_terms_epo{args.num_epochs}.csv"
    )
    topo_terms_df.to_csv(topo_terms_path, index=False)

    # === Save Bio Cluster → Top Enriched Terms to CSV ===
    bio_terms = []
    for cid, df in enrichment_results['bio'].items():
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= 60:
                bio_terms.append({
                    "Cluster": f"Bio_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })
    bio_terms_df = pd.DataFrame(bio_terms)
    bio_terms_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_cluster_top_terms_epo{args.num_epochs}.csv"
    )
    bio_terms_df.to_csv(bio_terms_path, index=False)

    # === Select 50 evenly spaced rows for plotting ===
    if heatmap_data.shape[0] > 50:
        step = max(1, heatmap_data.shape[0] // 50)
        selected_indices = heatmap_data.index[::step][:50]
        heatmap_data = heatmap_data.loc[selected_indices]

    # === Normalize for color contrast ===
    norm_data = heatmap_data.copy()
    norm_data = norm_data / norm_data.max().replace(0, 1)

    # === Apply group-wise colormaps ===
    colormaps = {
        'bio': get_cmap('Blues'),
        'topo': get_cmap('YlOrRd'),
    }

    colors = np.zeros((heatmap_data.shape[0], heatmap_data.shape[1], 4))  # RGBA
    col_types = []

    for i, col in enumerate(norm_data.columns):
        group = 'bio' if col.lower().startswith("bio") else 'topo'
        col_types.append(group)
        cmap = colormaps[group]
        colors[:, i, :] = cmap(norm_data[col].values)

    # === Plot ===
    fig, ax = plt.subplots(figsize=(0.5 * len(norm_data.columns), 0.2 * len(norm_data)))

    ax.imshow(colors, aspect='auto')
    ax.set_xticks(np.arange(len(norm_data.columns)))
    ax.set_xticklabels(norm_data.columns, rotation=90, fontsize=13)
    ax.set_yticks(np.arange(len(norm_data.index)))
    ax.set_yticklabels(norm_data.index, fontsize=14)
    ax.set_ylabel("Enriched Pathway", fontsize=18, labelpad=20)

    # Color x-axis labels
    for xtick, col in zip(ax.get_xticklabels(), col_types):
        xtick.set_color('darkblue' if col == 'bio' else 'darkred')

    ax.set_title("Top Enriched Pathways per Cluster (p < 0.05)", fontsize=14, pad=16)
    ax.set_xlabel("Cluster", fontsize=14)

    legend_patches = [
        Patch(color='cornflowerblue', label='Bio'),
        Patch(color='salmon', label='Topo')
    ]
    fig.legend(handles=legend_patches, loc='lower center', ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.08))

    sns.despine(ax=ax, trim=True)
    ax.tick_params(axis='both', which='both', length=0)
    plt.tight_layout(rect=[0, 0, 0.95, 0.93])

    # Save plot
    enriched_terms_heatmap_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_enriched_terms_heatmap_epo{args.num_epochs}.png"
    )
    plt.savefig(enriched_terms_heatmap_path, dpi=300)
    plt.close()

    # === Return DataFrames for downstream analysis ===
    return heatmap_data, topo_terms_df, bio_terms_df

def plot_collapsed_clusterfirst_multilevel_sankey_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        confirmed_genes,
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = [
        "ACTB", "ATR", "BCL6", "BRCA2", "CDK4", "CUL1",
        "E2F3", "EGFR", "ETS1", "FOXM1", "HDAC2", "RBM39",
        "SKP2", "SRC"
    ]

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    # for g in selected_novel_genes + selected_known_genes:
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 20:
            break

    #combined_genes = [g for g in combined_genes if g != "IRF2"]

    confirmed_genes = combined_genes

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Confirmed Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)

                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
    
                # if neighbor_idx == node_idx:
                #     continue
                neighbor_name = node_id_to_name[neighbor_idx]
                
                if neighbor_name == gene:
                    continue
                
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)

                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)

                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    if highlight_node_indices:
        x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
        y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

        fig.add_trace(go.Scatter(
            x=x_positions,
            y=y_positions,
            mode='none',
            marker=dict(
                size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
                color="rgba(255,0,0,0.3)",
                line=dict(width=2, color="rgba(255,0,0,0.7)"),
                sizemode='diameter'
            ),
            hoverinfo='skip',
            showlegend=False
        ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        scores
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def compute_relevance_scores_(model, graph, features, node_indices=None, use_abs=True):
    """
    Computes gradient-based relevance scores (saliency) for selected nodes using sigmoid probabilities.

    Args:
        model: Trained GNN model
        graph: DGL graph
        features: Input node features
        node_indices: List/Tensor of node indices to compute relevance for. If None, auto-select using probs > 0.0
        use_abs: Whether to use absolute gradients (recommended for visualization)

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features] (0s for nodes not analyzed)
    """
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        # Auto-select nodes (e.g., predicted cancer genes)
        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(node_indices):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            probs[idx].backward(retain_graph=(i != len(node_indices) - 1))

            grads = features.grad[idx]
            relevance_scores[idx] = grads.abs().detach() if use_abs else grads.detach()

    return relevance_scores

def extract_summary_features_np_bio_(bio_embeddings_np):
    """
    Extracts summary features from just the 1024 biological features (bio only).

    Args:
        bio_embeddings_np (np.ndarray): shape [num_nodes, 1024]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = bio_embeddings_np.shape
    summary_features = []

    assert num_features == 1024, f"Expected 1024 bio features, got {num_features}"

    for o_idx in range(4):  # 4 omics types
        for c_idx in range(16):  # 16 cancer types
            base = o_idx * 16 * 16 + c_idx * 16
            group = bio_embeddings_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)

def extract_summary_features_np_bio(bio_embeddings_np, omics_types, cancer_names, save_path=None):
    import pandas as pd

    num_nodes, num_features = bio_embeddings_np.shape
    summary_features = []

    assert num_features == 1024, f"Expected 1024 bio features, got {num_features}"

    # omics_types = ["expression", "methylation", "mutation", "copy_number"]
    # cancer_names = [
    #     "BLCA", "BRCA", "CESC", "COAD", "ESCA", "HNSC", "KIRC", "LGG",
    #     "LIHC", "LUAD", "LUSC", "OV", "PRAD", "READ", "STAD", "UCEC"
    # ]

    column_names = []

    for o_idx, omics in enumerate(omics_types):
        seen = set()
        for c_idx, cancer in enumerate(cancer_names):
            label = f"{omics}_{cancer}"
            if label in seen:
                raise ValueError(f"Duplicate label: {label}")
            seen.add(label)

            base = o_idx * 16 * 16 + c_idx * 16
            group = bio_embeddings_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)
            column_names.append(label)

    summary_array = np.concatenate(summary_features, axis=1)  # shape [num_nodes, 64]

    # Optional CSV save
    if save_path:
        df = pd.DataFrame(summary_array, columns=column_names)
        df.to_csv(save_path, index=False)
        print(f"✅ Summary bio features saved to: {save_path}")

    return summary_array, column_names

def extract_summary_features_np_topo(topo_features_np):
    """
    Extracts summary features from the topological embedding section (features 1024–2047)
    by computing the max over each 16-dimensional segment.

    Args:
        features_np (np.ndarray): shape [num_nodes, 2048]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = topo_features_np.shape
    assert num_features == 1024, f"Expected 1024 features, got {num_features}"

    # Select topological features only
    ##topo_features = features_np[:, 1024:]  # shape: [num_nodes, 1024]
    ##topo_features = topo_features_np[:, 1024:2048]
    topo_features = topo_features_np  # already 1024 features


    summary_features = []

    # Pool over 64 chunks of 16 features
    for i in range(64):
        start = i * 16
        end = start + 16
        group = topo_features[:, start:end]  # shape: [num_nodes, 16]
        max_vals = group.max(axis=1, keepdims=True)  # shape: [num_nodes, 1]
        summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # shape: [num_nodes, 64]

def extract_specific_omics_cancer(bio_embeddings_np, omics_target='mf', cancer_target='BRCA'):
    """
    Extracts 16 features for a specific omics-cancer pair from the 1024 bio features.

    Args:
        bio_embeddings_np (np.ndarray): shape [num_nodes, 1024]
        omics_target (str): one of ['cna', 'ge', 'meth', 'mf']
        cancer_target (str): one of 16 cancer types like 'BRCA'

    Returns:
        np.ndarray: shape [num_nodes, 16]
    """
    omics_types = ['cna', 'ge', 'meth', 'mf']
    # cancer_names = ['BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP', 'LIHC', 'LUAD',
    #                 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC']
    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]
    o_idx = omics_types.index(omics_target)
    c_idx = cancer_names.index(cancer_target)

    start = o_idx * 16 * 16 + c_idx * 16
    end = start + 16

    return bio_embeddings_np[:, start:end]

def plot_top_gene_ridge_from_specific_omics_(
    bio_embeddings_np,
    node_names,
    row_labels,
    output_path,
    omics_target='mf',
    cancer_target='BRCA',
    top_n=12
):
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    import os

    # 1. Extract 16-dim features for the selected omics-cancer pair
    features_16 = extract_specific_omics_cancer(bio_embeddings_np, omics_target, cancer_target)

    # 2. Ensure output directory exists
    os.makedirs(output_path, exist_ok=True)

    # 3. Build DataFrame
    df = pd.DataFrame(features_16)
    df["Gene"] = node_names
    df["Cluster"] = row_labels
    df["MeanFeatureValue"] = df.iloc[:, :-2].mean(axis=1)

    clusters = sorted(df["Cluster"].unique())

    for cluster in clusters:
        cluster_df = df[df["Cluster"] == cluster].copy()
        top_genes = cluster_df.nlargest(top_n, "MeanFeatureValue")

        # Prepare melted long-form data for seaborn
        plot_data = []
        for _, row in top_genes.iterrows():
            gene = row["Gene"]
            for i in range(16):
                plot_data.append({
                    "Gene": gene,
                    "FeatureIndex": i,
                    "Value": row[i]
                })
        plot_df = pd.DataFrame(plot_data)

        # Sort gene order
        gene_order = plot_df.groupby("Gene")["Value"].mean().sort_values().index
        plot_df["Gene"] = pd.Categorical(plot_df["Gene"], categories=gene_order, ordered=True)

        # Compute proper x-axis limits with padding
        xmin = plot_df["Value"].min()
        xmax = plot_df["Value"].max()
        x_range = xmax - xmin
        xmin -= x_range * 0.5
        xmax += x_range * 0.5


        # Plot with ridge style
        sns.set(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})
        g = sns.FacetGrid(
            plot_df,
            row="Gene",
            hue="Gene",
            aspect=12,
            height=0.4,
            palette="Spectral",
            sharex=True
        )

        g.map(
            sns.kdeplot,
            "Value",
            bw_adjust=0.5,
            fill=True,
            alpha=0.8,
            cut=100,
            clip=(xmin, xmax)
        )
        g.map(
            sns.kdeplot,
            "Value",
            bw_adjust=0.5,
            color="black",
            lw=1,
            cut=100,
            clip=(xmin, xmax)
        )


        g.set_titles("")
        g.set(xlim=(xmin, xmax), xlabel="Feature Value", ylabel="", yticks=[])

        # Label genes on the left side
        for ax, gene in zip(g.axes.flat, gene_order):
            ax.set_ylabel(gene, rotation=0, ha='right', va='top', fontsize=18, labelpad=10)

        g.despine(bottom=True, left=True)
        g.fig.subplots_adjust(hspace=-0.3, left=0.3, right=0.95, top=0.93)
        g.fig.suptitle(f"Cluster {cluster}", x=0.6, fontsize=20)

        # After g.set(...) and before plt.savefig(...)
        for ax in g.axes.flat:
            ax.tick_params(axis='x', labelsize=16)  # or 14, 16, etc.

        # Save
        out_path = os.path.join(
            output_path,
            f"{omics_target}_{cancer_target}_cluster{cluster}_top{top_n}_genes_ridge.png"
        )
        plt.savefig(out_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"✅ Saved: {out_path}")

def plot_top_gene_ridge_across_omics_(
    bio_embeddings_np,
    node_names,
    row_labels,
    output_path,
    cancer_target='BRCA',
    top_n=12
):
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    import os

    os.makedirs(output_path, exist_ok=True)

    # 1. Extract all 4 omics (64 features) for the cancer
    features_64 = extract_all_omics_for_cancer(bio_embeddings_np, cancer_target)

    # 2. Prepare DataFrame
    df = pd.DataFrame(features_64)
    df["Gene"] = node_names
    df["Cluster"] = row_labels
    df["MeanFeatureValue"] = df.iloc[:, :-2].mean(axis=1)

    clusters = sorted(df["Cluster"].unique())

    omics_types = ['cna', 'ge', 'meth', 'mf']
    omics_labels = []
    for omics in omics_types:
        omics_labels += [omics] * 16

    for cluster in clusters:
        cluster_df = df[df["Cluster"] == cluster].copy()
        top_genes = cluster_df.nlargest(top_n, "MeanFeatureValue")

        # Melt into long-form with omics and feature index
        plot_data = []
        for _, row in top_genes.iterrows():
            gene = row["Gene"]
            for i in range(64):
                plot_data.append({
                    "Gene": gene,
                    "FeatureIndex": i % 16,
                    "Omics": omics_labels[i],
                    "Value": row[i]
                })
        plot_df = pd.DataFrame(plot_data)

        # Sort genes
        gene_order = plot_df.groupby("Gene")["Value"].mean().sort_values().index
        plot_df["Gene"] = pd.Categorical(plot_df["Gene"], categories=gene_order, ordered=True)

        # X-axis limits
        xmin = plot_df["Value"].min()
        xmax = plot_df["Value"].max()
        x_range = xmax - xmin
        xmin -= x_range * 0.5
        xmax += x_range * 0.5

        # Plot
        sns.set(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})
        g = sns.FacetGrid(
            plot_df,
            row="Gene",
            hue="Omics",  # Color by omics
            aspect=12,
            height=0.4,
            palette="tab10",
            sharex=True
        )

        g.map(
            sns.kdeplot,
            "Value",
            bw_adjust=0.5,
            fill=True,
            alpha=0.8,
            cut=100,
            clip=(xmin, xmax)
        )
        g.map(
            sns.kdeplot,
            "Value",
            bw_adjust=0.5,
            color="black",
            lw=1,
            cut=100,
            clip=(xmin, xmax)
        )

        g.set_titles("")
        g.set(xlim=(xmin, xmax), xlabel="Feature Value", ylabel="", yticks=[])

        for ax, gene in zip(g.axes.flat, gene_order):
            ax.set_ylabel(gene, rotation=0, ha='right', va='top', fontsize=18, labelpad=10)

        g.despine(bottom=True, left=True)
        g.fig.subplots_adjust(hspace=-0.3, left=0.3, right=0.95, top=0.93)
        g.fig.suptitle(f"Cluster {cluster} — {cancer_target} (All Omics)", x=0.6, fontsize=20)

        for ax in g.axes.flat:
            ax.tick_params(axis='x', labelsize=16)

        # Save
        out_path = os.path.join(output_path, f"{cancer_target}_allomics_cluster{cluster}_top{top_n}_genes_ridge.png")
        plt.savefig(out_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"✅ Saved: {out_path}")

def run_gprofiler_enrichment(cluster_dict, cancer_type, tag):
    gp = GProfiler(return_dataframe=True)
    output_files = []
    for cluster, genes in cluster_dict.items():
        try:
            result = gp.profile(
                organism="hsapiens",
                query=genes,
                sources=["REAC", "KEGG", "GO:BP", "HP"],
                user_threshold=0.05,
                significance_threshold_method="fdr"
            )
            if not result.empty:
                for source in ["REAC", "KEGG", "GO:BP", "HP"]:
                    filtered = result[result["source"] == source]
                    path = f"results/gene_prediction/enrichment/{cancer_type}_{tag}_Cluster_{cluster}_{source}_enrichment.csv"
                    dir_path = os.path.dirname(path)
                    os.makedirs(dir_path, exist_ok=True)

                    filtered.to_csv(path, index=False)
                    output_files.append(path)
        except Exception as e:
            print(f"Enrichment failed for {cancer_type} {cluster}: {e}")
    return output_files

def plot_dot_enrichment_per_cluster(cancer_type, tag, source="REAC", top_n=10):
    files = glob.glob(f"results/gene_prediction/{cancer_type}_{tag}_Cluster_*_{source}_enrichment.csv")
    if not files:
        print(f"No enrichment files found for {cancer_type.upper()} [{tag}] and source {source}")
        return

    for f in files:
        try:
            cluster = Path(f).stem.split("_")[2]  # Cluster number
        except IndexError:
            print(f"Filename parsing failed: {f}")
            continue

        df = pd.read_csv(f)
        if df.empty or "intersection_size" not in df or "query_size" not in df:
            print(f"Invalid dataframe for {f}")
            continue

        df["gene_ratio"] = df["intersection_size"] / df["query_size"]
        df["-log10(FDR)"] = -np.log10(df["p_value"].clip(lower=1e-300))
        top_df = df.sort_values("p_value").head(top_n)

        plt.figure(figsize=(10, 6))
        sns.scatterplot(
            data=top_df,
            x="gene_ratio",
            y="name",
            hue="-log10(FDR)",
            size="-log10(FDR)",
            sizes=(40, 200),
            palette="viridis",
            legend="brief"
        )
        plt.title(f"{source} Enrichment for {cancer_type.upper()} [{tag}] - Cluster {cluster}")
        plt.xlabel("Gene Ratio")
        plt.ylabel("Pathway")
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(f"results/gene_prediction/{cancer_type}_{tag}_Cluster_{cluster}_{source}_dotplot.png", dpi=300)
        plt.close()

def plot_dot_enrichment_from_results(enrichment_results, top_n=10, source_filter=["REAC", "KEGG", "GO:BP", "HP"]):
    """
    Plot dot plots for enrichment results stored in memory.

    Args:
        enrichment_results (dict): Nested dictionary like {'bio': {0: df, 1: df, ...}, 'topo': {0: df, ...}}.
        top_n (int): Number of top enriched terms to display.
        source_filter (list): List of enrichment sources to include.
    """
    for cluster_type, cluster_data in enrichment_results.items():
        for cluster_id, df in cluster_data.items():
            if df.empty or "p_value" not in df or "intersection_size" not in df or "query_size" not in df:
                print(f"Skipping {cluster_type} cluster {cluster_id}: invalid or empty data")
                continue

            # Optional: filter by source
            if source_filter:
                df = df[df["source"].isin(source_filter)]

            # Compute additional metrics
            df["gene_ratio"] = df["intersection_size"] / df["query_size"]
            df["-log10(FDR)"] = -np.log10(df["p_value"].clip(lower=1e-300))
            df_plot = df.sort_values("p_value").head(top_n)

            if df_plot.empty:
                print(f"No significant enrichment to plot for {cluster_type} cluster {cluster_id}")
                continue

            # Plot
            plt.figure(figsize=(10, 6))
            sns.scatterplot(
                data=df_plot,
                x="gene_ratio",
                y="name",
                hue="-log10(FDR)",
                size="-log10(FDR)",
                sizes=(40, 200),
                palette="viridis",
                legend="brief"
            )
            plt.title(f"{cluster_type.capitalize()} Cluster {cluster_id} Enrichment")
            plt.xlabel("Gene Ratio")
            plt.ylabel("Pathway")
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()

            # Save plot
            out_path = f"results/enrichment_dotplots/{cluster_type}_cluster_{cluster_id}_dotplot.png"
            Path(out_path).parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(out_path, dpi=300)
            plt.close()
            print(f"Saved: {out_path}")

def plot_dot_enrichment_per_cluster_all(
    cancer_names,
    tag="bio",
    source="REAC",
    top_n=10,
    n_clusters=10,
    base_path="results/gene_prediction/enrichment"
):
    base_path = Path(base_path)

    for cancer_type in cancer_names:
        print(f"\n🔍 Processing {cancer_type.upper()} [{tag}] enrichment dotplots...")
        
        for cluster_id in range(n_clusters):
            file_path = base_path / f"{cancer_type}_{tag}_Cluster_{cluster_id}_{source}_enrichment.csv"

            if not file_path.exists():
                print(f"  ✗ Missing: {file_path.name}")
                continue

            try:
                df = pd.read_csv(file_path)
            except Exception as e:
                print(f"  ✗ Error reading {file_path.name}: {e}")
                continue

            if df.empty or "intersection_size" not in df or "query_size" not in df:
                print(f"  ✗ Invalid content: {file_path.name}")
                continue

            # Calculate gene ratio and transformed FDR
            df["gene_ratio"] = df["intersection_size"] / df["query_size"]
            df["-log10(FDR)"] = -np.log10(df["p_value"].clip(lower=1e-300))
            top_df = df.sort_values("p_value").head(top_n)

            # Plot
            plt.figure(figsize=(10, 6))
            sns.scatterplot(
                data=top_df,
                x="gene_ratio",
                y="name",
                hue="-log10(FDR)",
                size="-log10(FDR)",
                sizes=(40, 200),
                palette="viridis",
                legend="brief"
            )
            plt.title(f"{source} Enrichment: {cancer_type.upper()} [{tag}] - Cluster {cluster_id}")
            plt.xlabel("Gene Ratio")
            plt.ylabel("Pathway")
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()

            # Save plot
            out_path = base_path / f"{cancer_type}_{tag}_Cluster_{cluster_id}_{source}_dotplot.png"
            plt.savefig(out_path, dpi=300)
            plt.close()
            print(f"  ✓ Saved: {out_path.name}")

def apply_full_spectral_biclustering_cancer(cancer_feature, n_clusters):
    from sklearn.cluster import SpectralBiclustering
    import numpy as np

    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")

    assert cancer_feature.shape[1] == 64, f"Expected 64 summary features, got {cancer_feature.shape[1]}"

    # Perform spectral biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(cancer_feature)

    row_labels = bicluster.row_labels_
    print("✅ Spectral Biclustering complete.")

    return row_labels

def plot_all_cancer_ridges_all_omics(
    bio_embeddings_np,
    node_names,
    #row_labels,
    n_clusters_row,
    output_base_path,
    top_n=12
):
    cancer_list = [
        'BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP',
        'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC'
    ]

    for cancer in cancer_list:
        print(f"\n📊 Plotting ridge for ALL_OMICS — {cancer}...")
        output_path = os.path.join(output_base_path, f"ALL_OMICS_{cancer}")
        plot_top_gene_ridge_from_all_omics(
            bio_embeddings_np=bio_embeddings_np,
            node_names=node_names,
            #row_labels=row_labels,
            n_clusters_row=n_clusters_row,
            output_path=output_path,
            cancer_target=cancer,
            top_n=top_n
        )

def plot_top_gene_ridge_from_all_omics(
    bio_embeddings_np,
    node_names,
    #row_labels,
    n_clusters_row,
    output_path,
    cancer_target='BRCA',
    top_n=12
):
    # 1. Extract 64-dim features
    #features_64 = extract_all_omics_for_cancer(bio_embeddings_np, cancer_target)
    cancer_feature = extract_all_omics_for_cancer(bio_embeddings_np, cancer_target)
    row_labels = apply_full_spectral_biclustering_cancer(cancer_feature, n_clusters=n_clusters_row)
    
    # 2. Ensure output directory exists
    os.makedirs(output_path, exist_ok=True)

    # 3. Build DataFrame
    df = pd.DataFrame(cancer_feature)
    df["Gene"] = node_names
    df["Cluster"] = row_labels
    df["MeanFeatureValue"] = df.iloc[:, :-2].mean(axis=1)

    clusters = sorted(df["Cluster"].unique())
    cluster_dict = defaultdict(list)  # Save top genes for each cluster

    for cluster in clusters:
        cluster_df = df[df["Cluster"] == cluster].copy()
        top_genes = cluster_df.nlargest(top_n, "MeanFeatureValue")
        cluster_dict[cluster] = top_genes["Gene"].tolist()

        # Prepare long-form data for seaborn
        plot_data = []
        for _, row in top_genes.iterrows():
            gene = row["Gene"]
            for i in range(64):
                plot_data.append({
                    "Gene": gene,
                    "FeatureIndex": i,
                    "Value": row[i]
                })
        plot_df = pd.DataFrame(plot_data)

        gene_order = plot_df.groupby("Gene")["Value"].mean().sort_values().index
        plot_df["Gene"] = pd.Categorical(plot_df["Gene"], categories=gene_order, ordered=True)

        xmin = plot_df["Value"].min()
        xmax = plot_df["Value"].max()
        x_range = xmax - xmin
        xmin -= x_range * 0.5
        xmax += x_range * 0.5

        sns.set(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})
        g = sns.FacetGrid(
            plot_df,
            row="Gene",
            hue="Gene",
            aspect=12,
            height=0.4,
            palette="Spectral",
            sharex=True
        )

        g.map(sns.kdeplot, "Value", bw_adjust=0.5, fill=True, alpha=0.8, cut=100, clip=(xmin, xmax))
        g.map(sns.kdeplot, "Value", bw_adjust=0.5, color="black", lw=1, cut=100, clip=(xmin, xmax))

        g.set_titles("")
        g.set(xlim=(xmin, xmax), xlabel="Feature Value", ylabel="", yticks=[])

        for ax, gene in zip(g.axes.flat, gene_order):
            ax.set_ylabel(gene, rotation=0, ha='right', va='top', fontsize=18, labelpad=10)

        g.despine(bottom=True, left=True)
        g.fig.subplots_adjust(hspace=-0.3, left=0.3, right=0.95, top=0.93)
        g.fig.suptitle(f"Cluster {cluster}", x=0.6, fontsize=20)

        for ax in g.axes.flat:
            ax.tick_params(axis='x', labelsize=16)

        out_path = os.path.join(output_path, f"ALL_OMICS_{cancer_target}_cluster{cluster}_top{top_n}_genes_ridge.png")
        plt.savefig(out_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"✅ Saved: {out_path}")

    # Save cluster_dict as JSON
    '''cluster_json_path = os.path.join(output_path, f"ALL_OMICS_{cancer_target}_cluster_genes.json")

    def make_json_safe(obj):
        if isinstance(obj, dict):
            return {str(k): make_json_safe(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [make_json_safe(v) for v in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()  # Convert NumPy scalars to native Python types
        else:
            return obj

    cluster_dict_safe = make_json_safe(cluster_dict)

    with open(cluster_json_path, "w") as f:
        json.dump(cluster_dict_safe, f, indent=2)

    print(f"💾 Saved cluster gene dictionary: {cluster_json_path}")'''

def extract_all_omics_for_cancer(bio_embeddings_np, cancer_target='BRCA'):
    """
    Extracts 64 features (4 omics × 16 features) for a specific cancer type from the 1024 bio features.

    Args:
        bio_embeddings_np (np.ndarray): shape [num_nodes, 1024]
        cancer_target (str): one of 16 cancer types like 'BRCA'

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    omics_types = ['cna', 'ge', 'meth', 'mf']
    # cancer_names = ['BLCA', 'BRCA', 'CESC', 'COAD', 'ESCA', 'HNSC', 'KIRC', 'KIRP',
    #                 'LIHC', 'LUAD', 'LUSC', 'PRAD', 'READ', 'STAD', 'THCA', 'UCEC']
    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]    
    c_idx = cancer_names.index(cancer_target)

    feature_blocks = []
    for o_idx in range(len(omics_types)):
        start = o_idx * 16 * 16 + c_idx * 16
        end = start + 16
        feature_blocks.append(bio_embeddings_np[:, start:end])

    return np.concatenate(feature_blocks, axis=1)  # shape: [num_nodes, 64]

def make_cluster_dict(row_labels, node_names):
    cluster_dict = defaultdict(list)
    for idx, label in enumerate(row_labels):
        cluster_dict[label].append(node_names[idx])
    return cluster_dict

def collect_top_enrichments(cancer_type, tag="bio", source="REAC", top_n=10):
    base_path = Path("results/gene_prediction/enrichment")
    base_path.mkdir(parents=True, exist_ok=True)
    terms = []

    for cluster_id in range(10):  # Clusters 0–9
        file = base_path / f"{cancer_type}_{tag}_Cluster_{cluster_id}_{source}_enrichment.csv"
        if not file.exists():
            continue

        df = pd.read_csv(file).sort_values("p_value").head(top_n)
        for _, row in df.iterrows():
            term = f"{row['name']} (C{cluster_id})"
            score = -np.log10(row["p_value"] + 1e-10)
            terms.append((term, score, cluster_id))

    return sorted(terms, key=lambda x: x[1], reverse=True)

def draw_horizontal_bar_plot(terms, cancer_type, source):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    # Sort and select top 20 terms by score
    terms_sorted = sorted(terms, key=lambda x: x[1], reverse=True)[:20]

    labels = [t[0] for t in terms_sorted]
    scores = [t[1] for t in terms_sorted]
    clusters = [t[2] for t in terms_sorted]
    colors = [CLUSTER_COLORS[c] for c in clusters]

    fig, ax = plt.subplots(figsize=(12, 0.5 * len(terms_sorted)))
    y_pos = np.arange(len(terms_sorted))

    ax.barh(y_pos, scores, color=colors, edgecolor='black')
    ax.set_yticks(y_pos)
    ax.set_yticklabels(labels, fontsize=18)  # Larger font size
    ax.invert_yaxis()  # Highest scores on top
    ax.set_xlabel("-log10(p-value)", fontsize=18)
    ax.set_title(f"Top Enriched Pathways — {cancer_type.upper()} ({source})", fontsize=18)
    ax.tick_params(axis='x', labelsize=16) 
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.9, bottom=0.1) 

    out_path = Path(f"results/gene_prediction/{cancer_type}_{source}_bar_plot.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"✅ Saved bar plot: {out_path}")

def collect_enrichment_with_ratios(cancer_type, tag="bio", source="REAC", top_n=10):


    base_path = Path("results/gene_prediction/enrichment")
    base_path.mkdir(parents=True, exist_ok=True)
    terms = []

    for cluster_id in range(10):  # Clusters 0–9
        file = base_path / f"{cancer_type}_{tag}_Cluster_{cluster_id}_{source}_enrichment.csv"
        if not file.exists():
            continue

        df = pd.read_csv(file).sort_values("p_value").head(top_n)
        for _, row in df.iterrows():
            raw_name = row['name']
            # Always define `term`, truncate if needed
            term = raw_name if len(raw_name) <= 60 else raw_name[:57] + "..."
            pval = row['p_value']
            intersection = row['intersection_size']
            input_size = row.get('effective_domain_size', 1)  # Prevent zero division
            gene_ratio = intersection / input_size if input_size > 0 else 0
            terms.append((term, gene_ratio, -np.log10(pval + 1e-10), cluster_id))


    return sorted(terms, key=lambda x: x[2], reverse=True)

def draw_dot_plot_with_ratio(terms, cancer_type, source, top_n=20):
    if not terms:
        print(f"No enrichment terms for {cancer_type.upper()} ({source})")
        return

    import seaborn as sns
    import matplotlib.pyplot as plt
    import pandas as pd

    df = pd.DataFrame(terms, columns=["Term", "GeneRatio", "LogP", "Cluster"])
    df["Color"] = df["Cluster"].map(CLUSTER_COLORS)

    # 🔢 Select top N by LogP
    df = df.sort_values("LogP", ascending=False).head(top_n)
    
    plt.figure(figsize=(10, 0.4 * len(df)))
    scatter = sns.scatterplot(
        data=df,
        x="GeneRatio", y="Term",
        size="LogP", hue="Cluster",
        palette=CLUSTER_COLORS,
        sizes=(50, 300),
        edgecolor="black",
        linewidth=0.5
    )

    # Remove legend
    if scatter.legend_:
        scatter.legend_.remove()
        
    #plt.xlabel("Gene Ratio", fontsize=16)
    plt.ylabel("Enriched Pathway", fontsize=18)
    plt.title(f"Dot Plot (Ratio) — {cancer_type.upper()} ({source})", fontsize=18)
    plt.xscale("log")
    plt.xticks(fontsize=18)
    plt.xlabel("Gene Ratio (log scale)", fontsize=18)

    plt.yticks(fontsize=18)
    plt.grid(axis='x', linestyle='--', alpha=0.5)
    plt.tight_layout()

    out_path = Path(f"results/gene_prediction/enrichment/{cancer_type}_{source}_dot_plot_ratio.png")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=300)
    plt.close()
    print(f"✅ Saved dot plot with gene ratio: {out_path}")

def eigengap_analysis_ori(feature_matrix, max_clusters=25, normalize=True, plot_path=None):
    # Step 1: Normalize features (optional)
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, d = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Print eigenvalues
    print("Eigenvalues:\n", eigenvals)

    # Save eigenvalues to CSV
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({"Eigenvalue Index": np.arange(len(eigenvals)), "Eigenvalue": eigenvals}).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 5: Optional plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]
        
        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        gaps = np.diff(eigenvals[1:max_clusters + 1])
        n_clusters_row = np.argmax(gaps) + 1  # +1 because diff shifts index

        plt.axvline(
            x=n_clusters_row,
            color='pink',
            linestyle='--',
            label=f'Eigengap → k={n_clusters_row}'
        )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=14)

        # Set integer x-axis ticks
        plt.xticks(range(1, max_clusters + 1))

        # Remove grid
        plt.grid(False)

        # Remove legend frame
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)

        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()
    else:
        # If no plot is saved, still compute n_clusters_row
        gaps = np.diff(eigenvals[1:max_clusters + 1])
        n_clusters_row = np.argmax(gaps) + 1

    return n_clusters_row, eigenvals

def eigengap_analysis_(feature_matrix, max_clusters=25, normalize=True, plot_path=None):
    # Step 1: Normalize features (optional)
    '''if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)'''

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, d = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Print eigenvalues
    print("Eigenvalues:\n", eigenvals)

    # Save eigenvalues to CSV
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({"Eigenvalue Index": np.arange(len(eigenvals)), "Eigenvalue": eigenvals}).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 5: Compute eigengaps and n_clusters_row, ensuring k > 1
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    n_clusters_row = np.argmax(gaps) + 5  # +1 for diff shift, +1 to ensure k > 1

    # Clamp to max_clusters
    n_clusters_row = max(5, min(n_clusters_row, max_clusters))

    # Optional plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]
        
        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        plt.axvline(
            x=n_clusters_row,
            color='pink',
            linestyle='--',
            label=f'Eigengap → k={n_clusters_row}'
        )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=14)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)

        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)

        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return n_clusters_row, eigenvals

def plot_bio_clusterwise_feature_contributions(
    args,
    relevance_scores,           # 2D array (samples x features)
    row_labels,             # 1D array of cluster assignments
    feature_names,              # List of feature names (e.g., MF: BRCA, ...)
    per_cluster_feature_contributions_output_dir, 
    omics_colors):             # Dict of omics type colors (e.g., 'mf': '#D62728')
    
    import os
    import numpy as np
    import matplotlib.pyplot as plt
    from collections import defaultdict

    os.makedirs(per_cluster_feature_contributions_output_dir, exist_ok=True)

    def get_omics_color(feature_name):
        prefix = feature_name.split(":")[0].lower()
        return omics_colors.get(prefix, "#AAAAAA")

    def get_omics_prefix(feature_name):
        return feature_name.split(":")[0].lower()

    # Group features by omics type and preserve their indices
    omics_groups = defaultdict(list)
    for idx, fname in enumerate(feature_names):
        omics_groups[get_omics_prefix(fname)].append((idx, fname))

    # Follow omics_colors ordering if possible
    ordered_features = []
    for omics in omics_colors:
        ordered_features.extend(omics_groups.get(omics, []))
    for omics in omics_groups:
        if omics not in omics_colors:
            ordered_features.extend(omics_groups[omics])

    ordered_indices = [idx for idx, _ in ordered_features]
    ordered_feature_names = [name for _, name in ordered_features]

    unique_clusters = np.unique(row_labels)

    for cluster_id in sorted(unique_clusters):
        indices = np.where(row_labels == cluster_id)[0]
        cluster_scores = relevance_scores[indices]
        avg_contribution = np.mean(cluster_scores, axis=0)
        total_score = np.sum(avg_contribution)

        fig, ax = plt.subplots(figsize=(10, 2.5))

        x = np.linspace(0, 1, len(ordered_feature_names))
        bar_width = 1 / len(ordered_feature_names) * 0.95

        bars = ax.bar(
            x,
            avg_contribution[ordered_indices],
            width=bar_width,
            color=[get_omics_color(name) for name in ordered_feature_names],
            align='center'
        )

        ax.set_title(
            fr"Cluster {cluster_id} $\mathregular{{({len(indices)}\ genes,\ avg = {total_score:.2f})}}$",
            fontsize=14
        )

        clean_labels = [name.split(":")[1].strip() if ":" in name else name for name in ordered_feature_names]
        ax.set_xticks(x)
        ax.set_xticklabels(clean_labels, rotation=90)

        for label, feature_name in zip(ax.get_xticklabels(), ordered_feature_names):
            label.set_color(get_omics_color(feature_name))

        ax.tick_params(axis='x', labelsize=9)
        ax.set_xlim(-bar_width, 1 + bar_width)

        plt.tight_layout()
        save_path = os.path.join(
            per_cluster_feature_contributions_output_dir,
            f"{args.model_type}_{args.net_type}_BIO_cluster_{cluster_id}_feature_contributions_epo{args.num_epochs}.png"
        )
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"✅ Saved BIO feature contribution barplot for Cluster {cluster_id} to {save_path}")

def save_graph_with_clusters(graph, save_path):
    torch.save({
        'edges': graph.edges(),
        'features': graph.ndata['feat'],
        'labels': graph.ndata.get('label', None),
        'cluster_bio_summary': graph.ndata['cluster_bio_summary']
    }, save_path)

def compute_total_genes_per_cluster(row_labels, n_clusters):
    return {i: np.sum(row_labels == i) for i in range(n_clusters)}

def compute_relevance_scores_norm(
    model,
    graph,
    features,
    node_indices,
    normalize=True,
    feature_groups=None):  # e.g., {"bio": (0, 1024), "topo": (1024, 2048)}):
    """
    Compute saliency-based relevance scores with optional normalization and feature group selection.

    Args:
        model (torch.nn.Module): Your trained GNN model.
        graph (DGLGraph): The graph structure.
        features (torch.Tensor): Node feature matrix (N x F).
        node_indices (list[int]): Node indices to compute relevance for.
        normalize (bool): Whether to normalize saliency per node.
        feature_groups (dict): Dict of feature group name → (start, end) slice indices.

    Returns:
        dict: node_idx → dict of {group_name: relevance_tensor} or "all": full saliency
    """
    model.eval()
    features = features.clone().detach().requires_grad_(True)

    output = model(graph, features)
    relevance_dict = {}

    for node_idx in node_indices:
        model.zero_grad()
        node_score = output[node_idx].squeeze()
        node_score.backward(retain_graph=True)

        saliency = features.grad[node_idx].detach().abs()  # (F,)
        
        if normalize:
            saliency = saliency / (saliency.sum() + 1e-9)

        if feature_groups:
            group_scores = {}
            for group_name, (start, end) in feature_groups.items():
                group_scores[group_name] = saliency[start:end]
            relevance_dict[node_idx] = group_scores
        else:
            relevance_dict[node_idx] = {"all": saliency}

        features.grad.zero_()

    return relevance_dict

def count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters):
    pred_counts = {i: 0 for i in range(n_clusters)}
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    predicted_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]
    for idx in predicted_indices:
        if 0 <= idx < len(row_labels):
            pred_counts[row_labels[idx]] += 1
    return pred_counts, predicted_indices

def plot_bio_heatmap_unsort_no_legend_patches(summary_bio_features, row_labels, col_labels, predicted_indices, output_path):
    from matplotlib.colors import LinearSegmentedColormap
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    bio_feat_names_64 = [f"{omics}_{cancer}" for omics in ['cna', 'ge', 'meth', 'mf'] for cancer in cancer_names]
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_bio_features[predicted_indices]
    row_labels_topk = row_labels[predicted_indices]
    sorted_indices = np.argsort(row_labels_topk)
    sorted_matrix = summary_topk[sorted_indices, :]
    sorted_labels = row_labels_topk[sorted_indices]

    # === Heatmap layout with narrower cluster bar and no gap
    fig = plt.figure(figsize=(17, 17))
    gs = fig.add_gridspec(nrows=1, ncols=2, width_ratios=[0.25, 13.7], wspace=0)

    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])

    # === Cluster color bar
    cluster_colors = [to_rgb(CLUSTER_COLORS[label]) for label in sorted_labels]
    cluster_colors_array = np.array(cluster_colors).reshape(-1, 1, 3)
    ax_cluster.imshow(cluster_colors_array, aspect='auto')
    ax_cluster.axis("off")

    cluster_counts = Counter(sorted_labels)
    start_idx = 0
    for cluster_id in sorted(cluster_counts):
        count = cluster_counts[cluster_id]
        mid_idx = start_idx + count // 2
        ax_cluster.text(-0.75, mid_idx, f'{count}', va='center', ha='right',
                        fontsize=14, fontweight='bold', color='black')  # larger cluster number
        start_idx += count

        # === Heatmap
        bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
        sns.heatmap(sorted_matrix, cmap=bluish_gray_gradient, center=0, vmin=-2, vmax=2,
                    ax=ax_heatmap, cbar=False)
        ax_heatmap.set_title("Heatmap of Summary Bio Features Sorted by Spectral Biclusters", fontsize=18)
        ax_heatmap.set_yticks([])

        xticks = np.arange(len(bio_feat_names_64)) + 0.5
        ax_heatmap.set_xticks(xticks)

        # Set only cancer names, big and bold
        cancer_labels_only = [name.split('_')[1] for name in bio_feat_names_64]
        ax_heatmap.set_xticklabels(cancer_labels_only, rotation=90, fontsize=18)#, weight='bold')

        # Color each tick label based on omics type
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }
        for tick_label, name in zip(ax_heatmap.get_xticklabels(), bio_feat_names_64):
            omics = name.split('_')[0]
            tick_label.set_color(omics_colors.get(omics, 'black'))

        ax_heatmap.set_xlabel("Summary Bio Features (Grouped by Omics)", fontsize=18)
        fig.tight_layout()
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        fig.savefig(output_path, dpi=300)
        plt.close(fig)
        print(f"✅ Summary bio heatmap saved to {output_path}")

def plot_tsne(features, row_labels, predicted_indices, n_clusters, output_path):
    tsne = TSNE(n_components=2, perplexity=min(30, len(features) - 1), random_state=42)
    reduced_embeddings = tsne.fit_transform(features)
    plt.figure(figsize=(12, 10))
    for cluster_id in range(n_clusters):
        idx = np.where(row_labels == cluster_id)[0]
        plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1],
                    color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                    edgecolor='k', s=100, alpha=0.8)
    for idx in predicted_indices:
        x, y = reduced_embeddings[idx]
        plt.scatter(x, y, facecolors='none', edgecolors='red', s=50, linewidths=2)
    plt.xlabel("t-SNE Dimension 1", fontsize=18)
    plt.ylabel("t-SNE Dimension 2", fontsize=18)
    plt.title("t-SNE of Spectral Biclustering (Summary Bio Features)")
    plt.savefig(output_path, bbox_inches="tight")
    plt.close()
    print(f"✅ t-SNE visualization saved to {output_path}")

def plot_bio_heatmap_raw(summary_bio_features, predicted_indices, save_path):
    """
    Plot raw (unclustered) heatmap of summary bio features for predicted genes,
    with omics bar and consistent styling.

    Parameters:
    - summary_bio_features: np.ndarray, shape (num_nodes, 64)
    - predicted_indices: list of indices for predicted cancer genes
    - save_path: path to save the heatmap PNG
    """
    print("📊 Plotting raw bio heatmap (unclustered)...")
    assert summary_bio_features.shape[1] == 64, "Expected 64 summary bio features."

    # === Select only predicted cancer genes
    data = summary_bio_features[predicted_indices]

    # === Omics group boundaries for 64-dim summary bio features
    omics_group_sizes = {
        "expression": 16,
        "methylation": 16,
        "mutation": 16,
        "copy_number": 16
    }
    omics_colors = {
        "expression": "#76D7C4",
        "methylation": "#F7DC6F",
        "mutation": "#F5B7B1",
        "copy_number": "#C39BD3"
    }

    # === Feature names
    feature_names = [f"{omics}_{i}" for omics, size in omics_group_sizes.items() for i in range(size)]
    omics_color_bar = []
    for group, size in omics_group_sizes.items():
        omics_color_bar.extend([omics_colors[group]] * size)

    # === Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    # === Plotting
    fig = plt.figure(figsize=(14, 10))
    grid_spec = fig.add_gridspec(nrows=2, ncols=1, height_ratios=[0.15, 0.85])
    ax_contrib = fig.add_subplot(grid_spec[0])
    ax_heatmap = fig.add_subplot(grid_spec[1])

    # === Column contribution bar
    contrib = data.sum(axis=0)
    ax_contrib.bar(np.arange(data.shape[1]), contrib, color="#85929e", width=1.0)
    ax_contrib.axis("off")

    # === Heatmap
    sns.heatmap(
        data,
        cmap=bluish_gray_gradient,
        ax=ax_heatmap,
        cbar_kws={"label": "Feature Relevance"},
        xticklabels=feature_names,
        yticklabels=[f"Gene{i}" for i in range(data.shape[0])],
        linewidths=0.2,
        linecolor='gray'
    )
    ax_heatmap.set_xlabel("Biological Features")
    ax_heatmap.set_ylabel("Predicted Cancer Genes")
    ax_heatmap.tick_params(axis='x', rotation=90)

    # === Omics group bar
    for x, color in enumerate(omics_color_bar):
        ax_heatmap.add_patch(plt.Rectangle((x, -1), 1, 0.5, color=color, transform=ax_heatmap.transData, clip_on=False))

    # === Omics legend
    handles = [Patch(facecolor=color, label=label) for label, color in omics_colors.items()]
    ax_heatmap.legend(
        handles=handles,
        title="Omics Group",
        loc='upper right',
        bbox_to_anchor=(1.15, 1.0),
        frameon=True
    )

    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"✅ Raw bio heatmap saved to: {save_path}")

def plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path):
    """
    Plot unclustered raw heatmap of 64-dim summary bio features for top predicted genes.
    Features are grouped into 4 omics types x 16 cancers and are color-coded on x-axis.

    Parameters:
    - summary_bio_features: np.ndarray of shape (num_nodes, 64)
    - predicted_indices: list or array of top predicted gene indices
    - output_path: str, path to save the heatmap image
    """
    print("📊 Plotting raw bio heatmap without clustering...")

    # === Top-K predicted genes to show
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_bio_features[predicted_indices]

    # === Bio feature labels (4 omics × 16 cancers)
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    bio_feat_names_64 = [f"{omics}_{cancer}" for omics in ['cna', 'ge', 'meth', 'mf'] for cancer in cancer_names]

    # === Figure layout
    fig = plt.figure(figsize=(17, 18))
    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[17, 2], width_ratios=[0.25, 13.7], hspace=0.3, wspace=0.00)

    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])
    ax_legend = fig.add_subplot(gs[1, :])
    ax_legend.axis('off')

    # === Fake cluster panel (empty) for visual consistency
    ax_cluster.axis("off")

    # === Colormap and heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    sns.heatmap(summary_topk, cmap=bluish_gray_gradient, center=0, vmin=-2, vmax=2,
                ax=ax_heatmap, cbar=False)

    ax_heatmap.set_title("Raw Heatmap of Summary Bio Features (Top Predicted Genes)", fontsize=18)
    ax_heatmap.set_yticks([])

    # === X-axis feature names (with omics coloring)
    xticks = np.arange(len(bio_feat_names_64)) + 0.5
    ax_heatmap.set_xticks(xticks)
    cancer_labels_only = [name.split('_')[1] for name in bio_feat_names_64]
    ax_heatmap.set_xticklabels(cancer_labels_only, rotation=90, fontsize=18)

    # === Omics coloring for x-tick labels
    omics_colors = {'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222'}
    for tick_label, name in zip(ax_heatmap.get_xticklabels(), bio_feat_names_64):
        omics = name.split('_')[0]
        tick_label.set_color(omics_colors.get(omics, 'black'))

    # === Legend bar for omics types
    omics_patches = [Patch(color=color, label=omics.upper()) for omics, color in omics_colors.items()]
    ax_legend.legend(handles=omics_patches, loc='center', ncol=len(omics_patches),
                     fontsize=18, frameon=False)

    # === Final adjustments and save
    fig.subplots_adjust(left=0.03, right=0.99, top=0.95, bottom=0.03, hspace=0.2, wspace=0.01)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close(fig)

    print(f"✅ Raw bio heatmap saved to {output_path}")

def plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path):
    """
    Plot unclustered raw heatmap of 64-dim topological summary features for top predicted genes.

    Parameters:
    - summary_topo_features: np.ndarray of shape (num_nodes, 64)
    - predicted_indices: list or array of top predicted gene indices
    - output_path: str, path to save the heatmap image
    """
    print("📊 Plotting raw topo heatmap without clustering...")

    # === Top-K predicted genes to show
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_topo_features[predicted_indices]

    # === Topo feature labels: "00" to "63"
    topo_feat_names_64 = [f"{i:02d}" for i in range(64)]

    # === Layout
    fig = plt.figure(figsize=(17, 18))
    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[17, 2], width_ratios=[0.25, 13.7], hspace=0.3, wspace=0.00)
    
    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])
    ax_legend = fig.add_subplot(gs[1, :])
    ax_legend.axis('off')

    # === Empty cluster column for layout consistency
    ax_cluster.axis("off")

    # === Heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    sns.heatmap(
        summary_topk,
        cmap=bluish_gray_gradient,
        center=0,
        vmin=-2, vmax=2,
        ax=ax_heatmap,
        cbar=False
    )
    ax_heatmap.set_title("Raw Topological Feature Heatmap (Top Predicted Genes)", fontsize=18, pad=12)
    ax_heatmap.set_yticks([])

    xticks = np.arange(len(topo_feat_names_64)) + 0.5
    ax_heatmap.set_xticks(xticks)
    ax_heatmap.set_xticklabels(topo_feat_names_64, rotation=90, fontsize=12)

    # === Save
    fig.subplots_adjust(left=0.03, right=0.99, top=0.95, bottom=0.05)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close(fig)

    print(f"✅ Raw topo heatmap saved to {output_path}")

def apply_full_spectral_biclustering_bio_(graph, summary_bio_features, node_names,
                                         predicted_cancer_genes, n_clusters,
                                         save_path, save_row_labels_path,
                                         save_total_genes_per_cluster_path, save_predicted_counts_path,
                                         output_path_genes_clusters, output_path_heatmap):

    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    # === Normalize features
    summary_bio_features = StandardScaler().fit_transform(summary_bio_features)

    # === Spectral biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_bio_features)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    graph.ndata['cluster_bio_summary'] = torch.tensor(row_labels, dtype=torch.long)
    
    print("✅ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === Save original (unclustered) heatmap before biclustering
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    #plot_bio_heatmap_raw(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)    
    plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)



def plot_topo_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    """
    Plots a spectral biclustering heatmap for topological embeddings (1024–2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # 🔹 Extract 64D summary of topological features
    relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # 🔹 Create topo feature names (01–64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # 🔹 Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # 🔹 Sort by cluster → then by gene-wise relevance within cluster
    sorted_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        sorted_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[sorted_indices]
    sorted_clusters = row_labels[sorted_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in sorted_indices]

    # 🔹 Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # 🔹 Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # 🔹 Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # 🔹 Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # 🔹 Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # 🔹 Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # 🔹 Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # 🔹 Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved spectral clustering heatmap to {output_path}")

    # 🔹 Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path):

    topo_feat_names_64 = [f"{i:02d}" for i in range(64)]
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_topo_features[predicted_indices]
    row_labels_topk = row_labels[predicted_indices]
    sorted_indices = np.argsort(row_labels_topk)
    sorted_matrix = summary_topk[sorted_indices, :]
    sorted_labels = row_labels_topk[sorted_indices]

    fig = plt.figure(figsize=(17, 18))
    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[17, 2], width_ratios=[0.25, 13.7], hspace=0.3, wspace=0.00)
    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])
    ax_legend = fig.add_subplot(gs[1, :])
    ax_legend.axis('off')

    cluster_colors = [to_rgb(CLUSTER_COLORS[label]) for label in sorted_labels]
    cluster_colors_array = np.array(cluster_colors).reshape(-1, 1, 3)
    ax_cluster.imshow(cluster_colors_array, aspect='auto')
    ax_cluster.axis("off")

    # Add cluster size labels
    cluster_counts = Counter(sorted_labels)
    start_idx = 0
    for cluster_id in sorted(cluster_counts):
        count = cluster_counts[cluster_id]
        mid_idx = start_idx + count // 2
        ax_cluster.text(-0.75, mid_idx, f'{count}', va='center', ha='right',
                        fontsize=14, fontweight='bold', color='black')
        start_idx += count

    # Heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    sns.heatmap(
        sorted_matrix,
        cmap=bluish_gray_gradient,
        center=0,
        vmin=-2, vmax=2,
        ax=ax_heatmap,
        cbar=False
    )
    ax_heatmap.set_title("Topological Feature Heatmap (Sorted by Cluster)", fontsize=18, pad=12)
    ax_heatmap.set_yticks([])

    xticks = np.arange(len(topo_feat_names_64)) + 0.5
    ax_heatmap.set_xticks(xticks)
    ax_heatmap.set_xticklabels(topo_feat_names_64, rotation=90, fontsize=12)

    # Save
    fig.subplots_adjust(left=0.03, right=0.99, top=0.95, bottom=0.05)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"✅ Summary topo heatmap saved to {output_path}")

def plot_bio_heatmap_unsort(
    summary_bio_features, 
    row_labels, 
    col_labels, 
    predicted_indices, 
    output_path
    ):

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    bio_feat_names_64 = [f"{omics}_{cancer}" for omics in ['cna', 'ge', 'meth', 'mf'] for cancer in cancer_names]
    topk = 1000
    predicted_indices = predicted_indices[:topk]
    summary_topk = summary_bio_features[predicted_indices]
    row_labels_topk = row_labels[predicted_indices]
    sorted_indices = np.argsort(row_labels_topk)
    sorted_matrix = summary_topk[sorted_indices, :]
    sorted_labels = row_labels_topk[sorted_indices]

    fig = plt.figure(figsize=(17, 18))
    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[17, 2], width_ratios=[0.25, 13.7], hspace=0.3, wspace=0.00)
    ax_cluster = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[0, 1])
    ax_legend = fig.add_subplot(gs[1, :])
    ax_legend.axis('off')

    cluster_colors = [to_rgb(CLUSTER_COLORS[label]) for label in sorted_labels]
    cluster_colors_array = np.array(cluster_colors).reshape(-1, 1, 3)
    ax_cluster.imshow(cluster_colors_array, aspect='auto')
    ax_cluster.axis("off")

    cluster_counts = Counter(sorted_labels)
    start_idx = 0
    for cluster_id in sorted(cluster_counts):
        count = cluster_counts[cluster_id]
        mid_idx = start_idx + count // 2
        ax_cluster.text(-0.75, mid_idx, f'{count}', va='center', ha='right',
                        fontsize=14, fontweight='bold', color='black')
        start_idx += count

    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    sns.heatmap(sorted_matrix, cmap=bluish_gray_gradient, center=0, vmin=-2, vmax=2,
                ax=ax_heatmap, cbar=False)
    ax_heatmap.set_title("Heatmap of Summary Bio Features Sorted by Spectral Biclusters", fontsize=18)
    ax_heatmap.set_yticks([])

    xticks = np.arange(len(bio_feat_names_64)) + 0.5
    ax_heatmap.set_xticks(xticks)
    cancer_labels_only = [name.split('_')[1] for name in bio_feat_names_64]
    ax_heatmap.set_xticklabels(cancer_labels_only, rotation=90, fontsize=18)

    omics_colors = {'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222'}
    for tick_label, name in zip(ax_heatmap.get_xticklabels(), bio_feat_names_64):
        omics = name.split('_')[0]
        tick_label.set_color(omics_colors.get(omics, 'black'))

    omics_patches = [Patch(color=color, label=omics.upper()) for omics, color in omics_colors.items()]
    ax_legend.legend(handles=omics_patches, loc='center', ncol=len(omics_patches),
                     fontsize=18, frameon=False)

    fig.subplots_adjust(left=0.03, right=0.99, top=0.95, bottom=0.03, hspace=0.2, wspace=0.01)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"✅ Summary bio heatmap saved to {output_path}")

def plot_bio_biclustering_heatmap_unsort_random_clusterbar(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):  
    
    # 🔹 Extract and normalize relevance scores
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If col_labels provided, reorder columns accordingly
    # if col_labels is not None:
    #     sorted_order = np.argsort(col_labels)
    #     relevance_scores = relevance_scores[:, sorted_order]
    #     feature_names = [feature_names[i] for i in sorted_order]

    # Build feature color bar
    feature_colors = []
    for i in range(len(feature_names)):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")  # fallback color

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.6)

    # Normalize per-feature means
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(relevance_scores, 99)

    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripe
    for i, cluster in enumerate(row_labels):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(row_labels, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # Add xtick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # LRP curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = relevance_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)

    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(relevance_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]
        )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_clusters_unsort_(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)

    # Normalize to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',  # purple
            'ge': '#228B22',   # green
            'meth': '#00008B', # blue
            'mf': '#b22222',   # red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Sort by cluster only (no intra-cluster sorting)
    ##cluster_order = np.argsort(row_labels)
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    # Keep original feature (column) order
    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_colors = []

    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    feature_names = [feature_names[i] for i in original_col_indices]

    # Colormap setup
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # Top bar per feature
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(x=i + 0.5, height=val, width=1.0, color=color, edgecolor='black', linewidth=0.5, alpha=0.3 + 0.7 * val)

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # Feature tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve (LRP sum)
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(np.arange(len(saliency_sums)), 0, saliency_sums, color='#a9cce3', alpha=0.8)#, linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bar
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            #alpha=0.3 + 0.7 * norm_mean
            alpha = min(1.0, 0.3 + 0.7 * norm_mean)

        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_topo_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):

    # 🔹 Extract 64D summary of topological features
    relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    #relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    
    # 🔹 Create topo feature names (01–64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # 🔹 Sort by cluster → then by gene-wise relevance within cluster
    sorted_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        sorted_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[sorted_indices]
    sorted_clusters = row_labels[sorted_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in sorted_indices]

    # 🔹 Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # 🔹 Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    ax_legend = fig.add_subplot(gs[14, 2:45])

    # 🔹 Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # 🔹 Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # 🔹 Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # 🔹 Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # 🔹 Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Omics + LRP Legend
    ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )

    # 🔹 Saliency Sum curve
    lrp_sums = sorted_scores.sum(axis=1)
    lrp_sums = (lrp_sums - lrp_sums.min()) / (lrp_sums.max() - lrp_sums.min())
    y = np.arange(len(lrp_sums))

    ax_curve.fill_betweenx(
        y, 0, lrp_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(lrp_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved spectral clustering heatmap to {output_path}")

    # 🔹 Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_topo_heatmap_unsorted(
    args,
    relevance_scores,
    output_path,
    gene_names=None,
    col_labels=None
):
    """
    Plots a topological heatmap (unsorted) using the same visual formatting
    as the biclustering version, but with rows and columns in original order.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.
        col_labels (list of str, optional): Optional column label annotations.
    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # 🔹 Extract 64D summary of topological features
    relevance_scores = extract_summary_features_np_topo(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # 🔹 Feature names
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]
    if col_labels is not None:
        col_labels = np.array(col_labels)

    # 🔹 Apply log1p for better contrast
    scores_log = np.log1p(relevance_scores)

    # 🔹 Set up figure
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # 🔹 Colorbar range
    vmin, vmax = 0, np.percentile(scores_log, 99)

    # 🔹 Feature contribution bar (column saliency)
    feature_means = scores_log.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5,
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )
    ax_bar.set_xlim(0, len(feature_means))
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_xticks([])
    ax_bar.set_yticks([])
    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)

    # 🔹 Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    # 🔹 Heatmap
    sns.heatmap(
        scores_log,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Tick labels (X)
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    # 🔹 Omics/Saliency Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # 🔹 Saliency sum curve (row-wise sum)
    saliency_sums = scores_log.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ['right', 'left', 'bottom', 'top']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Final layout
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved unsorted topological heatmap to {output_path}")

    return pd.DataFrame(scores_log, index=gene_names, columns=feature_names)

def plot_topo_biclustering_heatmap_unsorted_(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    """
    Unsorted version of topo spectral biclustering heatmap — preserves original feature and gene order.
    """
    # 🔹 Extract 64D summary of topological features
    relevance_scores = extract_summary_features_np_topo(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # 🔹 Create topo feature names (01–64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    if col_labels is not None:
        col_labels = np.array(col_labels)

    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    # 🔹 No sorting of rows or columns
    scores = relevance_scores
    clusters = row_labels


    if gene_names is not None:
        gene_names = list(gene_names)

    # 🔹 Compute cluster boundaries and centers
    _, counts = np.unique(clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]


    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # 🔹 Apply log transformation
    scores = np.log1p(scores)

    # 🔹 Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )


    # 🔹 Feature-wise average bar (same order)
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0,
               color="#B0BEC5", linewidth=0, alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # 🔹 Heatmap intensity scaling
    vmin, vmax = 0, np.percentile(scores, 99)

    sns.heatmap(
        scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster color stripes
    for i, cluster in enumerate(clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Cluster size labels
    for cluster_id, center_y, count in zip(np.unique(clusters), cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # 🔹 Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # 🔹 Saliency Sum Curve
    saliency_sums = scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(y, 0, saliency_sums,
                           color='#a9cce3', alpha=0.8, linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.05, xmin=0, xmax=1,
                    color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ax_curve.spines.values():
        spine.set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Save figure
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved unsorted topo clustering heatmap to {output_path}")

    # 🔹 Optional: Per-cluster contribution plot
    '''plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # original, not sorted
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )'''

    return pd.DataFrame(scores, index=gene_names, columns=feature_names)

def plot_topo_biclustering_heatmap_clusters_unsort(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)

    # Normalize to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())# * 10

    # if topo_colors is None:
    #     topo_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'] * 16  # Adjust to match 64 features

    # 🔹 Generate feature names (01 to 64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]
    row_labels = np.array(row_labels)
    
    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    #feature_colors = topo_colors[:len(feature_names)]

    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    
    ax_bar    = fig.add_subplot(gs[0, 2:45])
    ax        = fig.add_subplot(gs[1:13, 2:45])
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # 🔹 Log-transform scores
    scores = np.log1p(relevance_scores)

    # 🔹 Color map
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # 🔹 Feature mean bar
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0, color="#B0BEC5", alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # 🔹 Heatmap
    vmin, vmax = 0, np.percentile(scores, 99)
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1, 
            linewidth=0, 
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), 
            clip_on=False
        ))

    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size
    
    # 🔹 X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # 🔹 Saliency Sum Curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 
        
        0, 
        saliency_sums,
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)
    
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # 🔹 Save figure
    plt.subplots_adjust(wspace=0, hspace=0)
    
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved unsorted topo clustering heatmap to {output_path}")

def plot_bio_biclustering_heatmap_clusters_unsort(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)

    # Normalize to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',  # purple
            'ge': '#228B22',   # green
            'meth': '#00008B', # blue
            'mf': '#b22222',   # red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", 
        ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    # Grid layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])       # top bar
    ax        = fig.add_subplot(gs[1:13, 2:45])     # main heatmap
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)  # saliency curve
    ax_cbar   = fig.add_subplot(gs[5:9, 49])       # colorbar

    # Top feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels with omics colors
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)


    # Saliency curve 
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)
    
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars (right below feature bar, above heatmap)
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):  
    
    # 🔹 Extract and normalize relevance scores
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If col_labels provided, reorder columns accordingly
    # if col_labels is not None:
    #     sorted_order = np.argsort(col_labels)
    #     relevance_scores = relevance_scores[:, sorted_order]
    #     feature_names = [feature_names[i] for i in sorted_order]

    # 🔹 Feature color mapping
    '''feature_colors = []
    for i in range(len(feature_names)):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")  # fallback color'''

    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    # Normalize per-feature means
    #feature_means = relevance_scores.mean(axis=0)
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Sort by cluster only (no intra-cluster sorting)
    '''cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]'''

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1, 
            linewidth=0, 
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), 
            clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # 🔹 Add cluster size labels
    # for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
    #     ax.text(
    #         -2.0, center_y, f"{count}",
    #         va='center', ha='right', fontsize=18, fontweight='bold'
    #     )
        
    # Add xtick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = relevance_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    '''mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)

    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(relevance_scores) + 1.5,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]
        )'''

    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )
        
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_topo_biclustering_heatmap_unsorted(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    """
    Unsorted topo heatmap with connected cluster bar and gene count labels.
    """
    from matplotlib.patches import Rectangle

    # 🔹 Normalize summary topo features (0-1)
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # 🔹 Generate feature names (01 to 64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]
    row_labels = np.array(row_labels)

    sorted_indices = np.argsort(row_labels)
    row_labels = row_labels[sorted_indices]
    
    # 🔹 Compute cluster stats
    _, counts = np.unique(row_labels, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # 🔹 Log-transform scores
    scores = np.log1p(relevance_scores)

    # 🔹 Color map
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # 🔹 Feature mean bar
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0, color="#B0BEC5", alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # 🔹 Heatmap
    vmin, vmax = 0, np.percentile(scores, 99)
    sns.heatmap(
        scores, cmap=bluish_gray_gradient, vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster bar
    for i, cluster in enumerate(row_labels):
        ax.add_patch(Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Add cluster counts
    for cluster_id, center_y, count in zip(np.unique(row_labels), cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # 🔹 Saliency sum curve
    saliency_sums = scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.05, xmin=0, xmax=1,
                    color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ax_curve.spines.values():
        spine.set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Save plot
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0.02, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved unsorted topo clustering heatmap to {output_path}")

    return pd.DataFrame(scores, index=gene_names, columns=feature_names)

def best_balanced_biclustering_(data, n_clusters, n_trials=20):
    best_model = None
    best_balance_score = float('inf')

    for seed in range(n_trials):
        model = SpectralBiclustering(
            n_clusters=n_clusters,
            method='log',
            svd_method='arpack',
            n_best=3,
            random_state=seed
        )
        model.fit(data)
        _, counts = np.unique(model.row_labels_, return_counts=True)
        balance_score = counts.max() - counts.min()
        if balance_score < best_balance_score:
            best_balance_score = balance_score
            best_model = model
    return best_model

def best_balanced_biclustering_(data, n_clusters, n_trials=20):
    from sklearn.cluster import SpectralBiclustering
    import numpy as np

    n_rows, n_cols = data.shape
    max_clusters = min(n_rows, n_cols) - 1
    if n_clusters >= max_clusters:
        print(f"⚠️ Reducing n_clusters from {n_clusters} to {max_clusters} to satisfy SVD requirements.")
        n_clusters = max_clusters

    best_model = None
    best_balance_score = float('inf')

    for seed in range(n_trials):
        try:
            model = SpectralBiclustering(
                n_clusters=n_clusters,
                method='log',
                svd_method='arpack',
                n_best=3,
                random_state=seed
            )
            model.fit(data)
            _, counts = np.unique(model.row_labels_, return_counts=True)
            balance_score = counts.max() - counts.min()
            if balance_score < best_balance_score:
                best_balance_score = balance_score
                best_model = model
        except ValueError as e:
            print(f"Trial {seed} failed with error: {e}")
            continue

    if best_model is None:
        raise RuntimeError("❌ All Spectral Biclustering trials failed. Check input shape or cluster size.")

    return best_model

def best_balanced_biclustering(data, n_clusters, n_trials=20, verbose=False):
    from sklearn.cluster import SpectralBiclustering
    import numpy as np

    n_rows, n_cols = data.shape
    max_clusters = min(n_rows, n_cols) - 1
    if n_clusters >= max_clusters:
        print(f"⚠️ Reducing n_clusters from {n_clusters} to {max_clusters} to satisfy SVD requirements.")
        n_clusters = max_clusters

    best_model = None
    best_balance_score = float('inf')
    best_seed = None

    for seed in range(n_trials):
        try:
            model = SpectralBiclustering(
                n_clusters=n_clusters,
                method='log',
                svd_method='arpack',
                n_best=3,
                random_state=seed
            )
            model.fit(data)
            _, row_counts = np.unique(model.row_labels_, return_counts=True)
            balance_score = row_counts.max() - row_counts.min()

            if verbose:
                print(f"Trial {seed}: cluster sizes = {row_counts}, balance score = {balance_score}")

            if balance_score < best_balance_score:
                best_balance_score = balance_score
                best_model = model
                best_seed = seed

        except ValueError as e:
            if verbose:
                print(f"⚠️ Trial {seed} failed: {e}")
            continue

    if best_model is None:
        raise RuntimeError("❌ All Spectral Biclustering trials failed. Check input shape or cluster size.")

    if verbose:
        print(f"✅ Best model found with seed {best_seed}, balance score = {best_balance_score}")

    return best_model

def plot_bio_biclustering_heatmap_clusters_unsort_descending(
    args,
    relevance_scores,
    row_labels,  # not used now
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize to [0, 10]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',  # purple
            'ge': '#228B22',   # green
            'meth': '#00008B', # blue
            'mf': '#b22222',   # red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = relevance_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", 
        ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])       # top bar
    ax        = fig.add_subplot(gs[1:13, 2:45])     # main heatmap
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)  # saliency curve
    ax_cbar   = fig.add_subplot(gs[5:9, 49])       # colorbar

    # Top feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # X-axis labels with omics colors
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve 
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)
    
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_topo_biclustering_heatmap_clusters_unsort_descending(
    args,
    relevance_scores,
    output_path,
    gene_names=None,
    col_labels=None
):
    # 🔹 Normalize relevance scores to [0, 1]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # 🔹 Generate feature names (01 to 64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # 🔹 Log-transform scores for smoother heatmap
    scores = np.log1p(relevance_scores)

    # 🔹 Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # 🔹 Prepare layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])
    ax        = fig.add_subplot(gs[1:13, 2:45])
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # 🔹 Feature mean bar
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0, color="#B0BEC5", alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # 🔹 Heatmap
    vmin, vmax = 0, np.percentile(scores, 99)
    sns.heatmap(
        scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # 🔹 Saliency Sum Curve
    saliency_sums = scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y,
        0,
        saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5,
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # 🔹 Save figure
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved topo heatmap (unsorted, no clusters) to {output_path}")

def plot_topo_biclustering_heatmap_unsorted_descending(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    """
    Unsorted topo heatmap with connected cluster bar and gene count labels.
    """
    from matplotlib.patches import Rectangle

    # 🔹 Normalize summary topo features (0-1)
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())

    # 🔹 Generate feature names (01 to 64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]
    row_labels = np.array(row_labels)

    sorted_indices = np.argsort(row_labels)
    row_labels = row_labels[sorted_indices]
    
    # 🔹 Compute cluster stats
    _, counts = np.unique(row_labels, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # 🔹 Log-transform scores
    scores = np.log1p(relevance_scores)

    # 🔹 Color map
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # 🔹 Feature mean bar
    feature_means = scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ax_bar.bar(np.arange(len(feature_means)) + 0.5, feature_means, width=1.0, color="#B0BEC5", alpha=0.6)
    ax_bar.set_xticks([]), ax_bar.set_yticks([]), ax_bar.set_xlim(0, len(feature_means)), ax_bar.set_ylim(0, 0.04)
    for spine in ax_bar.spines.values():
        spine.set_visible(False)

    # 🔹 Heatmap
    vmin, vmax = 0, np.percentile(scores, 99)
    sns.heatmap(
        scores, cmap=bluish_gray_gradient, vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster bar
    for i, cluster in enumerate(row_labels):
        ax.add_patch(Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Add cluster counts
    for cluster_id, center_y, count in zip(np.unique(row_labels), cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)
    ax.set_xlabel(""), ax.set_ylabel(""), ax.set_title("")

    # 🔹 Saliency sum curve
    saliency_sums = scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.05, xmin=0, xmax=1,
                    color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ax_curve.spines.values():
        spine.set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Save plot
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0.02, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved unsorted topo clustering heatmap to {output_path}")

    return pd.DataFrame(scores, index=gene_names, columns=feature_names)

def compute_relevance_scores_integrated_gradients_no_progress_bar(
    model, graph, features, node_indices=None,
    baseline=None, steps=50, use_abs=True
):
    """
    Computes relevance scores using Integrated Gradients for selected nodes.

    Args:
        model: Trained GNN model.
        graph: DGL graph.
        features: [N, D] tensor of input features.
        node_indices: Node indices to compute relevance for. If None, auto-select based on sigmoid output > 0.5.
        baseline: Baseline input tensor [N, D] to start integration from. If None, uses zeros.
        steps: Number of steps for the IG path.
        use_abs: Whether to return absolute attributions (recommended).

    Returns:
        relevance_scores: [N, D] tensor with non-zero rows for selected nodes.
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    features = features.clone().detach()

    if baseline is None:
        baseline = torch.zeros_like(features)

    if node_indices is None:
        with torch.no_grad():
            probs = torch.sigmoid(model(graph, features)).squeeze()
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

    relevance_scores = torch.zeros_like(features)

    for idx in node_indices:
        total_grad = torch.zeros_like(features[idx])
        for alpha in torch.linspace(0, 1, steps):
            interpolated = baseline[idx] + alpha * (features[idx] - baseline[idx])
            input_vec = features.clone().detach()
            input_vec[idx] = interpolated
            input_vec.requires_grad_(True)

            logits = model(graph, input_vec)
            prob = torch.sigmoid(logits.squeeze())[idx]

            model.zero_grad()
            if input_vec.grad is not None:
                input_vec.grad.zero_()
            prob.backward(retain_graph=True)

            grad = input_vec.grad[idx]
            if grad is not None:
                total_grad += grad
                # print('idx========\n', idx)
                # print('total_grad---------------------------\n', total_grad)
            else:
                print(f"⚠️ Gradient was None at alpha={alpha.item():.2f}")

        avg_grad = total_grad / steps
        ig = (features[idx] - baseline[idx]) * avg_grad
        relevance_scores[idx] = ig.abs() if use_abs else ig

    return relevance_scores

def plot_cluster_sizes_and_enrichment_(graph, cluster_key, enrichment_dict=None, title_suffix=""):
    import matplotlib.pyplot as plt

    cluster_counts = pd.Series(nx.get_node_attributes(graph, cluster_key)).value_counts().sort_index()
    fig, ax = plt.subplots(figsize=(10, 5))
    cluster_counts.plot(kind="bar", ax=ax)
    ax.set_xlabel("Cluster ID")
    ax.set_ylabel("Number of Genes")
    ax.set_title(f"Cluster Sizes {title_suffix}")
    plt.tight_layout()
    plt.close()

    if enrichment_dict:
        enriched_counts = pd.Series({k: len(v) for k, v in enrichment_dict.items()})
        fig, ax = plt.subplots(figsize=(10, 5))
        enriched_counts.plot(kind="bar", ax=ax, color='orange')
        ax.set_xlabel("Cluster ID")
        ax.set_ylabel("Enriched Terms Count")
        ax.set_title(f"Enrichment Counts per Cluster {title_suffix}")
        plt.tight_layout()
        plt.close()

def plot_cluster_sizes_and_enrichment(graph, cluster_key, enrichment_dict=None, title_suffix=""):
    import matplotlib.pyplot as plt
    import pandas as pd
    import networkx as nx

    cluster_attrs = nx.get_node_attributes(graph, cluster_key)
    missing = set(graph.nodes()) - set(cluster_attrs.keys())
    if missing:
        print(f"⚠️ Warning: {len(missing)} nodes missing '{cluster_key}' attribute.")

    cluster_counts = pd.Series(list(cluster_attrs.values())).value_counts().sort_index()
    fig, ax = plt.subplots(figsize=(10, 5))
    cluster_counts.plot(kind="bar", ax=ax)
    ax.set_xlabel("Cluster ID")
    ax.set_ylabel("Number of Genes")
    ax.set_title(f"Cluster Sizes {title_suffix}")
    plt.tight_layout()
    plt.close()

    if enrichment_dict:
        enriched_counts = pd.Series({k: len(v) for k, v in enrichment_dict.items()})
        fig, ax = plt.subplots(figsize=(10, 5))
        enriched_counts.plot(kind="bar", ax=ax, color='orange')
        ax.set_xlabel("Cluster ID")
        ax.set_ylabel("Enriched Terms Count")
        ax.set_title(f"Enrichment Counts per Cluster {title_suffix}")
        plt.tight_layout()
        plt.close()

def plot_relevance_boxplots(relevance_scores, graph, cluster_key, title_suffix=""):
    df = pd.DataFrame(relevance_scores, index=list(graph.nodes))
    df['cluster'] = pd.Series(nx.get_node_attributes(graph, cluster_key))
    df['relevance'] = df.drop(columns='cluster').max(axis=1)
    
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df, x='cluster', y='relevance')
    plt.title(f"Relevance Score Distribution per Cluster {title_suffix}")
    plt.xlabel("Cluster")
    plt.ylabel("Max Relevance Score")
    plt.tight_layout()
    plt.close()

def plot_top_genes_heatmap_(relevance_scores, graph, cluster_key, top_k=10, title_suffix=""):
    df = pd.DataFrame(relevance_scores, index=list(graph.nodes))
    df['cluster'] = pd.Series(nx.get_node_attributes(graph, cluster_key))

    top_genes = []
    for clust_id, group in df.groupby("cluster"):
        top = group.drop(columns='cluster').mean(axis=1).nlargest(top_k).index.tolist()
        top_genes.extend(top)
    
    df_top = df.loc[top_genes].drop(columns='cluster')
    sns.clustermap(df_top, cmap="viridis", figsize=(12, 10))
    plt.suptitle(f"Top {top_k} Genes per Cluster {title_suffix}", y=1.02)
    plt.close()

def plot_relevance_tsne_umap(relevance_scores, graph, cluster_key, method='tsne', title_suffix=""):

    X = np.array(relevance_scores)
    labels = pd.Series(nx.get_node_attributes(graph, cluster_key)).reindex(graph.nodes()).values

    reducer = TSNE(n_components=2, random_state=42) if method == 'tsne' else umap.UMAP(n_components=2, random_state=42)
    X_reduced = reducer.fit_transform(X)

    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=labels, palette='tab10', s=30)
    plt.title(f"{method.upper()} of Relevance Scores {title_suffix}")
    plt.xlabel("Dim 1")
    plt.ylabel("Dim 2")
    plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.close()

def plot_relevance_tsne_umap_grey(relevance_scores, graph, cluster_key, method='tsne', title_suffix=""):
    X = np.array(relevance_scores)
    labels = pd.Series(nx.get_node_attributes(graph, cluster_key)).reindex(graph.nodes()).values

    reducer = TSNE(n_components=2, random_state=42) if method == 'tsne' else umap.UMAP(n_components=2, random_state=42)
    X_reduced = reducer.fit_transform(X)

    # Map labels to colors using CLUSTER_COLORS
    palette = [CLUSTER_COLORS.get(label, "#777777") for label in sorted(set(labels))]

    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=labels, palette=palette, s=30)
    plt.title(f"{method.upper()} of Relevance Scores {title_suffix}")
    plt.xlabel("Dim 1")
    plt.ylabel("Dim 2")
    plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.close()

def run_enrichment_per_cluster(graph, cluster_key, gene_name_map=None, background_genes=None):
    from gseapy import enrichr

    cluster_genes = defaultdict(list)
    for node, clust in nx.get_node_attributes(graph, cluster_key).items():
        gene = gene_name_map[node] if gene_name_map else node
        cluster_genes[clust].append(gene)

    enrichment_results = {}
    for clust_id, genes in cluster_genes.items():
        enr = enrichr(gene_list=genes,
                      gene_sets='GO_Biological_Process_2021',
                      background=background_genes,
                      outdir=None, verbose=False)
        enrichment_results[clust_id] = enr.results
    return enrichment_results

def compute_relevance_scores_no_progress_bar(model, graph, features, node_indices=None, method="saliency", use_abs=True, baseline=None, steps=50):
    """
    Computes relevance scores for selected nodes using either saliency (gradients) or integrated gradients (IG).

    Args:
        model: Trained GNN model
        graph: DGL graph
        features: Input node features (torch.Tensor or np.ndarray)
        node_indices: List/Tensor of node indices to compute relevance for. If None, auto-select using probs > 0.0
        method: "saliency" or "integrated_gradients"
        use_abs: Whether to use absolute values of gradients
        baseline: Baseline input for IG (default: zero vector)
        steps: Number of steps for IG approximation

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features] (0s for nodes not analyzed)
    """
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(node_indices):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            if method == "saliency":
                probs[idx].backward(retain_graph=(i != len(node_indices) - 1))
                grads = features.grad[idx]
                relevance_scores[idx] = grads.abs().detach() if use_abs else grads.detach()

            elif method == "integrated_gradients":
                # Define baseline
                if baseline is None:
                    baseline_input = torch.zeros_like(features)
                else:
                    baseline_input = baseline.clone().detach()

                # Generate scaled inputs
                scaled_inputs = [baseline_input + (float(alpha) / steps) * (features - baseline_input) for alpha in range(1, steps + 1)]
                total_grad = torch.zeros_like(features)

                for input_step in scaled_inputs:
                    input_step.requires_grad_()
                    out = model(graph, input_step)
                    prob = torch.sigmoid(out.squeeze())[idx]

                    model.zero_grad()
                    if input_step.grad is not None:
                        input_step.grad.zero_()

                    prob.backward(retain_graph=True)
                    grad = input_step.grad#.detach()
                    total_grad += grad

                avg_grad = total_grad / steps
                ig = (features - baseline_input) * avg_grad
                relevance_scores[idx] = ig[idx].abs() if use_abs else ig[idx]

            else:
                raise ValueError(f"Unknown method: {method}. Use 'saliency' or 'integrated_gradients'.")

    return relevance_scores

def compute_relevance_scores(model, graph, features, node_indices=None, method="saliency", use_abs=True, baseline=None, steps=50):
    """
    Computes relevance scores for selected nodes using either saliency (gradients) or integrated gradients (IG).

    Args:
        model: Trained GNN model
        graph: DGL graph
        features: Input node features (torch.Tensor or np.ndarray)
        node_indices: List/Tensor of node indices to compute relevance for. If None, auto-select using probs > 0.0
        method: "saliency" or "integrated_gradients"
        use_abs: Whether to use absolute values of gradients
        baseline: Baseline input for IG (default: zero vector)
        steps: Number of steps for IG approximation

    Returns:
        relevance_scores: Tensor of shape [num_nodes, num_features] (0s for nodes not analyzed)
    """
    model.eval()
    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)

    features = features.clone().detach().requires_grad_(True)

    with torch.enable_grad():
        logits = model(graph, features)
        probs = torch.sigmoid(logits.squeeze())

        if node_indices is None:
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

        relevance_scores = torch.zeros_like(features)

        for i, idx in enumerate(tqdm(node_indices, desc=f"Computing relevance ({method})", leave=True)):
            model.zero_grad()
            if features.grad is not None:
                features.grad.zero_()

            if method == "saliency":
                probs[idx].backward(retain_graph=(i != len(node_indices) - 1))
                grads = features.grad[idx]
                relevance_scores[idx] = grads.abs().detach() if use_abs else grads.detach()

            elif method == "integrated_gradients":
                # Define baseline
                if baseline is None:
                    baseline_input = torch.zeros_like(features)
                else:
                    baseline_input = baseline.clone().detach()

                # Generate scaled inputs
                total_grad = torch.zeros_like(features)
                for alpha in range(1, steps + 1):
                    scaled_input = baseline_input + (alpha / steps) * (features - baseline_input)
                    scaled_input.requires_grad_()

                    out = model(graph, scaled_input)
                    prob = torch.sigmoid(out.squeeze())[idx]

                    model.zero_grad()
                    if scaled_input.grad is not None:
                        scaled_input.grad.zero_()

                    prob.backward(retain_graph=True)
                    grad = scaled_input.grad
                    total_grad += grad

                avg_grad = total_grad / steps
                ig = (features - baseline_input) * avg_grad
                relevance_scores[idx] = ig[idx].abs() if use_abs else ig[idx]

            else:
                raise ValueError(f"Unknown method: {method}. Use 'saliency' or 'integrated_gradients'.")

    return relevance_scores

def compute_relevance_scores_integrated_gradients_no_captum(
    model, graph, features, node_indices=None,
    baseline=None, steps=50, use_abs=True
):
    """
    Computes relevance scores using Integrated Gradients for selected nodes.

    Args:
        model: Trained GNN model.
        graph: DGL graph.
        features: [N, D] tensor of input features.
        node_indices: Node indices to compute relevance for. If None, auto-select based on sigmoid output > 0.5.
        baseline: Baseline input tensor [N, D] to start integration from. If None, uses zeros.
        steps: Number of steps for the IG path.
        use_abs: Whether to return absolute attributions (recommended).

    Returns:
        relevance_scores: [N, D] tensor with non-zero rows for selected nodes.
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    features = features.clone().detach()

    if baseline is None:
        baseline = torch.zeros_like(features)

    if node_indices is None:
        with torch.no_grad():
            probs = torch.sigmoid(model(graph, features)).squeeze()
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

    relevance_scores = torch.zeros_like(features)

    for idx in tqdm(node_indices, desc="Computing IG relevance"):
        total_grad = torch.zeros_like(features[idx])

        for alpha in torch.linspace(0, 1, steps):
            interpolated = baseline[idx] + alpha * (features[idx] - baseline[idx])
            input_vec = features.clone().detach()
            input_vec[idx] = interpolated
            input_vec.requires_grad_(True)

            logits = model(graph, input_vec)
            prob = torch.sigmoid(logits.squeeze())[idx]

            model.zero_grad()
            if input_vec.grad is not None:
                input_vec.grad.zero_()
            prob.backward(retain_graph=True)

            grad = input_vec.grad[idx]
            if grad is not None:
                total_grad += grad
            else:
                print(f"⚠️ Gradient was None at alpha={alpha.item():.2f}")

        avg_grad = total_grad / steps
        ig = (features[idx] - baseline[idx]) * avg_grad
        relevance_scores[idx] = ig.abs() if use_abs else ig

    return relevance_scores

def compute_relevance_scores_integrated_gradients(
    model, graph, features, node_indices=None,
    baseline=None, steps=50, use_abs=True
):
    """
    Computes relevance scores using Integrated Gradients via Captum for selected nodes.

    Args:
        model: Trained GNN model.
        graph: DGL graph.
        features: [N, D] input tensor.
        node_indices: Nodes to compute relevance for (auto-select if None).
        baseline: [N, D] baseline tensor. Default = zero.
        steps: Number of IG steps.
        use_abs: Return abs(relevance) if True.

    Returns:
        relevance_scores: [N, D] tensor, relevance per node; 0s for unselected nodes.
    """
    model.eval()

    if isinstance(features, np.ndarray):
        features = torch.tensor(features, dtype=torch.float32)
    features = features.clone().detach()

    if baseline is None:
        baseline = torch.zeros_like(features)

    if node_indices is None:
        with torch.no_grad():
            probs = torch.sigmoid(model(graph, features)).squeeze()
            node_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
            if node_indices.ndim == 0:
                node_indices = node_indices.unsqueeze(0)

    relevance_scores = torch.zeros_like(features)

    for idx in tqdm(node_indices.tolist(), desc="Computing IG relevance (Captum)"):

        # Only input for IG is [N, D] features
        def model_forward(input_feat):
            out = model(graph, input_feat)
            return torch.sigmoid(out).squeeze()[idx]  # scalar output for node idx

        ig = IntegratedGradients(model_forward)

        input_feat = features.clone().detach().requires_grad_(True)
        baseline_feat = baseline.clone().detach()

        attr = ig.attribute(
            inputs=input_feat,
            baselines=baseline_feat,
            n_steps=steps
        )

        relevance_scores[idx] = attr[idx].abs() if use_abs else attr[idx]

    return relevance_scores

def compute_entropy(labels):
    counts = np.array(list(Counter(labels).values()))
    probs = counts / counts.sum()
    return entropy(probs, base=2)

def compute_gini(labels):
    counts = np.array(list(Counter(labels).values()))
    sorted_counts = np.sort(counts)
    n = len(counts)
    cum_counts = np.cumsum(sorted_counts)
    gini = (n + 1 - 2 * np.sum(cum_counts) / cum_counts[-1]) / n
    return gini

def compute_silhouette(relevance_matrix, row_labels):
    try:
        return silhouette_score(relevance_matrix, row_labels)
    except:
        return float("nan")

#def evaluate_model_biclustering(model_type, graph, features, predicted_cancer_genes, top_gene_indices, node_names, n_clusters_row, device):
def evaluate_model_biclustering_(model, graph, relevance_matrix_bio, predicted_cancer_genes, top_gene_indices, node_names, n_clusters_row, device):
    
    # Load model
    #model = choose_model(model_type, in_feats=features.shape[1], hidden_feats=64, out_feats=1).to(device)
    #model.load_state_dict(torch.load(f'models/{model_type}.pt'))  # load trained model checkpoint
    model.eval()

    # Compute relevance scores
    '''relevance_scores, _ = compute_relevance_scores_integrated_gradients(
        model=model,
        graph=graph,
        features=features,
        node_indices=top_gene_indices,
        use_abs=True
    )'''
    '''relevance_scores = compute_relevance_scores(
        model=model,
        graph=graph,
        features=features,
        node_indices=top_gene_indices,
        use_abs=True
    )'''
    
    relevance_scores_bio = relevance_scores[:, :1024]
    topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
    relevance_scores_topk_bio = relevance_scores_bio[topk_node_indices_tensor]
    summary_bio_topk = extract_summary_features_np_bio(relevance_scores_topk_bio.detach().cpu().numpy())

    print(summary_bio_topk.shape)
    relevance_matrix_bio = summary_bio_topk
    
    # Run spectral biclustering
    graph_bio, row_labels_bio, col_labels_bio, _, _ = apply_full_spectral_biclustering_bio(
        graph=graph,
        summary_bio_features=relevance_matrix_bio, 
        node_names=node_names,
        predicted_cancer_genes=predicted_cancer_genes,
        n_clusters=n_clusters_row,
        save_path=None,
        save_row_labels_path=None,
        save_total_genes_per_cluster_path=None,
        save_predicted_counts_path=None,
        output_path_genes_clusters=None,
        output_path_heatmap=None,
        topk_node_indices=top_gene_indices
    )

    # Assign cluster labels
    full_row_labels_bio = torch.full((graph_bio.num_nodes(),), -1, dtype=torch.long)
    full_row_labels_bio[top_gene_indices] = torch.tensor(row_labels_bio, dtype=torch.long)
    graph_bio.ndata['cluster_bio'] = full_row_labels_bio

    # Compute metrics on clustered top genes
    valid_mask = full_row_labels_bio[top_gene_indices] != -1
    valid_clusters = full_row_labels_bio[top_gene_indices][valid_mask].cpu().numpy()
    valid_features = relevance_matrix_bio[top_gene_indices][valid_mask]

    entropy_score = compute_entropy(valid_clusters)
    gini_score = compute_gini(valid_clusters)
    silhouette = compute_silhouette(valid_features.cpu().numpy(), valid_clusters)

    return {
        "Model": model_type,
        "Entropy": entropy_score,
        "Gini": gini_score,
        "Silhouette": silhouette
    }

def apply_full_spectral_biclustering_bio_no_tsne(
        graph, summary_bio_features, node_names, 
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None,
        n_trials=20  # New param to control how many seeds to try
    ):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    row_sums = summary_bio_features.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]
    ##filtered_features = summary_bio_features[high_saliency_indices]

    #relevance_matrix = filtered_features

    # === Use balanced biclustering
    bicluster = best_balanced_biclustering(summary_bio_features, n_clusters, n_trials)

    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels

    # === Assign cluster labels to top-k nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Optional: Heatmaps and t-SNE
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    # plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    # plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    # plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_topo_no_tsne(
        graph, summary_topo_features, node_names,
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None
    ):

    print(f"Running Spectral Biclustering on topo features with {n_clusters} clusters...")
    assert summary_topo_features.shape[1] == 64, f"Expected 64 summary features, got {summary_topo_features.shape[1]}"

    # === Normalize topo features
    summary_topo_features = StandardScaler().fit_transform(summary_topo_features)

    # === Spectral Biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_topo_features)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels
    #graph.ndata['cluster_topo'] = torch.tensor(row_labels, dtype=torch.long)
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)

    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_topo_features.")

    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo'] = row_labels_tensor

    print("✅ Spectral Biclustering (topo) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)
    
    # === Save original (unclustered) heatmap before biclustering
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    '''plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path_unclustered_heatmap) 
    
    plot_tsne(summary_topo_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path_heatmap)'''

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_topo_biclustering_heatmap_no_csv(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    
    """
    Plots a spectral biclustering heatmap for topological embeddings (1024–2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # 🔹 Extract 64D summary of topological features
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # 🔹 Create topo feature names (01–64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # 🔹 Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # 🔹 Sort by cluster → then by gene-wise relevance within cluster
    sorted_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        sorted_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[sorted_indices]
    sorted_clusters = row_labels[sorted_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in sorted_indices]

    # 🔹 Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # 🔹 Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # 🔹 Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # 🔹 Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # 🔹 Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # 🔹 Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # 🔹 Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # 🔹 Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved spectral clustering heatmap to {output_path}")

    # 🔹 Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_bio_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    cancer_names,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
    ):  
    
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)
    # 🔹 Normalize relevance scores row-wise (per gene)
    '''
    relevance_scores = (relevance_scores - relevance_scores.min(axis=1, keepdims=True)) / \
                    (relevance_scores.max(axis=1, keepdims=True) - relevance_scores.min(axis=1, keepdims=True) + 1e-6)'''
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    ##relevance_scores = StandardScaler().fit_transform(relevance_scores)*20
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())*20
    
    # 🔹 Set default omics colors
    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',      # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    # 🔹 Cancer types and omics order
    
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]

    omics_order = ['cna', 'ge', 'meth', 'mf']

    # 🔹 Build feature names internally
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    sorted_indices = np.argsort(row_labels)
    relevance_scores = relevance_scores[sorted_indices]
    row_labels = row_labels[sorted_indices]

    # 🔹 Then sort within each cluster (optional: by LRP sum descending)
    new_order = []
    unique_clusters = np.unique(row_labels)
    for cluster in unique_clusters:
        cluster_indices = np.where(row_labels == cluster)[0]
        
        # Sort within this cluster, for example by total relevance score (descending)
        cluster_relevance_sums = relevance_scores[cluster_indices].sum(axis=1)
        sorted_within = cluster_indices[np.argsort(-cluster_relevance_sums)]  # Descending
        new_order.extend(sorted_within)

    # 🔹 Apply new sorted order
    sorted_scores = relevance_scores[new_order]
    sorted_clusters = row_labels[new_order]


    # 🔹 Reorder columns using col_labels (if provided)
    if col_labels is not None:
        sorted_order = np.argsort(col_labels)
        sorted_scores = sorted_scores[:, sorted_order]
        feature_names = [feature_names[i] for i in sorted_order]

            
    # 🔹 Feature color mapping
    feature_colors = []
    for i, name in enumerate(feature_names):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")

    # 🔹 Setup figure
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])
    ax_bar.axis("off")  # Hide regular axis stuff
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
        
        
    # 🔹 Compute cluster boundaries
    unique_clusters, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]


    # 🔹 Compute mean relevance per omics group
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = sorted_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    # 🔹 Sort omics groups by descending mean relevance
    sorted_omics = sorted(omics_means.keys(), key=lambda x: omics_means[x], reverse=True)

    # 🔹 Reorder columns: first by omics group, then within-group by mean column relevance
    sorted_col_indices = []
    sorted_feature_names = []
    feature_colors = []

    for omics in sorted_omics:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))

        # Get mean relevance per feature in this omics group
        col_means = sorted_scores[:, group_indices].mean(axis=0)
        group_sorted_indices = [group_indices[i] for i in np.argsort(-col_means)]  # descending

        sorted_col_indices.extend(group_sorted_indices)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted_indices])
        feature_colors.extend([omics_colors[omics]] * len(group_sorted_indices))

    # 🔹 Apply sorted column order
    sorted_scores = sorted_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names

    # 🔹 Compute per-feature mean relevance (column-wise)
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    # 🔹 Plot one bar per feature (aligned to heatmap)
    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,  # center over column
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val  # dimmer for low values
        )


    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )
    
    # 🔹 Use dynamic vmax for contrast
    vmin = 0#np.percentile(sorted_scores, 1)
    vmax = np.percentile(sorted_scores, 99)

    # 🔹 Plot heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )
    
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Add cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 Color x-tick labels

    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    #ax.tick_params(axis='x', bottom=True, labelbottom=True)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Omics + LRP curve legend
    '''ax_legend.axis("off")

    # Create omics patches
    omics_patches = [
        Patch(color=color, label=omics.upper())
        for omics, color in omics_colors.items()
    ]

    # Create LRP patch
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')

    # Combine and render
    ax_legend.legend(
        handles=omics_patches + [lrp_patch],
        loc="center",
        ncol=len(omics_patches) + 1,
        frameon=False,
        fontsize=18,
        handleheight=1.5,
        handlelength=3
    )'''


    # 🔹 LRP curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Compute mean relevance per omics group (column-wise)
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = sorted_scores[:, start:end+1]  # inclusive end
        omics_means[omics] = group_scores.mean()

    # Map omics to x-center positions
    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    # Normalize mean values for alpha mapping
    mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)  # prevent zero division

    # Plot bars with darkness mapped to mean relevance
    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,  # small bar height
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]  # range alpha from 0.3 (light) to 1.0 (dark)
        )


    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Create legend patches
    '''lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')

    # Optional: Also create cluster color legend (if not already somewhere else)
    cluster_patches = [
        Patch(facecolor=color, label=f'Cluster {cid}')
        for cid, color in CLUSTER_COLORS.items()
    ]

    # 🔹 Place legend near the X-axis label area (or wherever appropriate)
    legend_ax = fig.add_subplot(gs[11, 2:48])
    legend_ax.axis('off')  # Hide the axis

    legend_ax.legend(
        handles=[lrp_patch],  # add cluster_patches + [lrp_patch] if you want both
        loc='center',
        ncol=1,
        frameon=False,
        fontsize=12
    )'''

    # 🔹 Layout and save
    #plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout()
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved spectral clustering heatmap to {output_path}")

    # 🔹 Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

    return pd.DataFrame(relevance_scores, index=gene_names, columns=feature_names)

def plot_bio_biclustering_heatmap__(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Step 1: Sort rows by cluster labels
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    # Step 2: Column sorting
    feature_avgs = sorted_scores.mean(axis=0)

    # Sort omics groups by group average
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean

    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    # Sort features within each omics group
    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    # Apply column sort
    sorted_scores = sorted_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", 
        ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    # Grid layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])
    ax        = fig.add_subplot(gs[1:13, 2:45])
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars (new positions from sorted order)
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_not_clustermap(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):  # ⬅ flip cluster order
    ##for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_clusters_sorted(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def build_contribution_graph(gene_names, contribution_matrix, threshold=0.14):
    """
    Build a directed graph where an edge from A to B exists if A contributes to B
    with a score above the threshold.

    Parameters:
        gene_names: list of gene names corresponding to matrix indices.
        contribution_matrix: numpy array [num_genes x num_genes] of importance scores.
        threshold: float, minimum contribution score to keep an edge.

    Returns:
        G: networkx.DiGraph representing the reduced contribution graph.
    """
    G = nx.DiGraph()
    num_genes = len(gene_names)

    for i in range(num_genes):
        for j in range(num_genes):
            if i != j:
                score = contribution_matrix[i, j]
                if score >= threshold:
                    G.add_edge(gene_names[i], gene_names[j], weight=score)

    return G

def find_strongly_connected_modules(G, min_size=5):
    """
    Detect strongly connected components (SCCs) in a directed graph using Tarjan's algorithm.

    Parameters:
        G: networkx.DiGraph
        min_size: int, minimum number of nodes per component to keep

    Returns:
        modules: list of sets, each representing a SCC with at least min_size nodes
    """
    sccs = list(nx.strongly_connected_components(G))
    filtered_sccs = [scc for scc in sccs if len(scc) >= min_size]
    return filtered_sccs

def plot_predicted_gene_embeddings_by_cluster(graph, node_names, scores, output_path, score_threshold=0.5):
    cluster_ids = graph.ndata['cluster_bio'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores.cpu().numpy()

    predicted_mask = scores >= score_threshold
    predicted_indices = np.where(predicted_mask)[0]

    if len(predicted_indices) == 0:
        print("⚠️ No predicted genes above threshold.")
        return

    predicted_embeddings = embeddings[predicted_indices]
    predicted_clusters = cluster_ids[predicted_indices]

    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(predicted_embeddings)

    # Plot
    plt.figure(figsize=(10, 8))
    for c in np.unique(predicted_clusters):
        cluster_mask = predicted_clusters == c
        coords = tsne_coords[cluster_mask]
        label = f"Cluster {c}"
        plt.scatter(
            coords[:, 0], coords[:, 1],
            color=CLUSTER_COLORS.get(c, "#555555"),
            label=label,
            alpha=0.8,
            edgecolors='k',
            s=60
        )

    plt.title("t-SNE of Predicted Genes Colored by Cluster", fontsize=14)
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.legend(title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"✅ Clustered t-SNE plot of predicted genes saved to:\n{output_path}")

def plot_top_predicted_genes_tsne(graph, node_names, scores, output_path, top_k=1000):
    cluster_ids = graph.ndata['cluster'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores##.cpu().numpy()

    # Get top K predicted genes
    top_indices = np.argsort(scores)[-top_k:]
    top_embeddings = embeddings[top_indices]
    top_clusters = cluster_ids[top_indices]
    top_scores = scores[top_indices]
    top_names = [node_names[i] for i in top_indices]

    # t-SNE projection
    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(top_embeddings)

    # Plot
    plt.figure(figsize=(10, 8))
    rcParams['pdf.fonttype'] = 42  # prevent font issues in vector graphics

    for c in np.unique(top_clusters):
        mask = top_clusters == c
        coords = tsne_coords[mask]
        plt.scatter(
            coords[:, 0], coords[:, 1],
            color=CLUSTER_COLORS.get(c, "#555555"),
            edgecolors='k',
            s=60,
            alpha=0.8
        )

        # Top 1 in this cluster (within top_k)
        # cluster_scores = top_scores[mask]
        # if cluster_scores.size > 0:
        #     top_idx_in_cluster = np.argmax(cluster_scores)
        #     name = np.array(top_names)[mask][top_idx_in_cluster]
        #     x, y = coords[top_idx_in_cluster]
        #     # Highlight the top 1 with a yellow circle (half the size of the original dot)
        #     plt.scatter(x, y, s=60, edgecolors='yellow', alpha=0.6, linewidth=1, marker='o', color='red')
        #     # Change label text color to red
        #     plt.text(x, y, name, fontsize=9, fontweight='bold', ha='center', va='center', color='black')

    plt.title("t-SNE of Top 1000 Predicted Genes by Cluster", fontsize=14)
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    
    # Remove legend
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"✅ Top predicted gene t-SNE plot saved to:\n{output_path}")
 
def plot_top_predicted_genes_umap(graph, node_names, scores, output_path, top_k=1000):
    cluster_ids = graph.ndata['cluster'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores  # Assuming already on CPU or NumPy

    # Get top K predicted genes
    top_indices = np.argsort(scores)[-top_k:]
    top_embeddings = embeddings[top_indices]
    top_clusters = cluster_ids[top_indices]
    top_scores = scores[top_indices]
    top_names = [node_names[i] for i in top_indices]

    # UMAP projection
    reducer = umap.UMAP(n_components=2, random_state=42)
    umap_coords = reducer.fit_transform(top_embeddings)

    # Plot
    plt.figure(figsize=(10, 8))
    rcParams['pdf.fonttype'] = 42  # Prevent font issues in vector graphics

    for c in np.unique(top_clusters):
        mask = top_clusters == c
        coords = umap_coords[mask]
        plt.scatter(
            coords[:, 0], coords[:, 1],
            color=CLUSTER_COLORS.get(c, "#555555"),
            edgecolors='k',
            s=60,
            alpha=0.8
        )

        # Top 1 in this cluster (within top_k)
        # cluster_scores = top_scores[mask]
        # if cluster_scores.size > 0:
        #     top_idx_in_cluster = np.argmax(cluster_scores)
        #     name = np.array(top_names)[mask][top_idx_in_cluster]
        #     x, y = coords[top_idx_in_cluster]
        #     plt.scatter(x, y, s=60, edgecolors='yellow', alpha=0.6, linewidth=1, marker='o', color='red')
        #     plt.text(x, y, name, fontsize=9, fontweight='bold', ha='center', va='center', color='black')

    plt.title("UMAP of Top 1000 Predicted Genes by Cluster", fontsize=14)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"✅ Top predicted gene UMAP plot saved to:\n{output_path}")

def plot_tsne_predicted_genes(graph, node_names, scores, output_path, args):
    cluster_ids = graph.ndata['cluster'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores##.cpu().numpy()

    predicted_mask = scores >= args.score_threshold
    predicted_indices = np.where(predicted_mask)[0]
    predicted_scores = scores[predicted_indices]
    predicted_clusters = cluster_ids[predicted_indices]
    predicted_embeddings = embeddings[predicted_indices]

    tsne = TSNE(n_components=2, random_state=42)
    tsne_coords = tsne.fit_transform(predicted_embeddings)

    # Gather top 2 genes per cluster
    top_genes = []
    for c in np.unique(predicted_clusters):
        cluster_mask = predicted_clusters == c
        cluster_indices = np.where(cluster_mask)[0]
        if len(cluster_indices) == 0:
            continue
        top_indices = cluster_indices[np.argsort(predicted_scores[cluster_indices])[-2:]]  # top 2
        for idx in top_indices:
            top_genes.append((predicted_indices[idx], tsne_coords[idx], c))

    # Plot
    plt.figure(figsize=(10, 7))
    for idx, (node_idx, coord, cluster_id) in enumerate(top_genes):
        color = CLUSTER_COLORS.get(cluster_id, "#333333")
        plt.scatter(coord[0], coord[1], color=color, s=120, edgecolor='k')
        plt.text(coord[0]+1.5, coord[1], node_names[node_idx], fontsize=9, color=color)

    # Legend
    unique_predicted_clusters = np.unique(predicted_clusters)
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=f"Cluster {c}", markersize=10)
        for c, color in CLUSTER_COLORS.items()
        if c in unique_predicted_clusters
    ]
    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.title("t-SNE of Top 2 Predicted Genes per Cluster")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

def plot_umap_predicted_genes_top5(graph, node_names, scores, output_path, args):
    cluster_ids = graph.ndata['cluster'].cpu().numpy()
    embeddings = graph.ndata['feat'].cpu().numpy()
    scores = scores

    predicted_mask = scores >= args.score_threshold
    predicted_indices = np.where(predicted_mask)[0]
    predicted_scores = scores[predicted_indices]
    predicted_clusters = cluster_ids[predicted_indices]
    predicted_embeddings = embeddings[predicted_indices]

    # UMAP projection
    reducer = umap.UMAP(n_components=2, random_state=42)
    umap_coords = reducer.fit_transform(predicted_embeddings)

    # Gather top 5 genes per cluster
    top_genes = []
    for c in np.unique(predicted_clusters):
        cluster_mask = predicted_clusters == c
        cluster_indices = np.where(cluster_mask)[0]
        if len(cluster_indices) == 0:
            continue
        top_indices = cluster_indices[np.argsort(predicted_scores[cluster_indices])[-5:]]  # top 5
        for idx in top_indices:
            top_genes.append((predicted_indices[idx], umap_coords[idx], c))

    # Plot
    plt.figure(figsize=(12, 8))
    for idx, (node_idx, coord, cluster_id) in enumerate(top_genes):
        color = CLUSTER_COLORS.get(cluster_id, "#333333")
        plt.scatter(coord[0], coord[1], color=color, s=120, edgecolor='k')
        plt.text(coord[0] + 1.5, coord[1], node_names[node_idx], fontsize=9, color=color)

    # Legend
    unique_predicted_clusters = np.unique(predicted_clusters)
    handles = [
        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=f"Cluster {c}", markersize=10)
        for c, color in CLUSTER_COLORS.items()
        if c in unique_predicted_clusters
    ]
    plt.legend(handles=handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')

    # Remove frame and axes
    plt.axis('off')

    plt.title("UMAP of Top 5 Predicted Genes per Cluster")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"✅ UMAP plot of top 5 predicted genes per cluster saved to:\n{output_path}")

def apply_spectral_biclustering_ori(graph, embeddings, node_names, predicted_cancer_genes, n_clusters, save_path, save_row_labels_path, save_total_genes_per_cluster_path, save_predicted_counts_path, output_path_genes_clusters):
    print(f"Running Spectral Biclustering with {n_clusters} row clusters...")

    node_features = embeddings.cpu().numpy()

    # Run Spectral Biclustering
    ##bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    n_rows, n_cols = node_features.shape
    bicluster = SpectralBiclustering(
        n_clusters=(min(n_clusters, n_rows), min(n_clusters, n_cols)),
        method='log',
        random_state=42
    )

    bicluster.fit(node_features)

    # Assign cluster labels to graph
    row_labels = bicluster.row_labels_
    graph.ndata['cluster'] = torch.tensor(row_labels, dtype=torch.long)

    # Original graph-saving method (kept unchanged)
    print("Spectral Biclustering complete. Cluster labels assigned to graph.")
    print(f"Saving graph to {save_path}")
    torch.save({
        'edges': graph.edges(),
        'features': graph.ndata['feat'],
        'labels': graph.ndata.get('label', None),
        'cluster': graph.ndata['cluster']
    }, save_path)

    # Save cluster labels separately
    save_row_labels(row_labels, save_row_labels_path)

    # Count total genes per cluster
    total_genes_per_cluster = {i: np.sum(row_labels == i) for i in range(n_clusters)}
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    # Count predicted cancer genes per cluster
    pred_counts = {i: 0 for i in range(n_clusters)}
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    predicted_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]

    for idx in predicted_indices:
        if 0 <= idx < len(row_labels):
            cluster_id = row_labels[idx]
            pred_counts[cluster_id] += 1
        else:
            print(f"Skipping invalid predicted gene index: {idx}")

    # Save predicted counts separately
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Visualize with t-SNE
    if node_features.shape[0] < 2 or node_features.shape[1] < 2:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")
    else:
        tsne = TSNE(n_components=2, perplexity=min(30, len(node_features) - 1), random_state=42)
        reduced_embeddings = tsne.fit_transform(node_features)

        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)

        for idx in predicted_indices:
            x, y = reduced_embeddings[idx]
            plt.scatter(x, y, facecolors='none', edgecolors='red', s=50, linewidths=2)

        plt.xlabel("t-SNE Dimension 1", fontsize=18)
        plt.ylabel("t-SNE Dimension 2", fontsize=18)
        plt.title("Spectral Biclustering of Genes with Predicted Cancer Markers")
        plt.savefig(output_path_genes_clusters, bbox_inches="tight")
        plt.close()
        print(f"Cluster visualization saved to {output_path_genes_clusters}")


    return graph, row_labels, total_genes_per_cluster, pred_counts

def apply_spectral_biclustering_adapted(
    graph,
    embeddings,
    node_names,
    predicted_cancer_genes,
    n_clusters,
    save_path,
    save_row_labels_path,
    save_total_genes_per_cluster_path,
    save_predicted_counts_path,
    output_path_genes_clusters,
    n_trials=20
):
    print(f"Running Spectral Biclustering with {n_clusters} row clusters...")

    node_features = embeddings.cpu().numpy()
    n_rows, n_cols = node_features.shape

    # Balanced biclustering
    bicluster = best_balanced_biclustering(node_features, n_clusters, n_trials)

    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels  # Invert for consistency

    # Assign cluster labels to graph
    graph.ndata['cluster'] = torch.tensor(row_labels, dtype=torch.long)

    # Save graph
    print("Spectral Biclustering complete. Cluster labels assigned to graph.")
    print(f"Saving graph to {save_path}")
    torch.save({
        'edges': graph.edges(),
        'features': graph.ndata['feat'],
        'labels': graph.ndata.get('label', None),
        'cluster': graph.ndata['cluster']
    }, save_path)

    # Save cluster labels
    save_row_labels(row_labels, save_row_labels_path)

    # Count total genes per cluster
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    # Count predicted cancer genes per cluster
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Visualize with t-SNE
    if node_features.shape[0] < 2 or node_features.shape[1] < 2:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")
    else:
        tsne = TSNE(n_components=2, perplexity=min(30, len(node_features) - 1), random_state=42)
        reduced_embeddings = tsne.fit_transform(node_features)

        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)

        for idx in predicted_indices:
            x, y = reduced_embeddings[idx]
            plt.scatter(x, y, facecolors='none', edgecolors='red', s=50, linewidths=2)

        plt.xlabel("t-SNE Dimension 1", fontsize=18)
        plt.ylabel("t-SNE Dimension 2", fontsize=18)
        plt.title("Spectral Biclustering of Genes with Predicted Cancer Markers")
        plt.savefig(output_path_genes_clusters, bbox_inches="tight")
        plt.close()
        print(f"Cluster visualization saved to {output_path_genes_clusters}")

    return graph, row_labels, total_genes_per_cluster, pred_counts

def apply_spectral_biclustering(
    graph, embeddings, node_names, predicted_cancer_genes, n_clusters,
    save_path, save_row_labels_path, save_total_genes_per_cluster_path,
    save_predicted_counts_path, output_path_genes_clusters
):

    print(f"Running Spectral Biclustering with {n_clusters} row clusters...")

    node_features = embeddings#.cpu().numpy()
    n_rows, n_cols = node_features.shape

    bicluster = SpectralBiclustering(
        n_clusters=(min(n_clusters, n_rows), min(n_clusters, n_cols)),
        method='log',
        random_state=42
    )
    bicluster.fit(node_features)

    # Assign cluster labels to graph
    row_labels = bicluster.row_labels_
    graph.ndata['cluster'] = torch.tensor(row_labels, dtype=torch.long)

    print("Spectral Biclustering complete. Cluster labels assigned to graph.")
    print(f"Saving graph to {save_path}")
    torch.save({
        'edges': graph.edges(),
        'features': graph.ndata['feat'],
        'labels': graph.ndata.get('label', None),
        'cluster': graph.ndata['cluster']
    }, save_path)

    save_row_labels(row_labels, save_row_labels_path)

    total_genes_per_cluster = {i: np.sum(row_labels == i) for i in range(n_clusters)}
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts = {i: 0 for i in range(n_clusters)}
    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    predicted_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]

    for idx in predicted_indices:
        if 0 <= idx < len(row_labels):
            cluster_id = row_labels[idx]
            pred_counts[cluster_id] += 1
        else:
            print(f"Skipping invalid predicted gene index: {idx}")

    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # t-SNE visualization of nodes (rows)
    if n_rows < 2 or n_cols < 2:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")
    else:
        tsne = TSNE(n_components=2, perplexity=min(30, n_rows - 1), random_state=42)
        reduced_embeddings = tsne.fit_transform(node_features)

        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)

        for idx in predicted_indices:
            x, y = reduced_embeddings[idx]
            plt.scatter(x, y, facecolors='none', edgecolors='red', s=50, linewidths=2)

        plt.xlabel("t-SNE Dimension 1", fontsize=18)
        plt.ylabel("t-SNE Dimension 2", fontsize=18)
        plt.title("t-SNE of Gene Embeddings (Row Clusters)")
        plt.savefig(output_path_genes_clusters, bbox_inches="tight")
        plt.close()
        print(f"Row-cluster t-SNE saved to {output_path_genes_clusters}")

        # t-SNE visualization of features (columns)
        transposed_features = node_features.T  # shape: (n_features, n_nodes)
        feature_labels = bicluster.column_labels_
        n_feature_clusters = len(np.unique(feature_labels))

        tsne_feat = TSNE(n_components=2, perplexity=min(30, len(transposed_features) - 1), random_state=42)
        reduced_feats = tsne_feat.fit_transform(transposed_features)

        plt.figure(figsize=(12, 10))
        for fid in range(n_feature_clusters):
            idx = np.where(feature_labels == fid)[0]
            plt.scatter(reduced_feats[idx, 0], reduced_feats[idx, 1],
                        label=f"Feature Cluster {fid}",
                        edgecolor='k', s=100, alpha=0.8)

        plt.xlabel("t-SNE Dimension 1", fontsize=18)
        plt.ylabel("t-SNE Dimension 2", fontsize=18)
        plt.title("t-SNE of Feature Embeddings (Column Clusters)")
        plt.legend()
        output_path_feat_clusters = output_path_genes_clusters.replace(".png", "_features.png")
        plt.savefig(output_path_feat_clusters, bbox_inches="tight")
        plt.close()
        print(f"Column-cluster t-SNE saved to {output_path_feat_clusters}")

    return graph, row_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_tsne(
        graph, summary_bio_features, node_names, 
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None,
        n_trials=20
    ):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    row_sums = summary_bio_features.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]

    # Run balanced spectral biclustering
    bicluster = best_balanced_biclustering(summary_bio_features, n_clusters, n_trials)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels  # optional reverse

    # Assign cluster labels to nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    # Save outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === t-SNE visualization with column clustering as color-coded feature summary ===
    if summary_bio_features.shape[0] >= 2 and summary_bio_features.shape[1] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features) - 1), random_state=42)
            reduced = tsne.fit_transform(summary_bio_features)

            plt.figure(figsize=(12, 10))
            for cluster_id in range(n_clusters):
                idx = np.where(row_labels == cluster_id)[0]
                plt.scatter(reduced[idx, 0], reduced[idx, 1],
                            color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                            edgecolor='k', s=100, alpha=0.8, label=f"Cluster {cluster_id}")

            # Highlight predicted genes
            # if predicted_indices:
            #     pred_idx = [topk_node_indices.index(i) for i in predicted_indices if i in topk_node_indices]
            #     for idx in pred_idx:
            #         x, y = reduced[idx]
            #         plt.scatter(x, y, facecolors='none', edgecolors='red', s=80, linewidths=2)

            plt.xlabel("t-SNE Dimension 1", fontsize=18)
            plt.ylabel("t-SNE Dimension 2", fontsize=18)
            plt.title("t-SNE of Summary Bio Features (Spectral Biclustering)", fontsize=16)
            plt.legend()
            plt.tight_layout()
            plt.savefig(output_path_genes_clusters, bbox_inches='tight')
            plt.close()
            print(f"t-SNE visualization saved to {output_path_genes_clusters}")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")
    else:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_with_tick_label(
        graph, summary_bio_features_initial, node_names, 
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None,
        n_trials=20
    ):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features_initial.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_initial.shape[1]}"
    scaler = StandardScaler()
    #summary_bio_features = scaler.fit_transform(summary_bio_features)
    summary_bio_features_initial = normalize(summary_bio_features_initial, norm='l2', axis=1)

    row_sums = summary_bio_features_initial.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]

    # Run balanced spectral biclustering
    bicluster = best_balanced_biclustering(summary_bio_features_initial, n_clusters, n_trials)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels  # optional reverse

    # Assign cluster labels to nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features_initial.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    # Save outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === t-SNE Visualization ===
    if summary_bio_features_initial.shape[0] >= 2 and summary_bio_features_initial.shape[1] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features_initial) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_bio_features_initial)

            plt.figure(figsize=(12, 10))
            for cluster_id in range(n_clusters):
                idx = np.where(row_labels == cluster_id)[0]
                plt.scatter(reduced_tsne[idx, 0], reduced_tsne[idx, 1],
                            color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                            edgecolor='k', s=100, alpha=0.8, label=f"Cluster {cluster_id}")
            plt.xlabel("t-SNE Dimension 1", fontsize=18)
            plt.ylabel("t-SNE Dimension 2", fontsize=18)
            plt.tick_params(axis='both', which='major', labelsize=18)
            #plt.title("t-SNE of Summary Bio Features (Spectral Biclustering)", fontsize=16)
            #plt.legend()
            plt.tight_layout()
            tsne_path = output_path_genes_clusters.replace(".png", "_tsne_initial.png")
            plt.savefig(tsne_path, bbox_inches='tight')
            plt.close()
            print(f"t-SNE visualization saved to {tsne_path}")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")
    else:
        print("Not enough samples or features to run t-SNE. Skipping visualization.")

    # === UMAP Visualization ===
    if summary_bio_features_initial.shape[0] >= 2 and summary_bio_features_initial.shape[1] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_bio_features_initial)

            plt.figure(figsize=(12, 10))
            for cluster_id in range(n_clusters):
                idx = np.where(row_labels == cluster_id)[0]
                plt.scatter(reduced_umap[idx, 0], reduced_umap[idx, 1],
                            color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                            edgecolor='k', s=100, alpha=0.8, label=f"Cluster {cluster_id}")
            plt.xlabel("UMAP Dimension 1", fontsize=18)
            plt.ylabel("UMAP Dimension 2", fontsize=18)
            plt.tick_params(axis='both', which='major', labelsize=18)
            #plt.title("UMAP of Summary Bio Features (Spectral Biclustering)", fontsize=16)
            #plt.legend()
            plt.tight_layout()
            umap_path = output_path_genes_clusters.replace(".png", "_umap_initial.png")
            plt.savefig(umap_path, bbox_inches='tight')
            plt.close()
            print(f"UMAP visualization saved to {umap_path}")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")
    else:
        print("Not enough samples or features to run UMAP. Skipping visualization.")

    # === PCA Visualization ===
    if summary_bio_features_initial.shape[0] >= 2 and summary_bio_features_initial.shape[1] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_bio_features_initial)

            plt.figure(figsize=(12, 10))
            for cluster_id in range(n_clusters):
                idx = np.where(row_labels == cluster_id)[0]
                plt.scatter(reduced_pca[idx, 0], reduced_pca[idx, 1],
                            color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                            edgecolor='k', s=100, alpha=0.8, label=f"Cluster {cluster_id}")
            plt.xlabel(f"PCA Component 1 ({pca.explained_variance_ratio_[0]*100:.1f}%)", fontsize=18)
            plt.ylabel(f"PCA Component 2 ({pca.explained_variance_ratio_[1]*100:.1f}%)", fontsize=18)
            plt.tick_params(axis='both', which='major', labelsize=18)
            #plt.title("PCA of Summary Bio Features (Spectral Biclustering)", fontsize=16)
            #plt.legend()
            plt.tight_layout()
            pca_path = output_path_genes_clusters.replace(".png", "_pca_initial.png")
            plt.savefig(pca_path, bbox_inches='tight')
            plt.close()
            print(f"PCA visualization saved to {pca_path}")
        except Exception as e:
            print(f"PCA visualization failed: {e}")
    else:
        print("Not enough samples or features to run PCA. Skipping visualization.")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_initial(
    graph, summary_bio_features_initial, node_names, 
    predicted_cancer_genes, n_clusters,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    topk_node_indices=None,
    n_trials=20,
    feature_names=None
):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    #assert summary_bio_features_initial.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_initial.shape[1]}"
    if feature_names:
        assert len(set(feature_names)) == len(feature_names), "❌ Duplicate feature names detected!"
        print("✅ No duplicate feature names in summary bio features.")
            
    summary_bio_features_initial = normalize(summary_bio_features_initial, norm='l2', axis=1)
    row_sums = summary_bio_features_initial.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]

    bicluster = best_balanced_biclustering(summary_bio_features_initial, n_clusters, n_trials)
    row_labels = (n_clusters - 1) - bicluster.row_labels_  # Optional reverse
    col_labels = bicluster.column_labels_

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features_initial.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_data[idx, 0], reduced_data[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)
        # Fully hide axis ticks and labels
        plt.xticks([])  # Remove x-axis tick labels
        plt.yticks([])  # Remove y-axis tick labels
        plt.xlabel("")  # Remove x-axis label
        plt.ylabel("")  # Remove y-axis label
        plt.box(False)  # Remove box frame
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"{method_name} visualization saved to {path}")

    # t-SNE
    if summary_bio_features_initial.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features_initial) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_bio_features_initial)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_initial.png")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")

    # UMAP
    if summary_bio_features_initial.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_bio_features_initial)
            plot_embedding(reduced_umap, "UMAP", "_umap_initial.png")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")

    # PCA
    if summary_bio_features_initial.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_bio_features_initial)
            plot_embedding(reduced_pca, "PCA", "_pca_initial.png")
        except Exception as e:
            print(f"PCA visualization failed: {e}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_topo_initial(
        graph, summary_topo_features_initial, node_names,
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None
    ):

    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_topo_features_initial.shape[1] == 64, f"Expected 64 summary features, got {summary_topo_features_initial.shape[1]}"
    
    # === Normalize topo features
    ##summary_topo_features = StandardScaler().fit_transform(summary_topo_features)    
    summary_topo_features_initial = normalize(summary_topo_features_initial, norm='l2', axis=1)
    row_sums = summary_topo_features_initial.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]


    # === Spectral Biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_topo_features_initial)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels
    #graph.ndata['cluster_topo'] = torch.tensor(row_labels, dtype=torch.long)
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)

    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_topo_features.")

    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo'] = row_labels_tensor

    print("✅ Spectral Biclustering (topo) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_data[idx, 0], reduced_data[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)
        # Fully hide axis ticks and labels
        plt.xticks([])  # Remove x-axis tick labels
        plt.yticks([])  # Remove y-axis tick labels
        plt.xlabel("")  # Remove x-axis label
        plt.ylabel("")  # Remove y-axis label
        plt.box(False)  # Remove box frame
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"{method_name} visualization saved to {path}")

    # t-SNE
    if summary_topo_features_initial.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_topo_features_initial) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_topo_features_initial)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_initial.png")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")

    # UMAP
    if summary_topo_features_initial.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_topo_features_initial)
            plot_embedding(reduced_umap, "UMAP", "_umap_initial.png")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")

    # PCA
    if summary_topo_features_initial.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_topo_features_initial)
            plot_embedding(reduced_pca, "PCA", "_pca_initial.png")
        except Exception as e:
            print(f"PCA visualization failed: {e}")
    
    # === Save original (unclustered) heatmap before biclustering
    ##output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    '''plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path_unclustered_heatmap) 
    
    plot_tsne(summary_topo_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path_heatmap)'''

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_trained_(
    graph, summary_bio_features_trained, node_names, 
    predicted_cancer_genes, n_clusters,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    topk_node_indices=None,
    n_trials=20,
    feature_names=None
):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    #assert summary_bio_features_trained.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_trained.shape[1]}"
    
    summary_bio_features_trained = normalize(summary_bio_features_trained, norm='l2', axis=1)
    # row_sums = summary_bio_features_trained.sum(axis=1)
    # threshold = np.percentile(row_sums, 20)
    # high_saliency_indices = np.where(row_sums > threshold)[0]

    bicluster = best_balanced_biclustering(summary_bio_features_trained, n_clusters, n_trials)
    row_labels = (n_clusters - 1) - bicluster.row_labels_  # Optional reverse
    col_labels = bicluster.column_labels_

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features_trained.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)
    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_data[idx, 0], reduced_data[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)
        # Fully hide axis ticks and labels
        plt.xticks([])  # Remove x-axis tick labels
        plt.yticks([])  # Remove y-axis tick labels
        plt.xlabel("")  # Remove x-axis label
        plt.ylabel("")  # Remove y-axis label
        plt.box(False)  # Remove box frame
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"{method_name} visualization saved to {path}")

    # t-SNE
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features_trained) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_trained.png")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")

    # UMAP
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_umap, "UMAP", "_umap_trained.png")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")

    # PCA
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_pca, "PCA", "_pca_trained.png")
        except Exception as e:
            print(f"PCA visualization failed: {e}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_trained(
    graph,
    summary_bio_features_trained,
    node_names,
    predicted_cancer_genes,
    n_clusters,
    save_path,
    save_row_labels_path,
    save_total_genes_per_cluster_path,
    save_predicted_counts_path,
    output_path_genes_clusters,
    output_path_heatmap,
    topk_node_indices=None,
    n_trials=20,
    feature_names=None  # Optional: for labeling heatmap axes
):
    print(f"🔍 Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")

    # assert summary_bio_features_trained.shape[1] == 64, \
    #     f"Expected 64 summary features, got {summary_bio_features_trained.shape[1]}"
    
    if feature_names:
        assert len(set(feature_names)) == len(feature_names), "❌ Duplicate feature names detected!"
        print("✅ No duplicate feature names in summary bio features.")

    # Normalize features
    summary_bio_features_trained = normalize(summary_bio_features_trained, norm='l2', axis=1)

    # Run spectral biclustering
    bicluster = best_balanced_biclustering(summary_bio_features_trained, n_clusters, n_trials)
    row_labels = (n_clusters - 1) - bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # Assign cluster labels to graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to rows in summary_bio_features_trained.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    # Save outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)

    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Plotting helper
    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(
                reduced_data[idx, 0], reduced_data[idx, 1],
                color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                edgecolor='k', s=100, alpha=0.8
            )
        plt.xticks([]); plt.yticks([])
        plt.xlabel(""); plt.ylabel(""); plt.box(False)
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"📈 {method_name} visualization saved to {path}")

    # t-SNE
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_bio_features_trained) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_trained.png")
        except Exception as e:
            print(f"⚠️ t-SNE failed: {e}")

    # UMAP
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_umap, "UMAP", "_umap_trained.png")
        except Exception as e:
            print(f"⚠️ UMAP failed: {e}")

    # PCA
    if summary_bio_features_trained.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_bio_features_trained)
            plot_embedding(reduced_pca, "PCA", "_pca_trained.png")
        except Exception as e:
            print(f"⚠️ PCA failed: {e}")

    # Optional: heatmap visualization
    try:
        import seaborn as sns
        import pandas as pd
        df = pd.DataFrame(summary_bio_features_trained, columns=feature_names if feature_names else None)
        row_order = np.argsort(row_labels)
        sns.clustermap(df.iloc[row_order], col_cluster=False, row_cluster=False, figsize=(16, 12), cmap="viridis")
        plt.savefig(output_path_heatmap, bbox_inches='tight')
        plt.close()
        print(f"🔥 Heatmap saved to {output_path_heatmap}")
    except Exception as e:
        print(f"⚠️ Heatmap generation failed: {e}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_topo_trained(
        graph, summary_topo_features_trained, node_names,
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None
    ):

    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_topo_features_trained.shape[1] == 64, f"Expected 64 summary features, got {summary_topo_features_trained.shape[1]}"
    
    # === Normalize topo features
    ##summary_topo_features = StandardScaler().fit_transform(summary_topo_features)    
    summary_topo_features_trained = normalize(summary_topo_features_trained, norm='l2', axis=1)
    # row_sums = summary_topo_features_trained.sum(axis=1)
    # threshold = np.percentile(row_sums, 20)
    # high_saliency_indices = np.where(row_sums > threshold)[0]


    # === Spectral Biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_topo_features_trained)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    row_labels = (n_clusters - 1) - row_labels
    #graph.ndata['cluster_topo'] = torch.tensor(row_labels, dtype=torch.long)
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)

    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_topo_features.")

    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo'] = row_labels_tensor

    print("✅ Spectral Biclustering (topo) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    def plot_embedding(reduced_data, method_name, file_suffix):
        plt.figure(figsize=(12, 10))
        for cluster_id in range(n_clusters):
            idx = np.where(row_labels == cluster_id)[0]
            plt.scatter(reduced_data[idx, 0], reduced_data[idx, 1],
                        color=CLUSTER_COLORS.get(cluster_id, "#777777"),
                        edgecolor='k', s=100, alpha=0.8)
        # Fully hide axis ticks and labels
        plt.xticks([])  # Remove x-axis tick labels
        plt.yticks([])  # Remove y-axis tick labels
        plt.xlabel("")  # Remove x-axis label
        plt.ylabel("")  # Remove y-axis label
        plt.box(False)  # Remove box frame
        plt.tight_layout()
        path = output_path_genes_clusters.replace(".png", file_suffix)
        plt.savefig(path, bbox_inches='tight')
        plt.close()
        print(f"{method_name} visualization saved to {path}")

    # t-SNE
    if summary_topo_features_trained.shape[0] >= 2:
        try:
            tsne = TSNE(n_components=2, perplexity=min(30, len(summary_topo_features_trained) - 1), random_state=42)
            reduced_tsne = tsne.fit_transform(summary_topo_features_trained)
            plot_embedding(reduced_tsne, "t-SNE", "_tsne_trained.png")
        except Exception as e:
            print(f"t-SNE visualization failed: {e}")

    # UMAP
    if summary_topo_features_trained.shape[0] >= 2:
        try:
            reducer = umap.UMAP(n_components=2, random_state=42)
            reduced_umap = reducer.fit_transform(summary_topo_features_trained)
            plot_embedding(reduced_umap, "UMAP", "_umap_trained.png")
        except Exception as e:
            print(f"UMAP visualization failed: {e}")

    # PCA
    if summary_topo_features_trained.shape[0] >= 2:
        try:
            pca = PCA(n_components=2, random_state=42)
            reduced_pca = pca.fit_transform(summary_topo_features_trained)
            plot_embedding(reduced_pca, "PCA", "_pca_trained.png")
        except Exception as e:
            print(f"PCA visualization failed: {e}")
    
    # === Save original (unclustered) heatmap before biclustering
    ##output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    '''plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path_unclustered_heatmap) 
    
    plot_tsne(summary_topo_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path_heatmap)'''

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_gene_feature_contributions_bio_ori(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    # Barplot of all 64 features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_bio(df, barplot_path)

    # Prepare for heatmap
    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    # Handle gene name and score
    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_topo_ori(gene_name, relevance_vector, feature_names, score, output_path=None):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    # Barplot of all 64 topo features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    plot_omics_barplot_topo(df, barplot_path)

    # Prepare for heatmap
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def save_and_plot_confirmed_genes_bio(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    confirmed_genes_save_path,
    row_labels_topk,
    tag="bio",
    confirmed_gene_path="../acgnn/data/ncg_8886.txt"):
    """
    Finds confirmed cancer genes and plots their biological feature contributions.
    """

    
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]

    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics}:{cancer}" for omics in omics_order for cancer in cancer_names]

    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    confirmed_genes = [g for g in node_names_topk if g in known_cancer_genes]

    with open(confirmed_genes_save_path, "w") as f:
        for gene in confirmed_genes:
            f.write(f"{gene}\n")

    plot_dir = os.path.join(output_dir, f"{tag}_confirmed_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    # for gene_name in confirmed_genes:
    #     idx = node_names_topk.index(gene_name)
    #     relevance_vector = summary_feature_relevance[idx]
    #     score = get_scalar_score(node_scores_topk[idx])
    #     cluster_id = row_labels_topk[idx].item()

    #     output_path = os.path.join(
    #         "results/gene_prediction/bio_confirmed_feature_contributions/",
    #         f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_feature_contributions_epo{args.num_epochs}.png"
    #     )

    #     plot_gene_feature_contributions_bio(
    #         gene_name=gene,
    #         relevance_vector=relevance_vector,
    #         feature_names=feature_names,
    #         score=score,
    #         cluster_id=cluster_id,
    #         output_path=output_path
    #     )
    for gene_name in confirmed_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        cluster_id = row_labels_topk[idx].item()

        plot_gene_feature_contributions_bio(
            gene_name=gene_name,
            relevance_vector=relevance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=cluster_id,
            base_output_dir=os.path.join(
                "results/gene_prediction/bio_confirmed_feature_contributions",
                f"{args.model_type}_{args.net_type}_epo{args.num_epochs}"
            )
        )

def save_and_plot_confirmed_genes_topo(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    confirmed_genes_save_path,
    row_labels_topk,
    tag="topo",
    confirmed_gene_path="../acgnn/data/ncg_8886.txt"):
    """
    Finds confirmed cancer genes and plots their topological feature contributions.
    """

    cancer_names = [
        'BLADDER', 'BREAST', 'CERVIX', 'COLON', 'ESOPHAGUS', 'HEADNECK', 'KIDNEYCC', 'KIDNEYPC',
        'LIVER', 'LUNGAD', 'LUNGSC', 'PROSTATE', 'RECTUM', 'STOMACH', 'THYROID', 'UTERUS'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{cancer}_{omics}" for cancer in cancer_names for omics in omics_order]

    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    confirmed_genes = [g for g in node_names_topk if g in known_cancer_genes]

    with open(confirmed_genes_save_path, "w") as f:
        for gene in confirmed_genes:
            f.write(f"{gene}\n")

    ##summary_feature_relevance = extract_summary_features_np_topo(summary_feature_relevance)

    plot_dir = os.path.join(output_dir, f"{tag}_confirmed_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    for gene_name in confirmed_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        cluster_id = row_labels_topk[idx].item()

        plot_gene_feature_contributions_topo(
            gene_name=gene_name,
            relevance_vector=relevance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=cluster_id,
            base_output_dir=os.path.join(
                "results/gene_prediction/topo_confirmed_feature_contributions",
                f"{args.model_type}_{args.net_type}_epo{args.num_epochs}"
            )
        )
    
def plot_model_performance(args):
    """
    Generates and saves a scatter plot comparing AUROC and AUPRC values 
    for different models across multiple networks.

    Parameters:
    - models: List of model names.
    - networks: List of network names.
    - auroc: 2D list of AUROC scores (rows: models, cols: networks).
    - auprc: 2D list of AUPRC scores (rows: models, cols: networks).
    - args: Arguments containing model and training configuration.
    - output_dir: Directory to save the plot.
    """


    # Define models and networks
    models = ["GRAIL", "HGDC", "EMOGI", "MTGCN", "GCN", "GAT", "GraphSAGE", "GIN", "Chebnet"]
    networks = ["CPDB", "STRING", "HIPPIE"]

    # AUPRC values for ONGene and OncoKB for each model (rows: models, cols: networks)
    auroc = [
        [0.9652, 0.9578, 0.9297],  # ACGNN ACGNN & 0.9652 & 0.9783 & 0.9578 & 0.9738 & 0.9297 & 0.9597 \\
        [0.6776, 0.7133, 0.6525],  # HGDC
        [0.6735, 0.8184, 0.6672],  # EMOGI
        [0.6862, 0.7130, 0.6762],  # MTGCN
        [0.6915, 0.6688, 0.6708],  # GCN
        [0.6670, 0.8166, 0.6478],  # GAT
        [0.6664, 0.6166, 0.6571],  # GraphSAGE
        [0.5836, 0.5173, 0.5844],  # GIN
        [0.8017, 0.8777, 0.7409]   # Chebnet
    ]

    auprc = [
        [0.9783, 0.9738, 0.9597],  # ACGNN
        [0.7288, 0.7740, 0.7634],  # HGDC
        [0.7230, 0.8737, 0.7960],  # EMOGI
        [0.7712, 0.7878, 0.7785],  # MTGCN
        [0.7730, 0.7681, 0.7675],  # GCN
        [0.7086, 0.8791, 0.7496],  # GAT
        [0.7522, 0.7182, 0.7624],  # GraphSAGE
        [0.6405, 0.5918, 0.6791],  # GIN
        [0.8622, 0.9159, 0.8443]   # Chebnet
    ]

    # Compute averages for each model
    avg_auroc = np.mean(auroc, axis=1)
    avg_auprc = np.mean(auprc, axis=1)

    # Define colors for models and unique shapes for networks
    colors = ['red', 'grey', 'blue', 'green', 'purple', 'orange', 'cyan', 'brown', 'pink']
    network_markers = ['P', '^', 's']  # One shape for each network
    avg_marker = 'o'  # Marker for average points

    # Create the plot
    plt.figure(figsize=(8, 7))

    # Plot individual points for each model and network
    for i, model in enumerate(models):
        for j, network in enumerate(networks):
            plt.scatter(auprc[i][j], auroc[i][j], color=colors[i], 
                        marker=network_markers[j], s=90, alpha=0.6)

    # Add average points for each model
    for i, model in enumerate(models):
        plt.scatter(avg_auprc[i], avg_auroc[i], color=colors[i], marker=avg_marker, 
                    s=240, edgecolor='none', alpha=0.5)

    # Create legends for models (colors) and networks (shapes)
    model_legend = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], 
                            markersize=14, label=models[i], alpha=0.5) for i in range(len(models))]
    network_legend = [Line2D([0], [0], marker=network_markers[i], color='k', linestyle='None', 
                            markersize=8, label=networks[i]) for i in range(len(networks))]

    # Add legends
    network_legend_artist = plt.legend(handles=network_legend, loc='lower right', title="Networks", fontsize=12, title_fontsize=14, frameon=True)
    plt.gca().add_artist(network_legend_artist)
    plt.legend(handles=model_legend, loc='upper left', fontsize=12, frameon=True)

    # Labels and title
    plt.ylabel("AUPRC", fontsize=14)
    plt.xlabel("AUROC", fontsize=14)

    # Save the plot
    comp_output_path = os.path.join('results/gene_prediction/', f'{args.model_type}_{args.net_type}_comp_plot_epo{args.num_epochs}_infeats{args.in_feats}.jpeg')
    plt.savefig(comp_output_path, bbox_inches='tight')
    
    print(f"Comparison plot saved to {comp_output_path}")

    # Show plot
    plt.tight_layout()
    plt.close()

def train_with_relevance_tracking(args, model, graph, embeddings, labels, train_mask, output_dir):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    graph = graph.to(device)
    features = embeddings.to(device)
    labels = labels.to(device).float()
    train_mask = train_mask.to(device)

    loss_fn = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    epoch_times, cpu_usages, gpu_usages = [], [], []
    bio_scores, topo_scores = [], []

    for epoch in tqdm(range(args.num_epochs), desc="Training Progress", unit="epoch"):
        epoch_start = time.time()
        cpu_usage = psutil.cpu_percent(interval=None)
        gpu_usage = torch.cuda.memory_allocated(device) / 2048**2 if torch.cuda.is_available() else 0.0

        model.train()
        logits = model(graph, features).squeeze()
        loss = loss_fn(logits[train_mask], labels[train_mask])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_times.append(time.time() - epoch_start)
        cpu_usages.append(cpu_usage)
        gpu_usages.append(gpu_usage)

        tqdm.write(f"Epoch {epoch + 1}/{args.num_epochs}, Loss: {loss.item():.4f}, CPU: {cpu_usage}%, GPU: {gpu_usage:.2f} MB")

        # Relevance score tracking
        model.eval()
        with torch.no_grad():
            logits = model(graph, features).squeeze()
            probs = torch.sigmoid(logits).cpu()

        cancer_pred_indices = torch.nonzero(probs > 0.0, as_tuple=False).squeeze()
        if cancer_pred_indices.ndim == 0:
            cancer_pred_indices = cancer_pred_indices.unsqueeze(0)

        if len(cancer_pred_indices) > 0:
            relevance = compute_relevance_scores(model, graph, features, node_indices=cancer_pred_indices)
            bio_mean = relevance[:, 0:1024].mean().item()
            topo_mean = relevance[:, 1024:2048].mean().item()
            bio_scores.append(bio_mean)
            topo_scores.append(topo_mean)
        else:
            bio_scores.append(0.0)
            topo_scores.append(0.0)

    heatmap_path_relevance_scores = os.path.join(
                output_dir, f"{args.model_type}_{args.net_type}_relevance_scores_epo{args.num_epochs}.png"
            )

    # Relative score conversion
    if bio_scores[0] == 0:
        bio_scores_rel = bio_scores  # or set to all zeros, or handle however you prefer
        print("Warning: bio_scores[0] is zero; skipping relative normalization.")
    else:
        bio_scores_rel = [score / bio_scores[0] for score in bio_scores]
    if topo_scores[0] == 0:
        topo_scores_rel = topo_scores  # or set to all zeros, or handle however you prefer
        print("Warning: bio_scores[0] is zero; skipping relative normalization.")
    else:
        topo_scores_rel = [score / topo_scores[0] for score in topo_scores]

    # Plotting
    plt.figure(figsize=(8, 5))
    plt.plot(bio_scores_rel, label='Bio', color='#1f77b4')
    plt.plot(topo_scores_rel, label='Topo', color='#ff7f0e')

    plt.xlabel('Epoch')
    plt.ylabel('Relative Mean Relevance Score')
    plt.title('Relative Relevance Scores per Feature Group')
    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f'))

    # Remove legend frame
    plt.legend(frameon=False, loc='upper left', fontsize=18)
    
    plt.xticks(fontsize=18)
    plt.yticks(fontsize=18)
    plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
    sns.despine()

    plt.tight_layout()
    plt.savefig(os.path.join(heatmap_path_relevance_scores))
    plt.close()


    # ✅ RETURN FINAL SCORES
    return probs.numpy()  # This is the final sigmoid output per node

def get_adjacency_matrix(X, delta=0.3):
    dists = squareform(pdist(X, metric='correlation'))
    dists[dists <= delta] = 0
    return dists

def eigen_decomposition(A, topK=5):
    L = csgraph.laplacian(A, normed=True)
    eigenvalues, eigenvectors = LA.eig(L)
    idx = eigenvalues.argsort()
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    index_largest_gap = np.argsort(np.diff(eigenvalues))[::-1][:topK]
    nb_clusters = index_largest_gap + 1
    return nb_clusters, eigenvalues, eigenvectors

def eigengap_analysis(relevance_matrix, args, output_dir, name_prefix='bio', delta_row=1.185, delta_col=1.054):
    """
    Perform eigengap analysis for both rows and columns of the relevance matrix.

    Parameters:
        relevance_matrix (np.ndarray): The input relevance matrix (e.g. biological relevance scores)
        args: Namespace containing model_dir, model_type, net_type, num_epochs, etc.
        name_prefix (str): Prefix for file and log output (e.g. 'bio' or 'topo')
        delta_row (float): Distance threshold for row affinity
        delta_col (float): Distance threshold for column affinity

    Returns:
        dict: Contains row/column eigenvalues, eigenvectors, and estimated cluster counts
    """
    results = {}

    # Row-wise analysis
    affinity_row = get_adjacency_matrix(relevance_matrix, delta=delta_row)
    k_row, evals_row, evecs_row = eigen_decomposition(affinity_row)
    print(f'[{name_prefix}] Optimal number of row clusters: {k_row}')
    results['k_row'] = k_row
    results['evals_row'] = evals_row
    results['evecs_row'] = evecs_row

    # Column-wise analysis
    affinity_col = get_adjacency_matrix(relevance_matrix.T, delta=delta_col)
    k_col, evals_col, evecs_col = eigen_decomposition(affinity_col)
    print(f'[{name_prefix}] Optimal number of column clusters: {k_col}')
    results['k_col'] = k_col
    results['evals_col'] = evals_col
    results['evecs_col'] = evecs_col

    # Plot eigenvalues
    fig = plt.figure(figsize=(14, 5))
    plt.subplot(1, 2, 1)
    sns.scatterplot(x=np.arange(len(evals_row)), y=evals_row, label='Eigenvalues Rows',
                    s=100, color='black', alpha=0.6)
    plt.title(f'Row Eigengap ({name_prefix})')
    plt.legend(loc='lower right')

    plt.subplot(1, 2, 2)
    sns.scatterplot(x=np.arange(len(evals_col)), y=evals_col, label='Eigenvalues Columns',
                    s=100, color='black', alpha=0.6)
    plt.title(f'Column Eigengap ({name_prefix})')
    plt.legend(loc='lower right')
    plt.close()

    plot_path = os.path.join(
        output_dir, 
        f"{args.model_type}_{args.net_type}_eigengap_plot_{name_prefix}_epo{args.num_epochs}.png"
    )
    fig.savefig(plot_path)
    print(f'Saved eigengap plot to: {plot_path}')
    
    return results

def apply_full_spectral_biclustering_bio(
        graph, summary_bio_features, node_names, 
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None,
        n_trials=100  # New param to control how many seeds to try
    ):
    print(f"Running Spectral Biclustering on 64-dim summary bio features with {n_clusters} clusters...")
    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    row_sums = summary_bio_features.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]
    filtered_features = summary_bio_features[high_saliency_indices]

    relevance_matrix = filtered_features

    # === Use balanced biclustering
    bicluster = best_balanced_biclustering(summary_bio_features, n_clusters, n_trials)

    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === Assign cluster labels to top-k nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Optional: Heatmaps and t-SNE
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    # plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    # plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    # plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_topo(
        graph, summary_topo_features, node_names_topk,
        predicted_cancer_genes, n_clusters,
        save_path, save_row_labels_path,
        save_total_genes_per_cluster_path, 
        save_predicted_counts_path,
        output_path_genes_clusters, 
        output_path_heatmap,
        topk_node_indices=None
    ):

    print(f"Running Spectral Biclustering on topo features with {n_clusters} clusters...")
    assert summary_topo_features.shape[1] == 64, f"Expected 64 summary features, got {summary_topo_features.shape[1]}"

    # === Normalize topo features
    summary_topo_features = StandardScaler().fit_transform(summary_topo_features)

    # === Spectral Biclustering
    bicluster = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=42)
    bicluster.fit(summary_topo_features)
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_
    #graph.ndata['cluster_topo'] = torch.tensor(row_labels, dtype=torch.long)
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)

    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_topo_features.")

    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo'] = row_labels_tensor

    print("✅ Spectral Biclustering (topo) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(row_labels, node_names_topk, predicted_cancer_genes, n_clusters)
    save_predicted_counts(pred_counts, save_predicted_counts_path)
    
    # === Save original (unclustered) heatmap before biclustering
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    '''plot_topo_heatmap_raw_unsorted(summary_topo_features, predicted_indices, output_path_unclustered_heatmap) 
    
    plot_tsne(summary_topo_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    plot_topo_heatmap_unsort(summary_topo_features, row_labels, col_labels, predicted_indices, output_path_heatmap)'''

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def compute_laplacian_eigen(similarity_matrix):
    """Compute normalized Laplacian and its eigenvalues/vectors using stable decomposition."""
    L, _ = csgraph.laplacian(similarity_matrix, normed=True, return_diag=True)
    eigenvals, eigenvecs = eigh(L)
    return eigenvals, eigenvecs

def estimate_clusters_by_eigengap(eigenvals, max_k=25):
    """Estimate number of clusters using eigengap heuristic."""
    gaps = np.diff(eigenvals[1:max_k + 1])
    n_clusters_row = np.argmax(gaps) + 1
    return n_clusters_row

def eigengap_analysis(relevance_matrix, args, output_dir, name_prefix='bio', delta_row=1.185, delta_col=1.054):
    """
    Perform eigengap analysis for both rows and columns of the relevance matrix.

    Parameters:
        relevance_matrix (np.ndarray): The input relevance matrix (e.g. biological relevance scores)
        args: Namespace containing model_dir, model_type, net_type, num_epochs, etc.
        name_prefix (str): Prefix for file and log output (e.g. 'bio' or 'topo')
        delta_row (float): Not used (kept for interface compatibility)
        delta_col (float): Not used (kept for interface compatibility)

    Returns:
        dict: Contains row/column eigenvalues, eigenvectors, and estimated cluster counts
    """
    results = {}

    # Row-wise analysis using RBF kernel
    similarity_row = rbf_kernel(relevance_matrix, gamma=0.5)
    evals_row, evecs_row = compute_laplacian_eigen(similarity_row)
    k_row = estimate_clusters_by_eigengap(evals_row)
    print(f'[{name_prefix}] Optimal number of row clusters: {k_row}')
    results['k_row'] = k_row
    results['evals_row'] = evals_row
    results['evecs_row'] = evecs_row

    # Column-wise analysis using RBF kernel
    similarity_col = rbf_kernel(relevance_matrix.T, gamma=0.5)
    evals_col, evecs_col = compute_laplacian_eigen(similarity_col)
    k_col = estimate_clusters_by_eigengap(evals_col)
    print(f'[{name_prefix}] Optimal number of column clusters: {k_col}')
    results['k_col'] = k_col
    results['evals_col'] = evals_col
    results['evecs_col'] = evecs_col

    # Plot eigenvalues
    fig = plt.figure(figsize=(14, 5))
    plt.subplot(1, 2, 1)
    sns.scatterplot(x=np.arange(len(evals_row)), y=evals_row, label='Eigenvalues Rows',
                    s=100, color='black', alpha=0.6)
    plt.axvline(x=k_row, color='red', linestyle='--', label=f'k={k_row}')
    plt.title(f'Row Eigengap ({name_prefix})')
    plt.legend(loc='lower right')

    plt.subplot(1, 2, 2)
    sns.scatterplot(x=np.arange(len(evals_col)), y=evals_col, label='Eigenvalues Columns',
                    s=100, color='black', alpha=0.6)
    plt.axvline(x=k_col, color='red', linestyle='--', label=f'k={k_col}')
    plt.title(f'Column Eigengap ({name_prefix})')
    plt.legend(loc='lower right')

    fig.tight_layout()
    plt.close()

    # Save plot
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_eigengap_plot_{name_prefix}_epo{args.num_epochs}.png"
    )
    fig.savefig(plot_path)
    print(f'Saved eigengap plot to: {plot_path}')

    return results

def extract_summary_features_np_bio(bio_embeddings_np):
    """
    Extracts summary features from just the 1024 biological features (bio only).

    Args:
        bio_embeddings_np (np.ndarray): shape [num_nodes, 1024]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = bio_embeddings_np.shape
    summary_features = []

    assert num_features == 1024, f"Expected 1024 bio features, got {num_features}"

    for o_idx in range(4):  # 4 omics types
        for c_idx in range(16):  # 16 cancer types
            base = o_idx * 16 * 16 + c_idx * 16
            group = bio_embeddings_np[:, base:base + 16]  # [num_nodes, 16]
            max_vals = group.max(axis=1, keepdims=True)
            summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)

def extract_summary_features_np_topo(topo_features_np):
    """
    Extracts summary features from the topological embedding section (features 1024–2047)
    by computing the max over each 16-dimensional segment.

    Args:
        features_np (np.ndarray): shape [num_nodes, 2048]

    Returns:
        np.ndarray: shape [num_nodes, 64]
    """
    num_nodes, num_features = topo_features_np.shape
    assert num_features == 1024, f"Expected 2048 features, got {num_features}"

    # Select topological features only
    ##topo_features = features_np[:, 1024:]  # shape: [num_nodes, 1024]
    ##topo_features = topo_features_np[:, 1024:2048]
    topo_features = topo_features_np  # already 1024 features


    summary_features = []

    # Pool over 64 chunks of 16 features
    for i in range(64):
        start = i * 16
        end = start + 16
        group = topo_features[:, start:end]  # shape: [num_nodes, 16]
        max_vals = group.max(axis=1, keepdims=True)  # shape: [num_nodes, 1]
        summary_features.append(max_vals)

    return np.concatenate(summary_features, axis=1)  # shape: [num_nodes, 64]

def eigengap_analysis(feature_matrix, max_clusters=25, normalize=True, plot_path=None):
    # Step 1: Normalize features (optional)
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, d = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Print eigenvalues
    print("Eigenvalues:\n", eigenvals)

    # Save eigenvalues to CSV
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({"Eigenvalue Index": np.arange(len(eigenvals)), "Eigenvalue": eigenvals}).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 5: Optional plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]
        
        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        gaps = np.diff(eigenvals[1:max_clusters + 1])
        n_clusters_row = np.argmax(gaps) + 1  # +1 because diff shifts index

        plt.axvline(
            x=n_clusters_row,
            color='pink',
            linestyle='--',
            label=f'Eigengap → k={n_clusters_row}'
        )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=14)

        # Set integer x-axis ticks
        plt.xticks(range(1, max_clusters + 1))

        # Remove grid
        plt.grid(False)

        # Remove legend frame
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)

        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()
    else:
        # If no plot is saved, still compute n_clusters_row
        gaps = np.diff(eigenvals[1:max_clusters + 1])
        n_clusters_row = np.argmax(gaps) + 1

    return n_clusters_row, eigenvals

def eigengap_analysis(feature_matrix, max_clusters=20, normalize=True, plot_path=None, top_k=5):
    # Step 1: Normalize features (optional)
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, d = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)
    print("Eigenvalues:\n", eigenvals)

    # Save eigenvalues to CSV
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({
            "Eigenvalue Index": np.arange(len(eigenvals)),
            "Eigenvalue": eigenvals
        }).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 5: Eigengap Computation
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    top_k_indices = np.argsort(gaps)[-top_k:][::-1] + 1  # +1 to correct index offset from diff
    top_k_values = gaps[np.argsort(gaps)[-top_k:][::-1]]

    top_k_dict = dict(zip(top_k_indices, top_k_values))

    # Save top k to CSV
    if plot_path:
        topk_path = os.path.splitext(plot_path)[0] + "_topk.csv"
        pd.DataFrame({
            "Rank": np.arange(1, top_k + 1),
            "k": top_k_indices,
            "Eigengap": top_k_values
        }).to_csv(topk_path, index=False)
        print(f"Top-{top_k} eigengap values saved to: {topk_path}")

    # Plotting
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]

        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        for k in top_k_indices:
            plt.axvline(
                x=k,
                linestyle='--',
                alpha=0.5,
                label=f'k={k}'
            )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=14)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)

        # Remove duplicate legends
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        legend = plt.legend(by_label.values(), by_label.keys())
        legend.get_frame().set_linewidth(0.0)

        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return top_k_indices.tolist(), eigenvals

def run_eigengap_row_col(relevance_matrix, output_dir, args):
    # Paths for plots
    plot_path_row = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_row_epo{args.num_epochs}.png")
    plot_path_col = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_col_epo{args.num_epochs}.png")

    # Paths for saving top-k
    save_topk_row_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_top5k_row_epo{args.num_epochs}.txt")
    save_topk_col_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_top5k_col_epo{args.num_epochs}.txt")

    # Row eigengap
    top5_k_row, _ = eigengap_analysis_topk(
        relevance_matrix,
        max_clusters=25,
        normalize=True,
        plot_path=plot_path_row,
        top_k=5
    )

    # Column eigengap
    top5_k_col, _ = eigengap_analysis_topk(
        relevance_matrix.T,
        max_clusters=25,
        normalize=True,
        plot_path=plot_path_col,
        top_k=5
    )

    # Save row top-k
    with open(save_topk_row_path, 'w') as f:
        for k in top5_k_row:
            f.write(f"{k}\n")
    print(f"Saved top-5 row cluster ks to: {save_topk_row_path}")

    # Save column top-k
    with open(save_topk_col_path, 'w') as f:
        for k in top5_k_col:
            f.write(f"{k}\n")
    print(f"Saved top-5 column cluster ks to: {save_topk_col_path}")

    return top5_k_row, top5_k_col

def apply_full_spectral_biclustering_bio_no_progress_bar(
    graph, summary_bio_features, node_names, 
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None,
    n_trials=100,
    max_clusters=25,
    top_k=5
):

    print("Running Spectral Biclustering with eigengap-based top-k cluster search...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    row_sums = summary_bio_features.sum(axis=1)
    threshold = np.percentile(row_sums, 20)
    high_saliency_indices = np.where(row_sums > threshold)[0]
    filtered_features = summary_bio_features[high_saliency_indices]

    # === Eigengap for rows ===
    plot_path_row = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_row_epo{args.num_epochs}.png")
    top5_k_row, _ = eigengap_analysis_topk(
        filtered_features,
        max_clusters=max_clusters,
        normalize=True,
        plot_path=plot_path_row,
        top_k=top_k
    )

    # === Eigengap for columns ===
    plot_path_col = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_col_epo{args.num_epochs}.png")
    top5_k_col, _ = eigengap_analysis_topk(
        filtered_features.T,
        max_clusters=max_clusters,
        normalize=True,
        plot_path=plot_path_col,
        top_k=top_k
    )

    # Choose final n_clusters
    n_clusters = top5_k_row[0]  # or use mean/top vote/criteria
    print(f"→ Using n_clusters = {n_clusters} from top-5 eigengap analysis")

    # === Use balanced biclustering
    bicluster = best_balanced_biclustering(filtered_features, n_clusters, n_trials)

    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === Assign cluster labels to top-k nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    row_labels_tensor[topk_node_indices[high_saliency_indices]] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Optional: Heatmaps and t-SNE
    output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    # plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    # plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    # plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def eigengap_analysis_topk_include_k_1(feature_matrix, max_clusters=25, normalize=True, plot_path=None, top_k=5):
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, _ = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Step 5: Compute eigengaps
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    eigengap_indices = np.argsort(gaps)[::-1][:top_k]  # top-k largest gaps
    topk = (eigengap_indices + 1).tolist()  # shift index because of diff

    # Step 6: Save eigenvalues
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({
            "Eigenvalue Index": np.arange(len(eigenvals)),
            "Eigenvalue": eigenvals
        }).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 7: Plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]

        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        for k in topk:
            plt.axvline(
                x=k,
                color='pink',
                linestyle='--',
                alpha=0.7,
                label=f'k={k}'
            )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=14)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)
        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return topk, eigenvals

def eigengap_analysis_topk_skip_less_5(feature_matrix, max_clusters=25, normalize=True, plot_path=None, top_k=5):
    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, _ = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Step 5: Compute eigengaps
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    eigengap_indices = np.argsort(gaps)[::-1][:top_k]  # top-k largest gaps
    topk = (eigengap_indices + 1).tolist()  # shift index because of diff

    # Filter k > 4
    topk = [k for k in topk if k > 4]

    # Step 6: Save eigenvalues
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({
            "Eigenvalue Index": np.arange(len(eigenvals)),
            "Eigenvalue": eigenvals
        }).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 7: Plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]

        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        for k in topk:
            plt.axvline(
                x=k,
                color='pink',
                linestyle='--',
                alpha=0.7,
                label=f'k={k}'
            )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=14)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)
        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return topk, eigenvals

def get_optimal_column_clusters(score_matrix, max_clusters=25, top_k=5, normalize=True, plot_path=None):
    """
    Returns column cluster labels and sorted indices using optimal k from eigengap.
    """
    topk, _ = eigengap_analysis_topk(score_matrix.T, max_clusters=max_clusters, normalize=normalize, plot_path=plot_path, top_k=top_k)
    n_clusters_row = topk[0] if topk else 6  # fallback to 6 clusters if eigengap fails

    model = SpectralBiclustering(n_clusters=(1, n_clusters_row), method='log', random_state=0)
    model.fit(score_matrix)

    col_labels = model.column_labels_              # shape: [n_features]
    col_order = np.argsort(col_labels)

    return col_order, col_labels

def eigengap_analysis_topk(feature_matrix, max_clusters=25, normalize=True, plot_path=None, top_k=5):
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.metrics.pairwise import rbf_kernel
    from scipy.sparse.csgraph import laplacian
    from scipy.linalg import eigh
    import os

    if normalize:
        from sklearn.preprocessing import StandardScaler
        feature_matrix = StandardScaler().fit_transform(feature_matrix)

    # Step 2: Similarity matrix (RBF kernel)
    similarity = rbf_kernel(feature_matrix, gamma=0.5)

    # Step 3: Compute Laplacian
    L, _ = laplacian(similarity, normed=True, return_diag=True)

    # Step 4: Compute eigenvalues
    eigenvals, _ = eigh(L)

    # Step 5: Compute eigengaps
    gaps = np.diff(eigenvals[1:max_clusters + 1])
    all_indices = np.argsort(gaps)[::-1] + 1  # shift index because of diff

    # Step 6: Filter for k > 4 and take top_k
    topk = [k for k in all_indices if k > 4][:top_k]

    # Step 7: Save eigenvalues
    if plot_path:
        csv_path = os.path.splitext(plot_path)[0] + "_eigenvalues.csv"
        pd.DataFrame({
            "Eigenvalue Index": np.arange(len(eigenvals)),
            "Eigenvalue": eigenvals
        }).to_csv(csv_path, index=False)
        print(f"Eigenvalues saved to: {csv_path}")

    # Step 8: Plot
    if plot_path:
        x_vals = range(1, max_clusters + 1)
        y_vals = eigenvals[1:max_clusters + 1]

        plt.figure(figsize=(8, 5))
        plt.plot(x_vals, y_vals, color='blue', linestyle='-', label='Eigenvalues')
        plt.plot(x_vals, y_vals, color='red', marker='+', linestyle='None')

        for k in topk:
            plt.axvline(
                x=k,
                color='pink',
                linestyle='--',
                alpha=0.7,
                label=f'k={k}'
            )

        plt.xlabel("Eigenvalue Index", fontsize=12)
        plt.ylabel("Eigenvalue", fontsize=12)
        plt.title("Eigengap Analysis", fontsize=14)
        plt.xticks(range(1, max_clusters + 1))
        plt.grid(False)
        legend = plt.legend()
        legend.get_frame().set_linewidth(0.0)
        plt.tight_layout()
        plt.savefig(plot_path)
        plt.close()

    return topk, eigenvals

def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names, 
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None,
    n_trials=10,
    max_clusters_row=25,
    max_clusters_col=10,
    top_k=5
):
    print("🧪 Running Spectral Biclustering with eigengap-based top-k cluster search...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    # === Saliency-based filtering
    row_sums = summary_bio_features.sum(axis=1)
    ##threshold = np.percentile(row_sums, 20)
    # threshold = np.percentile(row_sums, 0)
    # high_saliency_indices = np.where(row_sums > threshold)[0]
    ##filtered_features = summary_bio_features[high_saliency_indices]
    filtered_features = summary_bio_features

    # === Eigengap for rows
    plot_path_row = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_row_epo{args.num_epochs}.png")
    top5_k_row, _ = eigengap_analysis_topk(
        filtered_features,
        max_clusters=max_clusters_row,
        normalize=True,
        plot_path=plot_path_row,
        top_k=top_k
    )

    # === Eigengap for columns
    plot_path_col = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_col_epo{args.num_epochs}.png")
    top5_k_col, _ = eigengap_analysis_topk(
        filtered_features.T,
        max_clusters=max_clusters_col,
        normalize=True,
        plot_path=plot_path_col,
        top_k=top_k
    )

    # Choose final number of clusters (rows only)
    ##n_clusters = top5_k_row[0]
    # Filter top-k row cluster candidates to ensure k >= 5
    valid_ks = [k for k in top5_k_col if k >= 5]
    if not valid_ks:
        raise ValueError(f"No valid row cluster counts ≥ 5 found in top-k candidates: {top5_k_row}")

    n_clusters = valid_ks[0]
    print(f"→ Using n_clusters = {n_clusters} (filtered to k ≥ 5) from top-5 eigengap analysis")

    ##print(f"→ Using n_clusters = {n_clusters} from top-5 eigengap analysis")

    # === Use balanced biclustering with progress bar

    best_model = None
    best_score = np.inf  # lower MSE is better

    print("🔁 Running biclustering trials:")
    for i in tqdm(range(n_trials), desc="Biclustering Trials", ncols=80):
        model = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=i)
        model.fit(filtered_features)

        # Reconstruct the data matrix from the clustering
        reconstructed = filtered_features[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        
        # Calculate reconstruction error
        mse = mean_squared_error(filtered_features, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model


    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === Assign cluster labels to top-k nodes in graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("You must provide `topk_node_indices` corresponding to the rows in summary_bio_features.")
    #row_labels_tensor[topk_node_indices[high_saliency_indices]] = torch.tensor(row_labels, dtype=torch.long)
    #indices_to_update = torch.tensor(topk_node_indices[high_saliency_indices], dtype=torch.long)
    # indices_to_update = torch.tensor(np.array(topk_node_indices)[high_saliency_indices], dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)

    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering (summary bio) complete.")

    # === Save results
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # Optional heatmap & t-SNE (uncomment as needed)
    # output_path_unclustered_heatmap = output_path_heatmap.replace(".png", "_unclustered.png")
    # plot_bio_heatmap_raw_unsorted(summary_bio_features, predicted_indices, output_path_unclustered_heatmap)
    # plot_tsne(summary_bio_features, row_labels, predicted_indices, n_clusters, output_path_genes_clusters)
    # plot_bio_heatmap_unsort(summary_bio_features, row_labels, col_labels, predicted_indices, output_path_heatmap)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_bio_biclustering_heatmap_clusters_unsort(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None,
    sorted_node_indices=None  # corresponds to topk_node_indices[high_saliency_indices]
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.colors import LinearSegmentedColormap, to_rgba

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Normalize scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    # Sort rows by cluster labels
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    col_order, col_labels = get_optimal_column_clusters(sorted_scores)
    
    raw_feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        raw_feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    if col_labels is not None:
        sorted_scores = sorted_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [raw_feature_colors[i] for i in col_order]
    else:
        feature_colors = raw_feature_colors


    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(x=i + 0.5, height=val, width=1.0, color=color, edgecolor='black', linewidth=0.5, alpha=0.3 + 0.7 * val)

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency side curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)),
        0,
        saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bar under feature bar
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end + 1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_spectral_biclustering_heatmap_topk(features, node_names, feature_names, pred,
                                             cgc_path, model_dir,
                                             n_row_clusters=17, n_col_clusters=5,
                                             top_k=1000,
                                             output_name='biclustering_input_topk.png'):
    """
    Performs spectral biclustering on the top-K predicted genes and plots a heatmap.

    Args:
        features (ndarray): Full feature matrix of shape (n_genes, n_features)
        node_names (ndarray): Array with gene names, shape (n_genes, 2), index 1 holds gene symbols
        feature_names (list): List of feature names
        pred (pd.DataFrame): DataFrame with predicted genes, must contain column 'Name'
        cgc_path (str): Path to Cancer Gene Census file with 'Gene Symbol' and 'Role in Cancer'
        model_dir (str): Output directory to save the heatmap
        n_row_clusters (int): Number of gene (row) clusters
        n_col_clusters (int): Number of feature (column) clusters
        top_k (int): Number of top predictions to include
        output_name (str): Name of the output PNG file
    """
    # Step 1: Prepare top-K gene-feature matrix
    X_df = pd.DataFrame(features, index=node_names[:, 1], columns=feature_names)
    X_topk = X_df[X_df.index.isin(pred.head(top_k).Name)]
    print(f"Selected {X_topk.shape[0]} Genes for Spectral Biclustering")

    # Step 2: Normalize
    X_norm = pd.DataFrame(StandardScaler().fit_transform(X_topk),
                          index=X_topk.index, columns=X_topk.columns)

    # Step 3: Biclustering
    model = SpectralBiclustering(n_clusters=(5,5),
                                 method='bistochastic',
                                 svd_method='randomized',
                                 n_jobs=-1, random_state=0)
    model.fit(X_norm)

    # Step 4: Row/column orders
    row_labels = pd.Series(model.row_labels_, index=X_norm.index)
    col_labels = pd.Series(model.column_labels_, index=X_norm.columns)

    ordered_rows = row_labels.sort_values().index.tolist()
    ordered_cols = col_labels.sort_values().index.tolist()

    X_plot = X_norm.loc[ordered_rows, ordered_cols]

    # Step 5: Cancer gene annotations
    cgc = pd.read_csv(cgc_path).set_index('Gene Symbol')

    gene_roles = pd.Series(0, index=X_norm.index, name='CancerGeneCensus')
    gene_roles[cgc['Role in Cancer'].str.contains('oncogene', na=False)] = 1
    gene_roles[cgc['Role in Cancer'].str.contains('TSG', na=False)] = 2
    gene_roles = gene_roles.loc[X_norm.index]

    # Step 6: Color palettes
    row_cluster_palette = dict(zip(range(n_row_clusters), sns.color_palette("deep", n_colors=n_row_clusters)))
    col_cluster_palette = dict(zip(range(n_col_clusters), sns.color_palette("muted", n_colors=n_col_clusters)))
    gene_role_palette = {0: 'grey', 1: 'darkred', 2: 'darkgreen'}

    row_colors = pd.DataFrame({
        'Role': gene_roles.map(gene_role_palette),
        'Cluster': row_labels.map(row_cluster_palette)
    }).loc[ordered_rows]

    col_colors = col_labels.map(col_cluster_palette).loc[ordered_cols]

    # Step 7: Plot heatmap
    cm = sns.clustermap(
        X_plot,
        method=None,
        metric='correlation',
        cmap='RdBu_r',
        row_cluster=False,
        col_cluster=False,
        row_colors=row_colors,
        col_colors=col_colors,
        yticklabels=7,
        xticklabels=1,
        figsize=(20, 30),
        center=0
    )

    # Step 8: Save
    os.makedirs(model_dir, exist_ok=True)
    out_path = os.path.join(model_dir, output_name)
    cm.savefig(out_path, dpi=300)
    print(f"Saved biclustering heatmap to: {out_path}")

    # Optionally return results
    return {
        "bicluster_model": model,
        "row_labels": row_labels,
        "col_labels": col_labels,
        "gene_roles": gene_roles,
        "X_plot": X_plot
    }

def plot_biclustering_input_feature_heatmap(
    feature_matrix: np.ndarray,
    node_names_topk: list,
    output_path: str,
    title: str = "Biclustering Heatmap (Input Features)",
    cmap: str = "viridis"
):
    """
    Applies spectral biclustering to input features of top-k predicted genes
    and visualizes the result as a heatmap.

    Parameters:
    - feature_matrix: np.ndarray, shape [n_genes, n_features]
        Input summary features (e.g., 64-dim bio or topo features)
    - node_names_topk: list of str
        Gene names for top-k predicted genes
    - output_path: str
        File path to save the heatmap
    - title: str
        Plot title
    - cmap: str
        Colormap for heatmap
    """
    assert feature_matrix.shape[0] == len(node_names_topk), \
        f"Mismatch: {feature_matrix.shape[0]} rows vs {len(node_names_topk)} gene names"

    # Normalize the feature matrix
    feature_matrix_norm = normalize(feature_matrix, axis=1)

    # Apply spectral biclustering
    n_clusters = 10
    model = SpectralBiclustering(n_clusters=n_clusters, method='log', random_state=0)
    model.fit(feature_matrix_norm)

    # Reorder matrix
    row_order = np.argsort(model.row_labels_)
    col_order = np.argsort(model.column_labels_)
    clustered_matrix = feature_matrix_norm[row_order][:, col_order]

    # Reorder gene names for heatmap annotation
    reordered_gene_names = [node_names_topk[i] for i in row_order]

    # Plot
    plt.figure(figsize=(10, 8))
    sns.heatmap(clustered_matrix, cmap=cmap, yticklabels=reordered_gene_names, xticklabels=False)
    plt.title(title, fontsize=14)
    plt.xlabel("Features")
    plt.ylabel("Top-k Genes (Reordered)")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()

    print(f"[Saved] Input feature biclustering heatmap → {output_path}")

def plot_spectral_biclustering_heatmap_topk_(
    scores,
    row_labels,
    col_labels,
    row_order,
    col_order,
    omics_splits,
    output_path,
    omics_colors=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.colors import LinearSegmentedColormap, to_rgba
    import numpy as np

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Reorder the matrix
    sorted_scores = scores[np.ix_(row_order, col_order)]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))
    feature_colors = [feature_colors[i] for i in col_order]
    feature_names = [feature_names[i] for i in col_order]

    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(x=i + 0.5, height=val, width=1.0, color=color, edgecolor='black', linewidth=0.5, alpha=0.3 + 0.7 * val)

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Row cluster sizes
    unique_clusters, cluster_sizes = np.unique(row_labels[row_order], return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # Cluster stripe
    for i, cluster in enumerate(row_labels[row_order]):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # X-axis tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency side curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)),
        0,
        saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bar under feature bar
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, col_order.index(start):col_order.index(end)+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None
):  
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from sklearn.cluster import SpectralBiclustering
    from matplotlib.colors import LinearSegmentedColormap

    # 🔹 Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Biclustering: 12 row clusters, 7 column clusters
    model = SpectralBiclustering(n_clusters=(5, 5), method='bistochastic', random_state=0)
    model.fit(relevance_scores)

    # Reorder matrix by clustered indices
    data_reordered = relevance_scores[model.row_order_, :][:, model.column_order_]
    feature_names_reordered = [feature_names[i] for i in model.column_order_]

    # Build feature_colors for reordered columns
    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))
    feature_colors = [feature_colors[i] for i in model.column_order_]

    # Begin plotting
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    # Top bar chart
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names_reordered))
    ax_bar.set_ylim(0, 1.1)

    feature_means = data_reordered.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(data_reordered, 99)

    sns.heatmap(
        data_reordered,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Show tick labels with small tick marks
    ax.set_xticks(np.arange(len(feature_names_reordered)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names_reordered], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # Saliency curve (row-wise sums)
    saliency_sums = data_reordered.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    reordered_omics_splits = {omics: [] for omics in omics_order}
    for i, name in enumerate(feature_names_reordered):
        omics = name.split(":")[0].lower()
        reordered_omics_splits[omics].append(i)

    for omics in omics_order:
        if reordered_omics_splits[omics]:
            start = min(reordered_omics_splits[omics])
            end = max(reordered_omics_splits[omics])
            group_center = (start + end) / 2 + 0.5
            mean_val = data_reordered[:, start:end+1].mean()
            norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
            ax.bar(
                x=group_center,
                height=0.15,
                width=end - start + 1,
                bottom=len(relevance_scores) + 1.5,
                color=omics_colors[omics],
                edgecolor='black',
                linewidth=1,
                alpha=min(1.0, 0.3 + 0.7 * norm_mean)
            )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_no_column_color_bar(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # 🔹 Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # 🔁 Reorder based on biclustering if provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])
    ax        = fig.add_subplot(gs[1:13, 2:45])
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(relevance_scores, 99)

    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def apply_full_spectral_biclustering_bio_pass(
    graph, summary_bio_features, node_names, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    print("🧪 Running Spectral Biclustering with fixed (10, 7) clusters...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    # === Biclustering
    n_clusters_row = 5
    n_clusters_col = 5
    n_clusters=(5,5)

    best_model = None
    best_score = np.inf

    print("🔁 Running biclustering trials:")
    for i in range(10):  # or use tqdm
        model = SpectralBiclustering(n_clusters=(5,5), method='bistochastic',
                             svd_method='randomized', random_state=0
                            )
        model.fit(summary_bio_features)

        reconstructed = summary_bio_features[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # Assign cluster labels to top-k nodes
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features,#relevance_scores_topk.detach().cpu().numpy(),
        #row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
        #gene_names=node_names_topk,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )
    
    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_bio_biclustering_heatmap_unsort_use_clustermap_no_clustering(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd
    from matplotlib.colors import LinearSegmentedColormap

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min() + 1e-8) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    # Column (feature) colors by omics type
    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))
    feature_colors = [feature_colors[i] for i in original_col_indices]

    # Define row_colors (if labels provided)
    row_colors = None
    if row_labels is not None:
        row_labels = np.array(row_labels)
        unique_row_labels = np.unique(row_labels)
        row_palette = sns.color_palette("Set2", len(unique_row_labels))
        row_lut = dict(zip(unique_row_labels, row_palette))
        row_colors = pd.Series(row_labels).map(row_lut).to_numpy()

    # Define col_colors (if labels provided)
    col_colors = None
    if col_labels is not None:
        col_labels = np.array(col_labels)
        unique_col_labels = np.unique(col_labels)
        col_palette = sns.color_palette("tab20", len(unique_col_labels))
        col_lut = dict(zip(unique_col_labels, col_palette))
        col_colors = pd.Series(col_labels).map(col_lut).to_numpy()
    else:
        col_colors = pd.Series(feature_colors)

    # Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # Create clustermap
    g = sns.clustermap(
        relevance_scores,
        # row_cluster=True,
        # col_cluster=True,
        row_cluster=False,
        col_cluster=False,
        row_colors=row_colors,
        col_colors=col_colors,
        cmap=bluish_gray_gradient,
        xticklabels=False,
        yticklabels=False,
        figsize=(18, 17),
        cbar_pos=(0.03, 0.8, 0.02, 0.15)  # adjust colorbar position
    )

    # Improve x-axis tick label coloring
    ax = g.ax_heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([name.split(": ")[1] for name in feature_names], rotation=90, fontsize=10)
    for label, color in zip(ax.get_xticklabels(), col_colors):
        label.set_color(color)

    # Optional: label color bar axes
    g.cax.set_ylabel("Relevance Score", fontsize=14)
    g.cax.yaxis.label.set_color("#85929e")
    g.cax.tick_params(colors="#85929e", labelsize=12)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_clustermap_no_top_cluster_color_bar(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd

    # Normalize the relevance scores to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    # Default omics colors
    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If there are fewer features, trim names
    feature_names = feature_names[:relevance_scores.shape[1]]
    df = pd.DataFrame(relevance_scores, columns=feature_names)

    # Column colors for omics (used as a colorbar)
    col_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        color = omics_colors[omics]
        col_colors.extend([color] * (end - start + 1))
    col_colors = col_colors[:relevance_scores.shape[1]]

    # Reorder rows/columns if labels are provided
    if row_labels is not None:
        df = df.iloc[np.argsort(row_labels)]
    if col_labels is not None:
        df = df.iloc[:, np.argsort(col_labels)]
        col_colors = [col_colors[i] for i in np.argsort(col_labels)]


    # Plot clustermap without clustering
    cg = sns.clustermap(
        df,
        cmap="Blues",
        row_cluster=False,
        col_cluster=False,
        col_colors=col_colors,
        xticklabels=True,
        yticklabels=False,
        figsize=(18, 14),
        cbar_pos=(0.02, 0.8, 0.02, 0.18),
        cbar_kws={"label": "Relevance Score"},
        dendrogram_ratio=(0.01, 0.01)
    )

    # Customize axis label colors based on omics type
    for label in cg.ax_heatmap.get_xticklabels():
        label_text = label.get_text()
        for omics in omics_order:
            if omics.upper() in label_text:
                label.set_color(omics_colors[omics])
                label.set_rotation(90)
                label.set_fontsize(10)
                break

    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_no_row_cluster_color_bar(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # 🔁 Reorder if labels are provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=16, ncols=50, wspace=0.0, hspace=0.0)

    ax_col_clusterbar = fig.add_subplot(gs[0, 2:45])  # 🔷 Top cluster color bar
    ax_bar = fig.add_subplot(gs[1, 2:45])
    ax = fig.add_subplot(gs[2:14, 2:45])
    ax_curve = fig.add_subplot(gs[2:14, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[6:10, 49])

    # 🔷 Draw column cluster color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        n_clusters = len(unique_clusters)
        palette = sns.color_palette("tab20", n_colors=n_clusters)
        cluster_color_map = {cl: palette[i] for i, cl in enumerate(unique_clusters)}
        cluster_colors = [cluster_color_map[cl] for cl in col_labels]
        ax_col_clusterbar.imshow([cluster_colors], aspect='auto')
        ax_col_clusterbar.axis("off")

    # Bar chart of average relevance
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, height=mean_val, width=1.0,
            bottom=0, color=color, edgecolor='black',
            linewidth=0.5, alpha=0.3 + 0.7 * mean_val
        )

    # Main heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin = 0
    vmax = np.percentile(relevance_scores, 99)
    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # Style color bar and ticks
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Relevance score curve
    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_clustermap(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd

    # Normalize the relevance scores to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 100

    # Default omics colors
    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If there are fewer features, trim names
    feature_names = feature_names[:relevance_scores.shape[1]]
    df = pd.DataFrame(relevance_scores, columns=feature_names)

    # Column colors for omics (used as a colorbar)
    col_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        color = omics_colors[omics]
        col_colors.extend([color] * (end - start + 1))
    col_colors = col_colors[:relevance_scores.shape[1]]

    # Reorder rows/columns if labels are provided
    if row_labels is not None:
        df = df.iloc[np.argsort(row_labels)]
    if col_labels is not None:
        df = df.iloc[:, np.argsort(col_labels)]
        col_colors = [col_colors[i] for i in np.argsort(col_labels)]


    # Plot clustermap without clustering
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    cg = sns.clustermap(
        df,
        #cmap="Blues",
        cmap=bluish_gray_gradient,
        row_cluster=False,
        col_cluster=False,
        col_colors=col_colors,
        xticklabels=True,
        yticklabels=False,
        figsize=(18, 14),
        cbar_pos=(0.02, 0.8, 0.02, 0.18),
        cbar_kws={"label": "Relevance Score"},
        dendrogram_ratio=(0.01, 0.01)
    )

    # Customize axis label colors based on omics type
    for label in cg.ax_heatmap.get_xticklabels():
        label_text = label.get_text()
        for omics in omics_order:
            if omics.upper() in label_text:
                label.set_color(omics_colors[omics])
                label.set_rotation(90)
                label.set_fontsize(10)
                break

    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_no_col_cluster_color_bar(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):  
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap, to_rgba

    # 🔹 Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # 🔹 Reorder rows by cluster
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    fig = plt.figure(figsize=(18, 18))
    gs = fig.add_gridspec(nrows=16, ncols=50, wspace=0.0, hspace=0.0)

    ax_row_clusterbar = fig.add_subplot(gs[2:14, 0])     # 🔹 Left cluster color bar
    ax_bar            = fig.add_subplot(gs[0, 2:45])     # 🔹 Top bar plot
    ax                = fig.add_subplot(gs[2:14, 2:45])   # 🔹 Heatmap
    ax_curve          = fig.add_subplot(gs[2:14, 45:48], sharey=ax)  # 🔹 Saliency curve
    ax_cbar           = fig.add_subplot(gs[6:10, 49])     # 🔹 Colorbar

    # 🔹 Top bar plot of feature means
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Row cluster color bar
    unique_clusters = np.unique(sorted_clusters)
    cluster_palette = sns.color_palette("tab20", len(unique_clusters))
    cluster_color_map = {cluster: cluster_palette[i] for i, cluster in enumerate(unique_clusters)}
    cluster_colors = [cluster_color_map[cl] for cl in sorted_clusters]

    ax_row_clusterbar.imshow(np.array(cluster_colors).reshape(-1, 1), aspect='auto')
    ax_row_clusterbar.axis("off")

    # 🔹 Feature tick labels with omics colors
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)

    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    saliency_sums = saliency_sums[cluster_order]
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    # 🔹 Omics mean bar (bottom of heatmap)
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_group_by_omics(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # 🔹 Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # 🔹 Reorder rows by cluster
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    fig = plt.figure(figsize=(18, 18))
    gs = fig.add_gridspec(nrows=20, ncols=50, wspace=0.0, hspace=0.0)

    ax_col_bar        = fig.add_subplot(gs[0, 2:45])       # 🔹 Top bar (omics color bar)
    ax_top_means      = fig.add_subplot(gs[1, 2:45])       # 🔹 Top bar plot of feature means
    ax_row_clusterbar = fig.add_subplot(gs[2:16, 0])       # 🔹 Row cluster color bar
    ax                = fig.add_subplot(gs[2:16, 2:45])     # 🔹 Heatmap
    ax_curve          = fig.add_subplot(gs[2:16, 45:48], sharey=ax)  # 🔹 Saliency curve
    ax_cbar           = fig.add_subplot(gs[6:10, 49])       # 🔹 Colorbar

    # 🔹 Omics type color bar (columns)
    ax_col_bar.axis("off")
    ax_col_bar.set_xlim(0, len(feature_names))
    ax_col_bar.set_ylim(0, 1)
    for i, color in enumerate(feature_colors):
        ax_col_bar.bar(
            x=i + 0.5,
            height=1,
            width=1.0,
            color=color,
            edgecolor='black',
            linewidth=0.2,
            align='center'
        )

    # 🔹 Top bar plot of feature means
    ax_top_means.axis("off")
    ax_top_means.set_xlim(0, len(feature_names))
    ax_top_means.set_ylim(0, 1.1)

    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_top_means.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Row cluster color bar
    unique_clusters = np.unique(sorted_clusters)
    cluster_palette = sns.color_palette("tab20", len(unique_clusters))
    cluster_color_map = {cluster: cluster_palette[i] for i, cluster in enumerate(unique_clusters)}
    cluster_colors = [cluster_color_map[cl] for cl in sorted_clusters]

    ax_row_clusterbar.imshow(np.array(cluster_colors).reshape(-1, 1), aspect='auto')
    ax_row_clusterbar.axis("off")

    # 🔹 Feature tick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)

    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    saliency_sums = saliency_sums[cluster_order]
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5,
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    # 🔹 Omics mean bar (bottom of heatmap)
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_not_good(
    args,
    relevance_scores,
    omics_splits,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # 🔹 Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    # 🔹 Sort rows by cluster
    row_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[row_order]
    sorted_row_clusters = row_labels[row_order]

    # 🔹 No column sorting — preserve original order
    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    row_labels = np.array(row_labels)[original_col_indices]

    # 🔹 Column color map by cluster ID
    unique_col_clusters = np.unique(row_labels)
    col_palette = sns.color_palette("tab10", len(unique_col_clusters))
    col_color_map = {cluster: col_palette[i] for i, cluster in enumerate(unique_col_clusters)}
    column_colors = [col_color_map[c] for c in row_labels]

    # 🔹 Row cluster colors
    unique_row_clusters = np.unique(sorted_row_clusters)
    row_palette = sns.color_palette("tab20", len(unique_row_clusters))
    row_color_map = {cluster: row_palette[i] for i, cluster in enumerate(unique_row_clusters)}
    row_colors = [row_color_map[c] for c in sorted_row_clusters]

    # 🔹 Column cluster color bar (top of heatmap)
    if col_labels is not None:
        col_labels = np.array(col_labels)
        unique_col_clusters = np.unique(col_labels)
        col_palette = sns.color_palette("tab10", len(unique_col_clusters))
        col_color_map = {cluster: col_palette[i] for i, cluster in enumerate(unique_col_clusters)}
        column_cluster_colors = [col_color_map[c] for c in col_labels]
    else:
        column_cluster_colors = ['#cccccc'] * relevance_scores.shape[1]  # default gray

    fig = plt.figure(figsize=(18, 18))
    gs = fig.add_gridspec(nrows=20, ncols=50, wspace=0.0, hspace=0.0)

    ax_col_bar        = fig.add_subplot(gs[0, 2:45])       # 🔹 Column cluster color bar
    ax_top_means      = fig.add_subplot(gs[1, 2:45])       # 🔹 Top feature mean bar
    ax_row_clusterbar = fig.add_subplot(gs[2:16, 0])       # 🔹 Row cluster color bar
    ax                = fig.add_subplot(gs[2:16, 2:45])    # 🔹 Heatmap
    ax_curve          = fig.add_subplot(gs[2:16, 45:48], sharey=ax)  # 🔹 Saliency curve
    ax_cbar           = fig.add_subplot(gs[6:10, 49])      # 🔹 Colorbar


    # 🔹 Column cluster color bar
    ax_col_bar.imshow([column_cluster_colors], aspect='auto')
    ax_col_bar.axis("off")

    # 🔹 Feature mean bar (same as before)
    ax_top_means.axis("off")
    ax_top_means.set_xlim(0, len(column_colors))
    ax_top_means.set_ylim(0, 1.1)
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, column_colors)):
        ax_top_means.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    # 🔹 Heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Row cluster color bar
    ax_row_clusterbar.imshow(np.array(row_colors).reshape(-1, 1), aspect='auto')
    ax_row_clusterbar.axis("off")

    # 🔹 Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    saliency_sums = saliency_sums[row_order]
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5,
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_only_top_cluster_bar(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # 🔁 Reorder if labels are provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=16, ncols=50, wspace=0.0, hspace=0.0)

    ax_col_clusterbar = fig.add_subplot(gs[0, 2:45])  # 🔷 Top cluster color bar
    ax_bar = fig.add_subplot(gs[1, 2:45])
    ax = fig.add_subplot(gs[2:14, 2:45])
    ax_curve = fig.add_subplot(gs[2:14, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[6:10, 49])

    # 🔷 Draw column cluster color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        n_clusters = len(unique_clusters)
        palette = sns.color_palette("tab20", n_colors=n_clusters)
        cluster_color_map = {cl: palette[i] for i, cl in enumerate(unique_clusters)}
        cluster_colors = [cluster_color_map[cl] for cl in col_labels]
        ax_col_clusterbar.imshow([cluster_colors], aspect='auto')
        ax_col_clusterbar.axis("off")

    # Bar chart of average relevance
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, height=mean_val, width=1.0,
            bottom=0, color=color, edgecolor='black',
            linewidth=0.5, alpha=0.3 + 0.7 * mean_val
        )

    # Main heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin = 0
    vmax = np.percentile(relevance_scores, 99)
    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # Style color bar and ticks
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Relevance score curve
    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # 🔁 Reorder if labels are provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=16, ncols=52, wspace=0.0, hspace=0.0)

    ax_row_clusterbar = fig.add_subplot(gs[2:14, 0])         # ⬅️ Row cluster color bar
    ax_col_clusterbar = fig.add_subplot(gs[0, 2:47])         # 🔷 Top cluster color bar
    ax_bar = fig.add_subplot(gs[1, 2:47])                    # Bar chart
    ax = fig.add_subplot(gs[2:14, 2:47])                     # Heatmap
    ax_curve = fig.add_subplot(gs[2:14, 47:50], sharey=ax)   # Right saliency curve
    ax_cbar = fig.add_subplot(gs[6:10, 51])                  # Colorbar

    # 🔷 Draw column cluster color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        n_clusters = len(unique_clusters)
        palette = sns.color_palette("tab20", n_colors=n_clusters)
        cluster_color_map = {cl: palette[i] for i, cl in enumerate(unique_clusters)}
        cluster_colors = [cluster_color_map[cl] for cl in col_labels]
        ax_col_clusterbar.imshow([cluster_colors], aspect='auto')
        ax_col_clusterbar.axis("off")

    # 🔷 Draw row cluster color bar
    if row_labels is not None:
        unique_row_clusters = np.unique(row_labels)
        n_row_clusters = len(unique_row_clusters)
        row_palette = sns.color_palette("tab20", n_colors=n_row_clusters)
        row_cluster_color_map = {cl: row_palette[i] for i, cl in enumerate(unique_row_clusters)}
        row_cluster_colors = [row_cluster_color_map[cl] for cl in row_labels[row_order]]
        ax_row_clusterbar.imshow(np.array(row_cluster_colors).reshape(-1, 1), aspect='auto')
        ax_row_clusterbar.axis("off")

    # Bar chart of average relevance
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, height=mean_val, width=1.0,
            bottom=0, color=color, edgecolor='black',
            linewidth=0.5, alpha=0.3 + 0.7 * mean_val
        )

    # Main heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin = 0
    vmax = np.percentile(relevance_scores, 99)
    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # Style color bar and ticks
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Relevance score curve
    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()


def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    print("🧪 Running Spectral Biclustering with fixed (10, 7) clusters...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"

    # === Biclustering
    n_clusters_row = 16
    n_clusters_col = 10
    #n_clusters=(5,5)

    best_model = None
    best_score = np.inf

    print("🔁 Running biclustering trials:")
    for i in range(10):  # or use tqdm
        model = SpectralBiclustering(n_clusters=(n_clusters_row,n_clusters_col), method='bistochastic',
                             svd_method='randomized', random_state=0
                            )
        model.fit(summary_bio_features)

        reconstructed = summary_bio_features[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # Assign cluster labels to top-k nodes
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)



    plot_bio_biclustering_heatmap(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )
    
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features,#relevance_scores_topk.detach().cpu().numpy(),
        #row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
        #gene_names=node_names_topk,
        omics_splits=omics_splits,
        output_path=output_path_heatmap,
        row_labels=row_labels,
        col_labels=col_labels
    )
    
    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_bio_biclustering_heatmap_unsort_not_clustermap(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap

    # Custom cluster colors
    CLUSTER_COLORS = {
        0: '#0077B6',   1: '#0000FF',   2: '#00B4D8',   3: '#48EAC4',
        4: '#F1C0E8',   5: '#B9FBC0',   6: '#32CD32',   7: '#bee1e6',
        8: '#8A2BE2',   9: '#E377C2',  10: '#8EECF5',  11: '#A3C4F3',
        12: '#FFB347', 13: '#FFD700',  14: '#FF69B4',  15: '#CD5C5C',
        16: '#7FFFD4', 17: '#FF7F50',  18: '#C71585',  19: '#20B2AA'
    }

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # 🔁 Reorder if labels are provided
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=16, ncols=52, wspace=0.0, hspace=0.0)

    ax_row_clusterbar = fig.add_subplot(gs[2:14, 0])         # ⬅️ Row cluster color bar
    ax_col_clusterbar = fig.add_subplot(gs[0, 2:47])         # 🔷 Top cluster color bar
    ax_bar = fig.add_subplot(gs[1, 2:47])                    # Bar chart
    ax = fig.add_subplot(gs[2:14, 2:47])                     # Heatmap
    ax_curve = fig.add_subplot(gs[2:14, 47:50], sharey=ax)   # Right saliency curve
    ax_cbar = fig.add_subplot(gs[6:10, 51])                  # Colorbar


    if col_labels is not None:
        cluster_colors = [to_rgb(CLUSTER_COLORS.get(cl, '#808080')) for cl in col_labels]
        ax_col_clusterbar.imshow([cluster_colors], aspect='auto')
        ax_col_clusterbar.axis("off")



    # ⬅️ Row cluster color bar
    if row_labels is not None:
        row_cluster_colors = [CLUSTER_COLORS.get(int(cl), '#000000') for cl in row_labels[row_order]]
        ax_row_clusterbar.imshow(np.array(row_cluster_colors).reshape(-1, 1), aspect='auto')
        ax_row_clusterbar.axis("off")

    # Bar chart of average relevance
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, height=mean_val, width=1.0,
            bottom=0, color=color, edgecolor='black',
            linewidth=0.5, alpha=0.3 + 0.7 * mean_val
        )

    # Main heatmap
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin = 0
    vmax = np.percentile(relevance_scores, 99)
    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin, vmax=vmax,
        xticklabels=False, yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # Style color bar and ticks
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=3)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Relevance score curve
    ax_curve.set_ylim(0, len(relevance_scores))
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    for spine in ['right', 'top', 'left', 'bottom']:
        ax_curve.spines[spine].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(y, 0, saliency_sums, color='#a9cce3', alpha=0.8, linewidth=3)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_clustermap(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None,
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd

    # Define cluster colors
    CLUSTER_COLORS = {
        0: '#0077B6', 1: '#0000FF', 2: '#00B4D8', 3: '#48EAC4', 4: '#F1C0E8',
        5: '#B9FBC0', 6: '#32CD32', 7: '#bee1e6', 8: '#8A2BE2', 9: '#E377C2',
        10: '#8EECF5', 11: '#A3C4F3', 12: '#FFB347', 13: '#FFD700', 14: '#FF69B4',
        15: '#CD5C5C', 16: '#7FFFD4', 17: '#FF7F50', 18: '#C71585', 19: '#20B2AA'
    }

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min() + 1e-8) * 20

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # Reorder based on provided labels
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
        row_labels = np.array(row_labels)[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    # Cluster colors for rows
    if row_labels is not None:
        row_color_labels = [CLUSTER_COLORS[int(lbl)] for lbl in row_labels]
    else:
        row_color_labels = None

    # Column color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        col_cluster_color_map = {cl: CLUSTER_COLORS[cl % len(CLUSTER_COLORS)] for cl in unique_clusters}
        col_color_labels = [col_cluster_color_map[cl] for cl in col_labels]
    else:
        col_color_labels = feature_colors

    # Create dataframe for clustermap
    df = pd.DataFrame(relevance_scores, columns=feature_names)

    # Plot clustermap without clustering
    g = sns.clustermap(
        df,
        row_cluster=False,
        col_cluster=False,
        row_colors=row_color_labels,
        col_colors=col_color_labels,
        cmap="Blues",
        xticklabels=False,
        yticklabels=False,
        figsize=(18, 17),
        cbar_kws={"label": "Relevance Score"},
    )

    # Color bar ticks
    g.cax.yaxis.label.set_size(18)
    g.cax.tick_params(labelsize=14)

    # Optional: save and show
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_x(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )


    # Normalize again just before plotting (optional)
    sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min() + 1e-6)

    # Create DataFrame for better label control
    import pandas as pd
    df_scores = pd.DataFrame(sorted_scores, index=[f"Gene {i}" for i in ordered_row_indices], columns=feature_names)

    # Create column colors based on omics types
    col_colors = pd.Series(feature_colors, index=df_scores.columns)

    # Generate clustermap
    g = sns.clustermap(
        df_scores,
        row_cluster=False,
        col_cluster=False,
        col_colors=col_colors,
        cmap=bluish_gray_gradient,
        xticklabels=True,
        yticklabels=False,
        figsize=(18, 14),
        vmin=vmin,
        vmax=vmax,
        cbar_kws={"label": "Relevance Score"},
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_predicted_genes_distribution(pred_counts, output_path):
    """
    Visualize the number of predicted cancer genes in each cluster.

    Args:
        pred_counts (dict): Dictionary mapping cluster_id -> number of predicted genes
        output_path (str): Path to save the plot
    """
    clusters = list(pred_counts.keys())
    counts = [pred_counts[c] for c in clusters]
    colors = [CLUSTER_COLORS.get(c, "#808080") for c in clusters]

    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(clusters, counts, color=colors, edgecolor='black')

    ax.set_title("Predicted Cancer Genes per Cluster", fontsize=16)
    ax.set_xlabel("Cluster ID", fontsize=14)
    ax.set_ylabel("Number of Predicted Genes", fontsize=14)
    ax.set_xticks(clusters)
    ax.set_xticklabels(clusters, rotation=0, fontsize=12)
    ax.tick_params(axis='y', labelsize=12)

    # Add value labels on top
    for bar in bars:
        height = bar.get_height()
        if height > 0:
            ax.annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=10)

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_clustermap_pass(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    row_labels=None,
    col_labels=None,
):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    import pandas as pd

    # Define cluster colors
    CLUSTER_COLORS = {
        0: '#0077B6', 1: '#0000FF', 2: '#00B4D8', 3: '#48EAC4', 4: '#F1C0E8',
        5: '#B9FBC0', 6: '#32CD32', 7: '#bee1e6', 8: '#8A2BE2', 9: '#E377C2',
        10: '#8EECF5', 11: '#A3C4F3', 12: '#FFB347', 13: '#FFD700', 14: '#FF69B4',
        15: '#CD5C5C', 16: '#7FFFD4', 17: '#FF7F50', 18: '#C71585', 19: '#20B2AA'
    }

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB', 'ge': '#228B22', 'meth': '#00008B', 'mf': '#b22222',
        }

    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min() + 1e-8) * 20

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]
    original_col_indices = list(range(relevance_scores.shape[1]))
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # Reorder based on provided labels
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
        row_labels = np.array(row_labels)[row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        feature_colors = [feature_colors[i] for i in col_order]
        col_labels = np.array(col_labels)[col_order]

    # Cluster colors for rows
    if row_labels is not None:
        row_color_labels = [CLUSTER_COLORS[int(lbl)] for lbl in row_labels]
    else:
        row_color_labels = None

    # Column color bar
    if col_labels is not None:
        unique_clusters = np.unique(col_labels)
        col_cluster_color_map = {cl: CLUSTER_COLORS[cl % len(CLUSTER_COLORS)] for cl in unique_clusters}
        col_color_labels = [col_cluster_color_map[cl] for cl in col_labels]
    else:
        col_color_labels = feature_colors

    # Create dataframe for clustermap
    df = pd.DataFrame(relevance_scores, columns=feature_names)

    # Plot clustermap without clustering
    g = sns.clustermap(
        df,
        row_cluster=False,
        col_cluster=False,
        row_colors=row_color_labels,
        col_colors=col_color_labels,
        cmap="Blues",
        xticklabels=False,
        yticklabels=False,
        figsize=(18, 17),
        cbar_kws={"label": "Relevance Score"},
    )

    # Color bar ticks
    g.cax.yaxis.label.set_size(18)
    g.cax.tick_params(labelsize=14)

    # Optional: save and show
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    # Construct DataFrame with optional gene and feature labels
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]
    col_feature_names = feature_names

    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=col_feature_names)

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix saved to {csv_output_path}")

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def apply_full_spectral_biclustering_bio_(
    graph, summary_bio_features, node_names, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    import numpy as np
    import torch
    from sklearn.metrics import mean_squared_error
    from sklearn.cluster import SpectralBiclustering

    print("🧪 Running Spectral Biclustering with fixed (16, 10) clusters...")

    assert summary_bio_features.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features.shape[1]}"
    assert topk_node_indices is not None, "`topk_node_indices` must be provided"

    n_clusters_row, n_clusters_col = 10, 6
    best_model, best_score = None, np.inf

    print("🔁 Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col),
                                     method='bistochastic',
                                     svd_method='randomized',
                                     random_state=i)
        model.fit(summary_bio_features)

        reconstructed = summary_bio_features[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    assert len(topk_node_indices) == len(row_labels), "Mismatch between top-k indices and row_labels"

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering complete.")

    # Save outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)

    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    # Generate visualizations with distinct paths
    plot_bio_biclustering_heatmap(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap.replace(".png", "_heatmap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap.replace(".png", "_unsorted.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features,
        omics_splits=omics_splits,
        output_path=output_path_heatmap.replace(".png", "_clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names_topk, omics_splits,
    predicted_cancer_genes, n_clusters_row, n_clusters_col,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    # import torch
    # import numpy as np
    # from sklearn.metrics import mean_squared_error
    # from sklearn.cluster import SpectralBiclustering
    # from utils import (  # Replace with actual locations if needed
    #     save_graph_with_clusters, save_row_labels, compute_total_genes_per_cluster,
    #     save_total_genes_per_cluster, count_predicted_genes_per_cluster, save_predicted_counts
    # )
    # from plotting import (
    #     plot_bio_biclustering_heatmap_unsort,
    #     plot_bio_biclustering_clustermap,
    #     plot_predicted_genes_distribution
    # )
    # import os

    print("🧪 Running Spectral Biclustering with fixed (16, 10) clusters...")

    # === ✅ Step 1: Filter top-k nodes
    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")

    ##summary_bio_features_topk = summary_bio_features[topk_node_indices]
    summary_bio_features_topk = summary_bio_features  # Already top-k
    #node_names_topk = node_names
    #node_names_topk = [node_names[i] for i in topk_node_indices]

    assert summary_bio_features_topk.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_topk.shape[1]}"

    # === ✅ Step 2: Run Biclustering
    n_clusters_row = n_clusters_row,
    n_clusters_col = n_clusters_col,

    best_model = None
    best_score = np.inf

    print("🔁 Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='bistochastic',
                                     svd_method='randomized', random_state=i)
        
        model.fit(summary_bio_features_topk)

        reconstructed = summary_bio_features_topk[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features_topk, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === ✅ Step 3: Assign row cluster labels back to graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering complete.")

    # === ✅ Step 4: Save clustering outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names_topk, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === ✅ Step 5: Plot heatmaps and distributions
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "heatmap_unsort.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_topo_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    

    """
    Plots a spectral biclustering heatmap for topological embeddings (1024–2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # 🔹 Extract 64D summary of topological features
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # 🔹 Create topo feature names (01–64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # 🔹 Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # 🔹 Sort by cluster → then by gene-wise relevance within cluster
    ordered_row_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        ordered_row_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in ordered_row_indices]

    # 🔹 Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # 🔹 Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    
    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")


    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # 🔹 Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # 🔹 Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # 🔹 Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # 🔹 Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # 🔹 Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # 🔹 Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved spectral clustering heatmap to {output_path}")

    # 🔹 Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_bio_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)



    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # 🔹 Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def plot_novel_predicted_cancer_genes(
    clusters,
    novel_predicted_cancer_genes,
    total_genes_per_cluster,
    node_names,
    row_labels,
    output_path):
    """
    Plots the percentage of novel predicted cancer genes per cluster.
    """

    # Convert to array and sort clusters
    clusters = np.array(sorted(total_genes_per_cluster.keys()))
    total_genes_array = np.array([total_genes_per_cluster[c] for c in clusters])

    # Count novel predicted genes per cluster
    cluster_to_novel_count = {c: 0 for c in clusters}
    for i, name in enumerate(node_names):
        if name in novel_predicted_cancer_genes:
            cluster = row_labels[i]
            cluster_to_novel_count[cluster] += 1

    # Get counts in cluster order
    predicted_counts = np.array([cluster_to_novel_count.get(c, 0) for c in clusters])
    percent_predicted = np.divide(predicted_counts, total_genes_array, where=total_genes_array > 0)

    # Prepare bar colors
    colors = [CLUSTER_COLORS.get(c, '#333333') for c in clusters]

    # Plot
    fig, ax = plt.subplots(figsize=(8, 5))
    bars = ax.bar(clusters, percent_predicted, color=colors, edgecolor='black')

    # Annotate bars
    for bar, cluster_id in zip(bars, clusters):
        height = bar.get_height()
        count = cluster_to_novel_count.get(cluster_id, 0)
        ax.text(bar.get_x() + bar.get_width() / 2, height, str(count),
                ha='center', va='bottom', fontsize=16, fontweight='bold')

    ax.set_xlim(-0.55, len(clusters) - 0.65)
    ax.set_ylabel("Percent of NPCGs", fontsize=20)
    plt.xticks(clusters, fontsize=16)
    plt.yticks(fontsize=16)
    ax.set_ylim(0, max(percent_predicted) + 0.1)

    sns.despine()
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"✅ Novel PCG plot saved to {output_path}")

def process_cluster_results_(
    tag,
    graph,
    row_labels,
    top_gene_indices,
    predicted_counts,
    total_genes_per_cluster,
    relevance_scores_subset,
    node_names,
    node_names_topk,
    ground_truth_cancer_genes,
    name_to_index,
    output_dir,
    args
):
    # Assign cluster labels to graph
    graph.ndata[f'cluster_{tag}'] = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    graph.ndata[f'cluster_{tag}'][top_gene_indices] = torch.tensor(row_labels, dtype=torch.long)

    # Check cluster size condition
    cluster_to_genes = {}
    for idx, label in zip(top_gene_indices, row_labels):
        cluster_to_genes.setdefault(label, []).append(idx)

    if not cluster_to_genes or not all(len(indices) >= 10 for indices in cluster_to_genes.values()):
        print(f"⚠️ Skipping cluster analysis for tag='{tag}': some clusters have <10 genes.")
        return

    # Visualize UMAP/t-SNE embeddings
    G_tmp = graph.to_networkx(node_attrs=[f'cluster_{tag}'])
    plot_relevance_tsne_umap(
        relevance_scores_subset,
        G_tmp,
        cluster_key=f'cluster_{tag}',
        method='umap',
        title_suffix=f"({tag.capitalize()})"
    )

    # Plot predicted PCG % bar chart
    plot_pcg_cancer_genes(
        clusters=range(len(predicted_counts)),
        predicted_cancer_genes_count=predicted_counts,
        total_genes_per_cluster=total_genes_per_cluster,
        node_names=node_names_topk,
        row_labels=row_labels,
        output_path=os.path.join(
            output_dir,
            f'{args.model_type}_{args.net_type}_pcg_percent_{tag}_epo{args.num_epochs}.png'
        )
    )

    # Prepare data for interactions
    row_labels_np = np.array(row_labels)
    degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()

    gt_indices = set(name_to_index[name] for name in ground_truth_cancer_genes if name in name_to_index)
    kcg_nodes = [i for i, name in enumerate(node_names_topk) if name in ground_truth_cancer_genes]

    kcg_data = pd.DataFrame({
        "Cluster": row_labels_np[kcg_nodes],
        "Interactions": degrees_np[top_gene_indices][kcg_nodes]
    })

    pcg_data = pd.DataFrame({
        "Cluster": row_labels_np,
        "Interactions": degrees_np[top_gene_indices]
    })

    plot_interactions_with_kcgs(
        kcg_data,
        os.path.join(output_dir, f"{args.model_type}_{args.net_type}_kcgs_interaction_{tag}_epo{args.num_epochs}.png")
    )

    plot_interactions_with_pcgs(
        pcg_data,
        os.path.join(output_dir, f"{args.model_type}_{args.net_type}_pcgs_interaction_{tag}_epo{args.num_epochs}.png")
    )

    # Count KCGs per cluster
    kcg_counts = {
        i: sum((row_labels_np == i) & np.isin(range(len(row_labels)), list(gt_indices)))
        for i in range(len(predicted_counts))
    }

    # Plot % KCGs per cluster
    plot_kcg_cancer_genes(
        clusters=range(len(predicted_counts)),
        kcg_count=kcg_counts,
        total_genes_per_cluster=total_genes_per_cluster,
        node_names=node_names_topk,
        row_labels=row_labels,
        output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcg_percent_{tag}_epo{args.num_epochs}.png')
    )

    # Optionally, add tag-specific extra logic
    if tag == 'bio':
        print("✅ Finished BIO-specific cluster visualizations.")
        # Add any extra bio-specific logic here if needed
    elif tag == 'topo':
        print("✅ Finished TOPO-specific cluster visualizations.")
        # Add any extra topo-specific logic here if needed

def plot_bio_biclustering_heatmap_clusters_unsort(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    row_labels=None,
    col_labels=None,
):
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)

    # Normalize to [0, 20]
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',  # purple
            'ge': '#228B22',   # green
            'meth': '#00008B', # blue
            'mf': '#b22222',   # red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Sort rows by cluster labels only
    ##cluster_order = np.argsort(row_labels)
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    # Colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", 
        ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)

    # Grid layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])       # top bar
    ax        = fig.add_subplot(gs[1:13, 2:45])     # main heatmap
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)  # saliency curve
    ax_cbar   = fig.add_subplot(gs[5:9, 49])       # colorbar

    # Top feature bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)
    
    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels with omics colors
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)


    # Saliency curve 
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)
    
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1, 
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars (right below feature bar, above heatmap)
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort_(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    row_labels=None,
    col_labels=None,
):  
    
    # 🔹 Extract and normalize relevance scores
    #relevance_scores = extract_summary_features_np_bio(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If col_labels provided, reorder columns accordingly
    # if col_labels is not None:
    #     sorted_order = np.argsort(col_labels)
    #     relevance_scores = relevance_scores[:, sorted_order]
    #     feature_names = [feature_names[i] for i in sorted_order]

    # 🔹 Feature color mapping
    '''feature_colors = []
    for i in range(len(feature_names)):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")  # fallback color'''

    # Sort rows by cluster labels only
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    original_col_indices = list(range(relevance_scores.shape[1]))
    sorted_scores = sorted_scores[:, original_col_indices]
    feature_names = [feature_names[i] for i in original_col_indices]

    feature_colors = []
    for omics in omics_order:
        start, end = omics_splits[omics]
        feature_colors.extend([omics_colors[omics]] * (end - start + 1))

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    # Normalize per-feature means
    #feature_means = relevance_scores.mean(axis=0)
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(sorted_scores, 99)

    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Sort by cluster only (no intra-cluster sorting)
    '''cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]'''

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1, 
            linewidth=0, 
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), 
            clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # 🔹 Add cluster size labels
    # for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
    #     ax.text(
    #         -2.0, center_y, f"{count}",
    #         va='center', ha='right', fontsize=18, fontweight='bold'
    #     )
        
    # Add xtick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # Saliency curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = relevance_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    '''mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)

    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(relevance_scores) + 1.5,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]
        )'''

    for omics in omics_order:
        start, end = omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )
        
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, 
        transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_unsort(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):  
    
    # 🔹 Extract and normalize relevance scores
    # relevance_scores = extract_summary_features_np_bio(relevance_scores)
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 20

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',    # purple
            'ge': '#228B22',     # dark green
            'meth': '#00008B',   # dark blue
            'mf': '#b22222',     # dark red
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # If col_labels provided, reorder columns accordingly
    # if col_labels is not None:
    #     sorted_order = np.argsort(col_labels)
    #     relevance_scores = relevance_scores[:, sorted_order]
    #     feature_names = [feature_names[i] for i in sorted_order]

    # Build feature color bar
    feature_colors = []
    for i in range(len(feature_names)):
        for omics, (start, end) in omics_splits.items():
            if start <= i <= end:
                feature_colors.append(omics_colors[omics])
                break
        else:
            feature_colors.append("#AAAAAA")  # fallback color

    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)

    ax_bar    = fig.add_subplot(gs[0, 2:45])      
    ax        = fig.add_subplot(gs[1:13, 2:45])    
    ax_curve  = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar   = fig.add_subplot(gs[5:9, 49])

    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    # Normalize per-feature means
    feature_means = relevance_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    for i, (mean_val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5,
            height=mean_val,
            width=1.0,
            bottom=0,
            color=color,
            edgecolor='black',
            linewidth=0.5,
            alpha=0.3 + 0.7 * mean_val
        )

    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient",
        ["#F0F3F4", "#85929e"]
    )

    vmin = 0
    vmax = np.percentile(relevance_scores, 99)

    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Sort by cluster only (no intra-cluster sorting)
    ##cluster_order = np.argsort(row_labels)
    cluster_order = np.argsort(row_labels)
    sorted_scores = relevance_scores[cluster_order]
    sorted_clusters = row_labels[cluster_order]

    # Cluster stripe
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size text
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # Add xtick labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([c.split(": ")[1] for c in feature_names], rotation=90, fontsize=14)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # LRP curve
    saliency_sums = relevance_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))
    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    # Omics bar below
    omics_means = {}
    for omics, (start, end) in omics_splits.items():
        group_scores = relevance_scores[:, start:end+1]
        omics_means[omics] = group_scores.mean()

    group_centers = {
        omics: (omics_splits[omics][0] + omics_splits[omics][1]) / 2 + 0.5
        for omics in omics_order
    }

    mean_vals = np.array([omics_means[om] for om in omics_order])
    min_mean, max_mean = mean_vals.min(), mean_vals.max()
    normalized_means = (mean_vals - min_mean) / (max_mean - min_mean + 1e-6)

    for i, omics in enumerate(omics_order):
        ax.bar(
            x=group_centers[omics],
            height=0.15,
            width=(omics_splits[omics][1] - omics_splits[omics][0] + 1),
            bottom=len(relevance_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=0.3 + 0.7 * normalized_means[i]
        )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.01, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.set_yticks([])
    ax_curve.set_ylabel("")

    plt.tight_layout()
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()

def plot_bio_biclustering_heatmap_kcg(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None,
    kcg_list=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)



    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # ➤ Filter only known cancer genes
    if kcg_list is not None:
        kcg_mask = [name in kcg_list for name in row_gene_names]
        sorted_scores = sorted_scores[kcg_mask]
        sorted_clusters = sorted_clusters[kcg_mask]
        row_gene_names = [name for name, keep in zip(row_gene_names, kcg_mask) if keep]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # 🔹 Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def plot_bio_biclustering_heatmap_npcg(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None,
    kcg_list=None,                  # List of known cancer gene symbols
    plot_only_novel=True 
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)


    # Construct row labels: use gene names if available, else fallback
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # ➤ Filter to plot only novel predicted cancer genes
    if kcg_list is not None and plot_only_novel:
        novel_mask = [name not in kcg_list for name in row_gene_names]
        sorted_scores = sorted_scores[novel_mask]
        sorted_clusters = sorted_clusters[novel_mask]
        row_gene_names = [name for name, keep in zip(row_gene_names, novel_mask) if keep]


    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # 🔹 Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def plot_topo_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    

    """
    Plots a spectral biclustering heatmap for topological embeddings (1024–2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # 🔹 Extract 64D summary of topological features
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # 🔹 Create topo feature names (01–64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # 🔹 Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # 🔹 Sort by cluster → then by gene-wise relevance within cluster
    ordered_row_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        ordered_row_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in ordered_row_indices]

    # 🔹 Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # 🔹 Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    
    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")


    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # 🔹 Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # 🔹 Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # 🔹 Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # 🔹 Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # 🔹 Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # 🔹 Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved spectral clustering heatmap to {output_path}")

    # 🔹 Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names_topk, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):
    # import torch
    # import numpy as np
    # from sklearn.metrics import mean_squared_error
    # from sklearn.cluster import SpectralBiclustering
    # from utils import (  # Replace with actual locations if needed
    #     save_graph_with_clusters, save_row_labels, compute_total_genes_per_cluster,
    #     save_total_genes_per_cluster, count_predicted_genes_per_cluster, save_predicted_counts
    # )
    # from plotting import (
    #     plot_bio_biclustering_heatmap_unsort,
    #     plot_bio_biclustering_clustermap,
    #     plot_predicted_genes_distribution
    # )
    # import os

    print("🧪 Running Spectral Biclustering with fixed (16, 10) clusters...")

    # === ✅ Step 1: Filter top-k nodes
    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")

    ##summary_bio_features_topk = summary_bio_features[topk_node_indices]
    summary_bio_features_topk = summary_bio_features  # Already top-k
    #node_names_topk = node_names
    #node_names_topk = [node_names[i] for i in topk_node_indices]

    assert summary_bio_features_topk.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_topk.shape[1]}"

    # === ✅ Step 2: Run Biclustering
    n_clusters_row = 10
    n_clusters_col = 5

    best_model = None
    best_score = np.inf

    print("🔁 Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='bistochastic',
                                     svd_method='randomized', random_state=i)
        
        model.fit(summary_bio_features_topk)

        reconstructed = summary_bio_features_topk[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features_topk, reconstructed)

        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === ✅ Step 3: Assign row cluster labels back to graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering complete.")

    # === ✅ Step 4: Save clustering outputs
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names_topk, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === ✅ Step 5: Plot heatmaps and distributions
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "heatmap_unsort.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_bio_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)



    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.1)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # 🔹 Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def save_and_plot_enriched_pathways(enrichment_results, args, output_dir):
    # === Prepare Data ===
    heatmap_data = pd.DataFrame()

    for cluster_type in ['bio', 'topo']:
        for cid, df in enrichment_results[cluster_type].items():
            colname = f"{cluster_type.capitalize()}_{cid}"
            vals = {}
            for _, row in df.iterrows():
                p = row['p_value']
                name = row['name']
                if p < 0.05 and len(name) <= 60:
                    term = f"{name} ({row['source']})"
                    vals[term] = -np.log10(p)
            heatmap_data[colname] = pd.Series(vals)

    # Clean and filter
    heatmap_data = heatmap_data.fillna(0)
    heatmap_data = heatmap_data[heatmap_data.max(axis=1) > 1]

    # Save full enrichment data to CSV
    enrichment_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_enrichment_matrix_epo{args.num_epochs}.csv"
    )
    heatmap_data.to_csv(enrichment_csv_path, index_label='Enriched Pathway')

    # === Save Topo Cluster → Top Enriched Terms to CSV ===
    topo_terms = []
    for cid, df in enrichment_results['topo'].items():
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= 60:
                topo_terms.append({
                    "Cluster": f"Topo_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })
    topo_terms_df = pd.DataFrame(topo_terms)
    topo_terms_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_cluster_top_terms_epo{args.num_epochs}.csv"
    )
    topo_terms_df.to_csv(topo_terms_path, index=False)

    # === Save Bio Cluster → Top Enriched Terms to CSV ===
    bio_terms = []
    for cid, df in enrichment_results['bio'].items():
        for _, row in df.iterrows():
            if row['p_value'] < 0.05 and len(row['name']) <= 60:
                bio_terms.append({
                    "Cluster": f"Bio_{cid}",
                    "Term": row['name'],
                    "Source": row['source'],
                    "p_value": row['p_value'],
                    "-log10(p)": -np.log10(row['p_value']),
                })
    bio_terms_df = pd.DataFrame(bio_terms)
    bio_terms_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_cluster_top_terms_epo{args.num_epochs}.csv"
    )
    bio_terms_df.to_csv(bio_terms_path, index=False)

    # === Select 50 evenly spaced rows for plotting ===
    if heatmap_data.shape[0] > 50:
        step = max(1, heatmap_data.shape[0] // 50)
        selected_indices = heatmap_data.index[::step][:50]
        heatmap_data = heatmap_data.loc[selected_indices]

    # === Normalize for color contrast ===
    norm_data = heatmap_data.copy()
    norm_data = norm_data / norm_data.max().replace(0, 1)

    # === Apply group-wise colormaps ===
    colormaps = {
        'bio': get_cmap('Blues'),
        'topo': get_cmap('YlOrRd'),
    }

    colors = np.zeros((heatmap_data.shape[0], heatmap_data.shape[1], 4))  # RGBA
    col_types = []

    for i, col in enumerate(norm_data.columns):
        group = 'bio' if col.lower().startswith("bio") else 'topo'
        col_types.append(group)
        cmap = colormaps[group]
        colors[:, i, :] = cmap(norm_data[col].values)

    # === Plot ===
    fig, ax = plt.subplots(figsize=(0.5 * len(norm_data.columns), 0.2 * len(norm_data)))

    ax.imshow(colors, aspect='auto')
    ax.set_xticks(np.arange(len(norm_data.columns)))
    ax.set_xticklabels(norm_data.columns, rotation=90, fontsize=13)
    ax.set_yticks(np.arange(len(norm_data.index)))
    ax.set_yticklabels(norm_data.index, fontsize=14)
    ax.set_ylabel("Enriched Pathway", fontsize=18, labelpad=20)

    # Color x-axis labels
    for xtick, col in zip(ax.get_xticklabels(), col_types):
        xtick.set_color('darkblue' if col == 'bio' else 'darkred')

    ax.set_title("Top Enriched Pathways per Cluster (p < 0.05)", fontsize=14, pad=16)
    ax.set_xlabel("Cluster", fontsize=14)

    legend_patches = [
        Patch(color='cornflowerblue', label='Bio'),
        Patch(color='salmon', label='Topo')
    ]
    fig.legend(handles=legend_patches, loc='lower center', ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.08))

    sns.despine(ax=ax, trim=True)
    ax.tick_params(axis='both', which='both', length=0)
    plt.tight_layout(rect=[0, 0, 0.95, 0.93])

    # Save plot
    enriched_terms_heatmap_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_enriched_terms_heatmap_epo{args.num_epochs}.png"
    )
    plt.savefig(enriched_terms_heatmap_path, dpi=300)
    plt.close()

    # === Return DataFrames for downstream analysis ===
    return heatmap_data, topo_terms_df, bio_terms_df

def plot_gene_feature_contributions_topo_(
    gene_name,
    relevance_vector,
    feature_names,
    score,
    cluster_id,
    base_output_dir
):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    cluster_dir = os.path.join(base_output_dir, f"cluster_{cluster_id}")
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(cluster_dir, f"{gene_name}.png")
    barplot_path = output_path.replace(".png", "_omics_barplot.png")

    # Barplot of all 64 topo features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    plot_omics_barplot_topo(df, str(barplot_path))

    # Prepare for heatmap
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()


    heatmap_data = df.pivot(index='Omics', columns='Cancer', values='Relevance')
    omics_order = ['cna', 'ge', 'meth', 'mf']
    heatmap_data = heatmap_data.reindex(omics_order)

    plt.figure(figsize=(8, 2.8))
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')
    ##plt.title(f"{gene_name} ({score:.3f})", fontsize=14)
    ##plt.title(f"{gene_name} ({float(score):.3f})", fontsize=14)
    if isinstance(score, np.ndarray):
        score = score.item()

    ##plt.title(f"{gene_name} ({score.item():.3f})", fontsize=14)
    plt.title(f"{gene_name}", fontsize=14)


    plt.yticks(rotation=0)
    plt.xticks(rotation=90, ha='right')
    plt.xlabel('')
    plt.ylabel('')
    
    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_bio_(
    gene_name,
    relevance_vector,
    feature_names,
    score,
    cluster_id,
    base_output_dir
):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    cluster_dir = os.path.join(base_output_dir, f"cluster_{cluster_id}")
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(cluster_dir, f"{gene_name}.png")
    barplot_path = output_path.replace(".png", "_omics_barplot.png")


    # Barplot of all 64 features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    plot_omics_barplot_bio(df, str(barplot_path))

    # Prepare for heatmap
    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Omics', columns='Cancer', values='Relevance')
    omics_order = ['cna', 'ge', 'meth', 'mf']
    heatmap_data = heatmap_data.reindex(omics_order)

    plt.figure(figsize=(8, 2.8))
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')
    ##plt.title(f"{gene_name} ({score:.3f})", fontsize=14)
    ##plt.title(f"{gene_name} ({float(score):.3f})", fontsize=14)
    if isinstance(score, np.ndarray):
        score = score.item()

    ##plt.title(f"{gene_name} ({score.item():.3f})", fontsize=14)
    plt.title(f"{gene_name}", fontsize=14)


    plt.yticks(rotation=0)
    plt.xticks(rotation=90, ha='right')
    plt.xlabel('')
    plt.ylabel('')
    
    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.close()

def plot_gene_feature_contributions_bio(
    gene_name,
    relevance_vector,
    feature_names,
    score,
    cluster_id,
    base_output_dir
):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    cluster_dir = os.path.join(base_output_dir, f"cluster_{cluster_id}")
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(cluster_dir, f"{gene_name}.png")
    barplot_path = output_path.replace(".png", "_omics_barplot.png")


    # Barplot of all 64 features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    plot_omics_barplot_bio(df, str(barplot_path))
    
    # Barplot of all 64 features
    # df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    # barplot_path = output_path.replace(".png", "_omics_barplot.png") if output_path else None
    # plot_omics_barplot_bio(df, barplot_path)

    # Prepare for heatmap
    df[['Omics', 'Cancer']] = df['Feature'].str.split(':', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    # Handle gene name and score
    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_gene_feature_contributions_topo(
    gene_name,
    relevance_vector,
    feature_names,
    score,
    cluster_id,
    base_output_dir
):
    assert len(relevance_vector) == 64, "Expected 64 feature contributions (4 omics × 16 cancers)."

    cluster_dir = os.path.join(base_output_dir, f"cluster_{cluster_id}")
    os.makedirs(cluster_dir, exist_ok=True)

    output_path = os.path.join(cluster_dir, f"{gene_name}.png")
    barplot_path = output_path.replace(".png", "_omics_barplot.png")


    # Barplot of all 64 features
    df = pd.DataFrame({'Feature': feature_names, 'Relevance': relevance_vector})
    plot_omics_barplot_topo(df, str(barplot_path))

    # Prepare for heatmap
    df[['Cancer', 'Omics']] = df['Feature'].str.split('_', expand=True)
    df['Omics'] = df['Omics'].str.lower()

    heatmap_data = df.pivot(index='Cancer', columns='Omics', values='Relevance')
    heatmap_data = heatmap_data[['cna', 'ge', 'meth', 'mf']]  # Ensure column order

    # Plot vertical heatmap (Cancers as rows)
    plt.figure(figsize=(2.0, 5.0))
    # Capitalize omics column labels
    heatmap_data.columns = [col.upper() for col in heatmap_data.columns]
    sns.heatmap(heatmap_data, cmap='RdBu_r', center=0, cbar=False, linewidths=0.3, linecolor='gray')

    # Capitalize the first letter of each y-tick (cancer name)
    plt.yticks(
        ticks=plt.yticks()[0], 
        labels=[label.get_text().capitalize() for label in plt.gca().get_yticklabels()],
        rotation=0, fontsize=10
    )

    if isinstance(score, np.ndarray):
        score = score.item()
    plt.title(f"{gene_name}", fontsize=12)

    plt.yticks(rotation=0, fontsize=10)
    plt.xticks(rotation=90, ha='center', fontsize=10)
    plt.xlabel('')
    plt.ylabel('')

    if output_path:
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def plot_collapsed_clusterfirst_multilevel_sankey_bio_(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    import os
    import plotly.graph_objects as go
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    from collections import defaultdict
    from utils.sankey_helpers import get_neighbors_gene_names, hex_to_rgba, analyze_sankey_structure  # adjust import

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        confirmed_genes,
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]

    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break
    confirmed_genes = combined_genes

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = defaultdict(list)
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            continue
        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]
        cluster_label = f"Confirmed Cluster {gene_cluster}"
        cluster_to_genes[cluster_label].append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}
        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score
        neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores

    source, target, value, link_colors = [], [], [], []

    for cluster_label, genes in cluster_to_genes.items():
        if not genes:
            continue
        cluster_id = int(cluster_label.split()[-1])
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                color = CLUSTER_COLORS.get(row_labels[rel_idx], "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)
                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]
            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(cluster_id, "#000000"), 0.4))

            for neighbor_idx, neighbor_score in gene_to_neighbors[gene].items():
                neighbor_name = node_id_to_name[neighbor_idx]
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)
                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)
                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Sankey saved to: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ PNG export failed: {e}")

    # === Sankey Structural Analysis
    sankey_stats = analyze_sankey_structure(
        source, target, value,
        label_to_idx, node_names,
        cluster_to_genes, gene_to_neighbors,
        row_labels, name_to_index, scores
    )

    # === Entropy Plot
    entropy_df = pd.DataFrame(list(sankey_stats["cluster_entropy"].items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === Centrality Plot
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_collapsed_clusterfirst_multilevel_sankey_bio_pa(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        confirmed_genes,
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BCL6", "CUL1", "CDK4", "E2F3", "FOXM1", "SKP2", "BRCA2", "HDAC2", "ETS1", "ATR"]

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break

    # Ensure genes have neighbors
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)
    combined_genes = [gene for gene in combined_genes if neighbors_dict.get(gene)]  # 🔥 filter out genes without neighbors
    confirmed_genes = combined_genes

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Confirmed Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)

                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            neighbors = gene_to_neighbors.get(gene, {})

            # ✅ Avoid self-loop if no neighbors
            if neighbors:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

                for neighbor_idx, neighbor_score in neighbors.items():
                    neighbor_name = node_id_to_name[neighbor_idx]
                    neighbor_cluster = row_labels[neighbor_idx]
                    neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                    if neighbor_name not in label_to_idx:
                        label_to_idx[neighbor_name] = len(all_labels)
                        all_labels.append(neighbor_name)

                        saliency = relevance_scores[neighbor_idx].sum().item()
                        color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                        all_colors.append(color)
                        font_sizes.append(16 if saliency > 0.5 else 10)

                        if saliency > 0.5:
                            highlight_node_indices.append(label_to_idx[neighbor_name])
                            highlight_node_saliency.append(saliency)

                    neighbor_node_idx = label_to_idx[neighbor_name]

                    if neighbor_cluster_label not in label_to_idx:
                        label_to_idx[neighbor_cluster_label] = len(all_labels)
                        all_labels.append(neighbor_cluster_label)
                        all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                        font_sizes.append(18)

                    neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")

                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    if highlight_node_indices:
        x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
        y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

        fig.add_trace(go.Scatter(
            x=x_positions,
            y=y_positions,
            mode='none',
            marker=dict(
                size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
                color="rgba(255,0,0,0.3)",
                line=dict(width=2, color="rgba(255,0,0,0.7)"),
                sizemode='diameter'
            ),
            hoverinfo='skip',
            showlegend=False
        ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        scores
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_gene_neighbor_cluster_sankey_only(
    args,
    graph,
    node_names,
    name_to_index,
    confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    import os
    import plotly.graph_objects as go
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    from utils import hex_to_rgba, get_neighbors_gene_names, analyze_sankey_structure

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 12 by model score
    top_scored_genes = sorted(
        confirmed_genes,
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    combined_genes = []
    seen = set()
    for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 12:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)
    combined_genes = [gene for gene in combined_genes if neighbors_dict.get(gene)]
    confirmed_genes = combined_genes

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    source = []
    target = []
    value = []
    link_colors = []

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in confirmed_genes:
        if gene not in topk_name_to_index:
            continue
        gene_idx = topk_name_to_index[gene]
        gene_cluster = row_labels[gene_idx]

        if gene not in label_to_idx:
            label_to_idx[gene] = len(all_labels)
            all_labels.append(gene)
            saliency = relevance_scores[gene_idx].sum().item()
            color = CLUSTER_COLORS.get(gene_cluster, "#000000")
            all_colors.append(color)
            font_sizes.append(18 if saliency > 0.5 else 10)
            if saliency > 0.5:
                highlight_node_indices.append(label_to_idx[gene])
                highlight_node_saliency.append(saliency)

        gene_node_idx = label_to_idx[gene]

        for neighbor_name in neighbors_dict.get(gene, []):
            if neighbor_name not in topk_name_to_index:
                continue
            neighbor_idx = topk_name_to_index[neighbor_name]
            neighbor_cluster = row_labels[neighbor_idx]
            saliency = relevance_scores[neighbor_idx].sum().item()

            if neighbor_name not in label_to_idx:
                label_to_idx[neighbor_name] = len(all_labels)
                all_labels.append(neighbor_name)
                color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(16 if saliency > 0.5 else 10)
                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[neighbor_name])
                    highlight_node_saliency.append(saliency)

            neighbor_node_idx = label_to_idx[neighbor_name]

            cluster_label = f"Cluster {neighbor_cluster}"
            if cluster_label not in label_to_idx:
                label_to_idx[cluster_label] = len(all_labels)
                all_labels.append(cluster_label)
                all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                font_sizes.append(18)

            cluster_node_idx = label_to_idx[cluster_label]

            rel_score = relevance_scores[neighbor_idx].sum().item()

            # Gene → Neighbor
            source.append(gene_node_idx)
            target.append(neighbor_node_idx)
            value.append(rel_score)
            link_colors.append("rgba(160,160,160,0.5)")

            # Neighbor → Cluster
            source.append(neighbor_node_idx)
            target.append(cluster_node_idx)
            value.append(rel_score)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_gene_neighbor_cluster_sankey_only/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_gene_neighbor_cluster_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Gene-Neighbor-Cluster Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_path = save_path.replace(".html", ".png")
        fig.write_image(png_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

def plot_collapsed_clusterfirst_multilevel_sankey_bio_novel(
    args,
    graph,
    node_names,
    name_to_index,
    novel_predicted_genes, #confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        novel_predicted_genes,  # or confirmed_genes
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    # Exclude MED subunits
    top_scored_genes = [
        g for g in top_scored_genes
        if not g.startswith("MED")
    ]


    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = [
        "ACTB", "ATR", "BCL6", "BRCA2", "CDK4", "CUL1",
        "E2F3", "EGFR", "ETS1", "FOXM1", "HDAC2", "RBM39",
        "SKP2", "SRC"
    ]

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]
    
    selected_known_genes = ["BRCA2", "HDAC2"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    # for g in selected_novel_genes + selected_known_genes:
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 20:
            break


    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Confirmed Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)

                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
    
                # if neighbor_idx == node_idx:
                #     continue
                neighbor_name = node_id_to_name[neighbor_idx]
                
                if neighbor_name == gene:
                    continue
                
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)

                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)

                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    if highlight_node_indices:
        x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
        y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

        fig.add_trace(go.Scatter(
            x=x_positions,
            y=y_positions,
            mode='none',
            marker=dict(
                size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
                color="rgba(255,0,0,0.3)",
                line=dict(width=2, color="rgba(255,0,0,0.7)"),
                sizemode='diameter'
            ),
            hoverinfo='skip',
            showlegend=False
        ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        scores
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_collapsed_clusterfirst_multilevel_sankey_topo(
    args,
    graph,
    node_names,
    name_to_index,
    novel_predicted_genes, #confirmed_genes,
    scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Top 10 by model score
    top_scored_genes = sorted(
        novel_predicted_genes,  # or confirmed_genes
        key=lambda g: scores[name_to_index[g]],
        reverse=True
    )

    # Exclude MED subunits
    top_scored_genes = [
        g for g in top_scored_genes
        if not g.startswith("MED")
    ]


    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = [
        "ACTB", "ATR", "BCL6", "BRCA2", "CDK4", "CUL1",
        "E2F3", "EGFR", "ETS1", "FOXM1", "HDAC2", "RBM39",
        "SKP2", "SRC"
    ]

    selected_known_genes = ["EGFR", "SRC", "ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]
    
    selected_known_genes = ["BRCA2", "HDAC2"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    # for g in selected_novel_genes + selected_known_genes:
    for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 20:
            break

    #combined_genes = [g for g in combined_genes if g != "IRF2"]

    #confirmed_genes = combined_genes

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Confirmed Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: scores[name_to_index[g]], reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18 if saliency > 0.5 else 10)

                if saliency > 0.5:
                    highlight_node_indices.append(label_to_idx[gene])
                    highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
    
                # if neighbor_idx == node_idx:
                #     continue
                neighbor_name = node_id_to_name[neighbor_idx]
                
                if neighbor_name == gene:
                    continue
                
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)

                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)

                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    if highlight_node_indices:
        x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
        y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

        fig.add_trace(go.Scatter(
            x=x_positions,
            y=y_positions,
            mode='none',
            marker=dict(
                size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
                color="rgba(255,0,0,0.3)",
                line=dict(width=2, color="rgba(255,0,0,0.7)"),
                sizemode='diameter'
            ),
            hoverinfo='skip',
            showlegend=False
        ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/topo_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_topo_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_topo_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        scores
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_neighbor_relevance_itself_neighbor(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter top neighbors
    filtered = {k: v for k, v in neighbor_scores.items() if v > 0.0}
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            node_names_topkint(row_labels[int(nid)])
            cluster_ids.append(cid)
            colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
        else:
            colors.append("white")

    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        main_cluster_id = int(row_labels[int(real_neighbors[0])]) if real_neighbors else 0
        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)


    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved neighbor relevance plot to {output_path}")

def plot_neighbor_relevance_(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter top neighbors
    filtered = {k: v for k, v in neighbor_scores.items() if v > 0.0}
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    # neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    neighbor_names = []
    neighbor_name_ids = []
        
    for nid in neighbor_ids:
        neighbor_name_ids.append(nid)
        if row_labels is not None and not str(nid).startswith("dummy_"):
            node_names_topkint(row_labels[int(nid)])
            cluster_ids.append(cid)
            colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
            neighbor_name = node_id_to_name.get(nid, f"{nid}")
            neighbor_names.append(neighbor_name)
        else:
            colors.append("white")
            neighbor_names.append("")
            
    print('neighbor_names======================\n',neighbor_names)
    prinkt()
    
    # for nid in neighbor_ids:
    #     if row_labels is not None and isinstance(nid, int):
    #         node_names_topkint(row_labels[nid])
    #         cluster_ids.append(cid)
    #         colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
    #         neighbor_name = node_id_to_name.get(nid, f"{nid}")
    #         neighbor_names.append(neighbor_name)
    #         neighbor_name_ids.append(nid)
    #     else:
    #         colors.append("white")
    #         neighbor_names.append("")
    #         neighbor_name_ids.append(None)
            
                        
    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        main_cluster_id = int(row_labels[int(real_neighbors[0])]) if real_neighbors else 0
        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)


    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved neighbor relevance plot to {output_path}")

def plot_neighbor_relevance_x(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter top neighbors
    filtered = {k: v for k, v in neighbor_scores.items() if v > 0.0}
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    #neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    # colors = []
    # cluster_ids = []
    # for nid in neighbor_ids:
    #     if row_labels is not None and not str(nid).startswith("dummy_"):
    #         node_names_topkint(row_labels[int(nid)])
    #         cluster_ids.append(cid)
    #         colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
    #     else:
    #         colors.append("white")
    # Assign cluster colors
    colors = []
    cluster_ids = []
    neighbor_names = []
    neighbor_name_ids = []
        
    for nid in neighbor_ids:
        neighbor_name_ids.append(nid)
        if row_labels is not None and not str(nid).startswith("dummy_"):
            node_names_topkint(row_labels[int(nid)])
            cluster_ids.append(cid)
            colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
            neighbor_name = node_id_to_name.get(nid, f"{nid}")
            neighbor_names.append(neighbor_name)
        else:
            colors.append("white")
            neighbor_names.append("")
            
    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        main_cluster_id = int(row_labels[int(real_neighbors[0])]) if real_neighbors else 0
        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)


    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved neighbor relevance plot to {output_path}")

def save_and_plot_confirmed_genes_bio(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    confirmed_genes_save_path,
    row_labels_topk,
    tag="bio",
    confirmed_gene_path="../acgnn/data/ncg_8886.txt"):
    """
    Finds confirmed cancer genes and plots their biological feature contributions.
    """

    
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]

    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics}:{cancer}" for omics in omics_order for cancer in cancer_names]

    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    confirmed_genes = [g for g in node_names_topk if g in known_cancer_genes]

    with open(confirmed_genes_save_path, "w") as f:
        for gene in confirmed_genes:
            f.write(f"{gene}\n")

    plot_dir = os.path.join(output_dir, f"{tag}_confirmed_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    # for gene_name in confirmed_genes:
    #     idx = node_names_topk.index(gene_name)
    #     relevance_vector = summary_feature_relevance[idx]
    #     score = get_scalar_score(node_scores_topk[idx])
    #     cluster_id = row_labels_topk[idx].item()

    #     output_path = os.path.join(
    #         "results/gene_prediction/bio_confirmed_feature_contributions/",
    #         f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_feature_contributions_epo{args.num_epochs}.png"
    #     )

    #     plot_gene_feature_contributions_bio(
    #         gene_name=gene,
    #         relevance_vector=relevance_vector,
    #         feature_names=feature_names,
    #         score=score,
    #         cluster_id=cluster_id,
    #         output_path=output_path
    #     )
    for gene_name in confirmed_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        cluster_id = row_labels_topk[idx].item()

        plot_gene_feature_contributions_bio(
            gene_name=gene_name,
            relevance_vector=relevance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=cluster_id,
            base_output_dir=os.path.join(
                "results/gene_prediction/bio_confirmed_feature_contributions",
                f"{args.model_type}_{args.net_type}_epo{args.num_epochs}"
            )
        )

def save_and_plot_novel_genes_bio(
    args,
    node_names_topk,
    node_scores_topk,
    summary_feature_relevance,
    output_dir,
    novel_genes_save_path,
    row_labels_topk,
    tag="bio",
    confirmed_gene_path="../acgnn/data/ncg_8886.txt"
):
    """
    Finds novel predicted cancer genes (not in known list) and plots their biological feature contributions.
    """

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics}:{cancer}" for omics in omics_order for cancer in cancer_names]

    # Load confirmed genes
    with open(confirmed_gene_path) as f:
        known_cancer_genes = set(line.strip() for line in f if line.strip())

    # Filter for novel genes
    novel_predicted_genes = [g for g in node_names_topk if g not in known_cancer_genes]

    # Save novel genes
    with open(novel_genes_save_path, "w") as f:
        for gene in novel_predicted_genes:
            f.write(f"{gene}\n")

    # Plot directory for NPCGs
    plot_dir = os.path.join(output_dir, f"{tag}_novel_feature_contributions")
    os.makedirs(plot_dir, exist_ok=True)

    def get_scalar_score(score):
        if isinstance(score, np.ndarray):
            return score.item() if score.size == 1 else score[0]
        return float(score)

    for gene_name in novel_predicted_genes:
        idx = node_names_topk.index(gene_name)
        relevance_vector = summary_feature_relevance[idx]
        score = get_scalar_score(node_scores_topk[idx])
        cluster_id = row_labels_topk[idx].item()

        plot_gene_feature_contributions_bio(
            gene_name=gene_name,
            relevance_vector=relevance_vector,
            feature_names=feature_names,
            score=score,
            cluster_id=cluster_id,
            base_output_dir=os.path.join(
                "results/gene_prediction/bio_novel_feature_contributions",
                f"{args.model_type}_{args.net_type}_epo{args.num_epochs}"
            )
        )

def plot_collapsed_clusterfirst_multilevel_sankey_bio_linked_loop(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk, #confirmed_genes,
    ## scores,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 

    # Top 10 by model score
    # top_scored_genes = sorted(
    #     node_names_topk,  # or confirmed_genes
    #     key=lambda g: scores[name_to_index[g]],
    #     reverse=True
    # )
    # Top 10 by mean saliency score
    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(), #.mean(),
        reverse=True
    )

    # Exclude MED subunits
    top_scored_genes = [
        g for g in top_scored_genes
        if not g.startswith("MED")
    ]


    # Manually selected important genes
    selected_genes = ["BRCA1", "TP53", "PIK3CA", "KRAS", "ALK"]
    # selected_known_genes = [
    #     "BRCA2",   # Breast, ovarian, pancreatic cancer
    #     "CDK4",    # Melanoma, sarcoma
    #     "BCL6",    # B-cell lymphomas
    #     "E2F3",    # Bladder, prostate cancer
    #     "CUL1",    # Cell cycle, implicated in various cancers
    #     "FOXM1",   # Proliferation driver, overexpressed in many tumors
    #     "ETS1",    # Leukemia, lymphomas
    #     "SKP2",    # Regulates cell cycle, implicated in prostate, breast, etc.
    #     "ATR",     # DNA repair gene, involved in many cancers
    #     "HDAC2"    # Epigenetic regulator, therapeutic target in hematologic cancers
    # ]
    selected_known_genes = [
        "ACTB", "ATR", "BCL6", "BRCA2", "CDK4", "CUL1",
        "E2F3", "EGFR", "ETS1", "FOXM1", "HDAC2", "RBM39",
        "SKP2", "SRC"
    ]

    selected_known_genes = ["ACTB", "RBM39", "BRCA2", "HDAC2", "ETS1", "ATR"]
    
    selected_known_genes = ["EGFR", "SRC", "BRCA2", "HDAC2"]
    selected_known_genes = ["BRCA2", "HDAC2"]

    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]


    # Combine and deduplicate while preserving order
    combined_genes = []
    seen = set()
    for g in selected_known_genes + top_scored_genes:
    # for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 10:
            break


    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}

    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]

        cluster_label = f"Confirmed Cluster {gene_cluster}"

        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])

        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        # genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]], reverse=True)[:10]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]


        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)

                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                # font_sizes.append(18 if saliency > 0.5 else 10)

                # if saliency > 0.5:
                #     highlight_node_indices.append(label_to_idx[gene])
                #     highlight_node_saliency.append(saliency)

            gene_idx = label_to_idx[gene]

            source.append(cluster_idx)
            target.append(gene_idx)
            value.append(1)
            link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
    
                # if neighbor_idx == node_idx:
                #     continue
                neighbor_name = node_id_to_name[neighbor_idx]
                
                if neighbor_name == gene:
                    continue
                
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)

                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)

                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                source.append(gene_idx)
                target.append(neighbor_node_idx)
                value.append(neighbor_score)
                link_colors.append("rgba(160,160,160,0.5)")

                source.append(neighbor_node_idx)
                target.append(neighbor_cluster_idx)
                value.append(neighbor_score)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    # if highlight_node_indices:
    #     x_positions = [0.1 + (idx % 6) * 0.15 for idx in highlight_node_indices]
    #     y_positions = [0.9 - (idx // 6) * 0.1 for idx in highlight_node_indices]

    #     fig.add_trace(go.Scatter(
    #         x=x_positions,
    #         y=y_positions,
    #         mode='none',
    #         marker=dict(
    #             size=[30 + 40 * (s-0.5) for s in highlight_node_saliency],
    #             color="rgba(255,0,0,0.3)",
    #             line=dict(width=2, color="rgba(255,0,0,0.7)"),
    #             sizemode='diameter'
    #         ),
    #         hoverinfo='skip',
    #         showlegend=False
    #     ))

    fig.update_layout(
        title=None,
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, showticklabels=False, zeroline=False)
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    # === Run Structure Analysis
    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    # === Summary Plot: Entropy per Cluster
    entropy = sankey_stats["cluster_entropy"]

    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"])
    entropy_df = entropy_df.sort_values("Entropy", ascending=False)

    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    # === Summary Plot: Jaccard Heatmap
    jaccard = sankey_stats["cluster_jaccard"]
    ##jaccard_df = pd.DataFrame(jaccard).fillna(0)
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])

    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)


    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    # === (Optional) Summary Plot: Centrality per Gene
    #centrality_df = pd.DataFrame(list(sankey_stats["centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])

    centrality_df = centrality_df.sort_values("Centrality", ascending=False)

    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_collapsed_clusterfirst_multilevel_sankey_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["BRCA2", "HDAC2"]
    selected_known_genes = ["EGFR", "SRC", "BRCA2", "HDAC2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]

    combined_genes = []
    seen = set()
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 10:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        gene_cluster = row_labels[node_idx]
        cluster_label = f"Confirmed Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []

    existing_edges = set()  # Track (source, target) to avoid A→B and B→A cycles

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)

            gene_idx = label_to_idx[gene]

            # Cluster → Gene
            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
                neighbor_name = node_id_to_name[neighbor_idx]

                if neighbor_name == gene:
                    continue

                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                # Gene → Neighbor (skip if reverse exists)
                if (neighbor_node_idx, gene_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                # Neighbor → Cluster (skip if reverse exists)
                if (neighbor_cluster_idx, neighbor_node_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_neighbor_relevance_filt_by_index(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Remove the gene itself from neighbors (by checking name match)
    gene_index = None
    for idx, name in node_id_to_name.items():
        if name == gene_name:
            gene_index = idx
            break

    # Filter out self and zero-relevance neighbors
    filtered = {
        k: v for k, v in neighbor_scores.items()
        if v > 0.0 and (str(k) != str(gene_index))
    }

    # Keep top 10
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            node_names_topkint(row_labels[int(nid)])
            cluster_ids.append(cid)
            colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
        else:
            colors.append("white")

    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        main_cluster_id = int(row_labels[int(real_neighbors[0])]) if real_neighbors else 0
        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved neighbor relevance plot to {output_path}")

def plot_neighbor_relevance_(
    neighbor_scores,
    # gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    Filters out the gene itself from being plotted as its own neighbor (by name).
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter out self-links by comparing names
    filtered = {
        k: v for k, v in neighbor_scores.items()
        if v > 0.0 and node_id_to_name.get(k, "") != gene_name
    }

    # Keep top 10
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            cid = int(row_labels[int(nid)])
            cluster_ids.append(cid)
            colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
        else:
            colors.append("white")

    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        main_cluster_id = int(row_labels[int(real_neighbors[0])]) if real_neighbors else 0
        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved neighbor relevance plot to {output_path}")

def plot_neighbor_relevance(
    neighbor_scores,
    gene_name,
    node_id_to_name,
    output_path,
    row_labels=None,
    total_clusters=10,
    add_legend=False
):
    """
    Plots the relevance scores of top neighbors and saves into a cluster-specific folder,
    inferred from row_labels.
    Always plots 10 bars, padding with zero-height bars if necessary.
    Filters out the gene itself from being plotted as its own neighbor (by name).
    """
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import matplotlib.patches as mpatches
    from pathlib import Path

    # Filter out self-links by comparing names
    filtered = {
        k: v for k, v in neighbor_scores.items()
        if v > 0.0 and node_id_to_name.get(k, "") != gene_name
    }

    # Keep top 10
    top_neighbors = dict(sorted(filtered.items(), key=lambda x: -x[1])[:10])

    # Pad with dummy neighbors if fewer than 10
    while len(top_neighbors) < 10:
        dummy_id = f"dummy_{len(top_neighbors)}"
        top_neighbors[dummy_id] = 0.0

    neighbor_ids = list(top_neighbors.keys())
    neighbor_names = [node_id_to_name.get(nid, f"{nid}") for nid in neighbor_ids]
    raw_scores = list(top_neighbors.values())

    # Normalize scores (skip if all are zero)
    if np.max(raw_scores) - np.min(raw_scores) > 1e-8:
        norm_scores = (np.array(raw_scores) - np.min(raw_scores)) / (np.max(raw_scores) - np.min(raw_scores) + 1e-8)
        norm_scores = norm_scores * 0.95 + 0.025
    else:
        norm_scores = np.zeros_like(raw_scores)

    # Assign cluster colors
    colors = []
    cluster_ids = []
    for nid in neighbor_ids:
        if row_labels is not None and not str(nid).startswith("dummy_"):
            try:
                nid_int = int(nid)
                if nid_int < len(row_labels):
                    cid = int(row_labels[nid_int])
                    cluster_ids.append(cid)
                    colors.append(CLUSTER_COLORS.get(cid % total_clusters, "gray"))
                else:
                    print(f"⚠️ nid {nid_int} is out of bounds (row_labels size: {len(row_labels)}).")
                    colors.append("gray")
            except Exception as e:
                print(f"⚠️ Skipping nid={nid} due to error: {e}")
                colors.append("gray")
        else:
            colors.append("white")

    # Inject cluster subfolder into output_path
    output_path = Path(output_path)
    if row_labels is not None:
        real_neighbors = [nid for nid in neighbor_ids if not str(nid).startswith("dummy_")]
        if real_neighbors:
            try:
                # Try extracting cluster from gene_name string, if it's in format: "GENE (Cluster X)"
                import re
                match = re.search(r'\(Cluster (\d+)\)', gene_name)
                if match:
                    main_cluster_id = int(match.group(1))
                else:
                    main_cluster_id = 0

            except Exception:
                main_cluster_id = 0
        else:
            main_cluster_id = 0

        cluster_subdir = output_path.parent / f"cluster_{main_cluster_id}"
        output_path = cluster_subdir / output_path.name
        cluster_subdir.mkdir(parents=True, exist_ok=True)
    else:
        output_path.parent.mkdir(parents=True, exist_ok=True)

    # Plot
    plt.figure(figsize=(2.2, 2.2))
    sns.set_style("white")
    ax = sns.barplot(x=neighbor_names, y=norm_scores, palette=colors)

    plt.title(f"{gene_name}", fontsize=12)
    plt.ylabel("Relevance score", fontsize=10)
    plt.xticks(rotation=90, fontsize=8)
    plt.yticks(fontsize=8)

    sns.despine(left=False, bottom=False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(0.8)
    ax.spines['bottom'].set_linewidth(0.8)
    ax.tick_params(axis='x', length=0)
    ax.tick_params(axis='y', length=0)

    if add_legend and row_labels is not None:
        unique_clusters = sorted(set(cluster_ids))
        legend_handles = [
            mpatches.Patch(color=CLUSTER_COLORS.get(cid % total_clusters, "gray"), label=f"Cluster {cid}")
            for cid in unique_clusters
        ]
        plt.legend(handles=legend_handles, title="Clusters", bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        plt.legend().remove()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved neighbor relevance plot to {output_path}")

def plot_confirmed_neighbors_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    # scores,
    row_labels,
    total_clusters,
    relevance_scores):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        # gene_score = relevance_scores[node_idx]
        gene_score = relevance_scores[node_idx].sum().item()  # OR .mean().item()

        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()
        # gene_cluster = int(row_labels[node_idx])

        print(f"{gene} → Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])

        # Filter only those neighbors present in top-k
        neighbor_scores_dict = {}
        for n in neighbors:
            if n == gene:
                continue  # ✅ Skip self
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score


        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/bio_neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

def plot_collapsed_clusterfirst_multilevel_sankey_bio_sankey_color_not_consistant(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):


    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "SRC", "BRCA2", "HDAC2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]

    combined_genes = []
    seen = set()
    # for g in top_scored_genes:
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 10:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        graph_node_idx = name_to_index[gene]
        gene_cluster = cluster_assignments[graph_node_idx]
        cluster_label = f"Confirmed Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores


    source = []
    target = []
    value = []
    link_colors = []

    existing_edges = set()  # Track (source, target) to avoid A→B and B→A cycles

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)

            gene_idx = label_to_idx[gene]

            # Cluster → Gene
            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
                neighbor_name = node_id_to_name[neighbor_idx]

                if neighbor_name == gene:
                    continue

                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                # Gene → Neighbor (skip if reverse exists)
                if (neighbor_node_idx, gene_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                # Neighbor → Cluster (skip if reverse exists)
                if (neighbor_cluster_idx, neighbor_node_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_feature_importance_bio(relevance_vector, feature_names, node_name=None, output_path="plots"):
    """
    Plot biological feature importance in OMICS:CANCER format using alphabetical order of cancer names.

    Parameters:
        relevance_vector (array-like): Relevance scores.
        feature_names (list of str): Feature names in the format Cancer_Omics.
        node_name (str, optional): Name of the node (used for title).
        output_path (str): Path to save the plot.
    """
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    import os

    # Define mapping from TCGA codes to cancer names
    cancer_map = {
        'BLCA': 'Bladder', 'BRCA': 'Breast', 'CESC': 'Cervix', 'COAD': 'Colon',
        'ESCA': 'Esophagus', 'HNSC': 'HeadNeck', 'KIRC': 'KidneyCC', 'KIRP': 'KidneyPC',
        'LIHC': 'Liver', 'LUAD': 'LungAD', 'LUSC': 'LungSC', 'PRAD': 'Prostate',
        'READ': 'Rectum', 'STAD': 'Stomach', 'THCA': 'Thyroid', 'UCEC': 'Uterus'
    }

    # Sort cancer codes by alphabetical order of their full names
    sorted_cancer_codes = sorted(cancer_map.keys(), key=lambda k: cancer_map[k])
    omics_order = ['cna', 'ge', 'meth', 'mf']
    column_labels = [f"{omics.upper()}:{cancer}" for omics in omics_order for cancer in sorted_cancer_codes]

    # Validate input
    if len(relevance_vector) != len(feature_names):
        raise ValueError(f"Mismatch: {len(relevance_vector)} values vs {len(feature_names)} names")

    # Build DataFrame
    df = pd.DataFrame({
        "feature": feature_names,
        "relevance": relevance_vector
    })

    df["omics_type"] = df["feature"].apply(lambda x: x.split("_")[1].lower())
    df["cancer_type"] = df["feature"].apply(lambda x: x.split("_")[0])
    df["formatted_label"] = df.apply(lambda row: f"{row['omics_type'].upper()}:{row['cancer_type']}", axis=1)

    # Reorder using column_labels
    df["order"] = df["formatted_label"].apply(lambda x: column_labels.index(x) if x in column_labels else -1)
    df = df[df["order"] != -1].sort_values("order")

    # Color mapping
    omics_color = {
        'cna': '#1F77B4',
        'ge': '#9467BD',
        'meth': '#2CA02C',
        'mf': '#D62728'
    }
    df["bar_color"] = df["omics_type"].map(omics_color)

    # Plotting
    plt.figure(figsize=(24, 5))
    ax = sns.barplot(x="formatted_label", y="relevance", data=df, palette=df["bar_color"].tolist())

    num_bars = len(df)
    margin = 0.75
    ax.set_xlim(-margin, num_bars - 1 + margin)

    ax.set_title(node_name if node_name else "", fontsize=16)
    ax.set_ylabel("Saliency score", fontsize=14)
    ax.set_xlabel("Feature (Omics: Cancer)", fontsize=14)

    # Color tick labels
    for tick, omics in zip(ax.get_xticklabels(), df["omics_type"]):
        tick.set_color(omics_color.get(omics, "black"))

    plt.xticks(rotation=90, fontsize=14)
    plt.yticks(fontsize=14)
    plt.tight_layout()

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"✅ Saved BIO plot to {output_path}")

def plot_collapsed_clusterfirst_multilevel_sankey_bio_(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):



    # from utils import hex_to_rgba, analyze_sankey_structure, get_neighbors_gene_names

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    combined_genes = []
    seen = set()
    for g in top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 10:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        node_idx = topk_name_to_index[gene]
        if node_idx >= len(row_labels):
            continue
        graph_node_idx = name_to_index[gene]
        gene_cluster = cluster_assignments[graph_node_idx]
        cluster_label = f"Confirmed Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []
    existing_edges = set()

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                saliency = relevance_scores[rel_idx].sum().item()
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)

            gene_idx = label_to_idx[gene]

            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
                neighbor_name = node_id_to_name[neighbor_idx]
                if neighbor_name == gene:
                    continue

                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                if (gene_idx, neighbor_node_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                if (neighbor_node_idx, neighbor_cluster_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats


def plot_collapsed_clusterfirst_multilevel_sankey_bio_(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,  # Spectral Biclustering assignments
    total_clusters,
    relevance_scores,
    CLUSTER_COLORS
):

    # from utils import get_neighbors_gene_names, hex_to_rgba, analyze_sankey_structure  # replace with your own utils

    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )
    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "SRC", "BRCA2", "HDAC2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]

    combined_genes = []
    seen = set()
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 10:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        rel_idx = topk_name_to_index[gene]
        if rel_idx >= len(row_labels):
            continue

        gene_cluster = row_labels[rel_idx]
        cluster_label = f"Confirmed Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n in topk_name_to_index:
                n_rel_idx = topk_name_to_index[n]
                if n_rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[n_rel_idx].sum().item()
                    neighbor_scores[n_rel_idx] = rel_score

        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:5])
        gene_to_neighbors[gene] = neighbor_scores

    source = []
    target = []
    value = []
    link_colors = []
    existing_edges = set()

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[topk_name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            gene_rel_idx = topk_name_to_index[gene]

            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                gene_cluster = row_labels[gene_rel_idx]
                all_colors.append(CLUSTER_COLORS.get(gene_cluster, "#000000"))
                font_sizes.append(16)

            gene_idx = label_to_idx[gene]

            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(gene_cluster, "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            for neighbor_idx, neighbor_score in neighbors.items():
                neighbor_name = node_id_to_name[neighbor_idx]

                if neighbor_name == gene:
                    continue

                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"Cluster {neighbor_cluster}"

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[neighbor_idx].sum().item()
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    if saliency > 0.5:
                        highlight_node_indices.append(label_to_idx[neighbor_name])
                        highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                if (gene_idx, neighbor_node_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                if (neighbor_node_idx, neighbor_cluster_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=1200, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_collapsed_clusterfirst_multilevel_sankey_bio(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    output_dir,
    CLUSTER_COLORS
):


    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)} 
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "BRCA2"]
    selected_novel_genes = ["EIF2B3", "TOM1L2", "SYNCRIP", "GEMIN5", "WDR24", "ZNF598", "TAF8", "SEC61A1", "FUBP1", "TRIP13"]

    combined_genes = []
    seen = set()
    # for g in top_scored_genes:
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 30:
            break

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}
    gene_to_neighbors = {}
    highlight_node_indices = []
    highlight_node_saliency = []

    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        # node_idx = topk_name_to_index[gene]
        # if node_idx >= len(row_labels):
        #     continue

        rel_idx = topk_name_to_index[gene]
        if rel_idx >= len(row_labels):
            continue
        
        # graph_node_idx = name_to_index[gene]
        # gene_cluster = cluster_assignments[graph_node_idx]
        gene_cluster = row_labels[rel_idx]
        
        cluster_label = f"Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n == gene:
                continue
            if n in combined_genes:
                continue
            if n not in topk_name_to_index:
                continue

            # rel_idx = topk_name_to_index[n]
            # if rel_idx >= relevance_scores.shape[0]:
            #     continue
            neighbor_rel_idx = topk_name_to_index[n]
            if neighbor_rel_idx >= len(row_labels):
                continue
            
            rel_score = relevance_scores[neighbor_rel_idx].sum().item()
            neighbor_scores[neighbor_rel_idx] = rel_score


        if neighbor_scores:
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:2])
        gene_to_neighbors[gene] = neighbor_scores

    # 📁 Save gene-neighbor relationships (CSV export)
    all_neighbor_rows = []
    for gene, neighbors in gene_to_neighbors.items():
        # graph_node_idx = name_to_index[gene]
        # gene_cluster = int(cluster_assignments[graph_node_idx])
        gene_rel_idx = topk_name_to_index[gene]
        gene_cluster = int(row_labels[gene_rel_idx])
        for neighbor_idx, score in neighbors.items():
            neighbor_name = node_names_topk[neighbor_idx]
            neighbor_cluster = int(row_labels[neighbor_idx])
            all_neighbor_rows.append([
                gene, 
                neighbor_name, 
                neighbor_idx, 
                float(score), 
                neighbor_cluster
            ])
        # for neighbor_idx, score in neighbors.items():
        #     if neighbor_idx >= len(cluster_assignments):
        #         continue
        #     neighbor_name = node_id_to_name[neighbor_idx]
        #     neighbor_cluster = int(cluster_assignments[neighbor_idx])
        #     # 🧼 Skip neighbors with cluster -1
        #     if neighbor_cluster == -1:
        #         continue
        #     all_neighbor_rows.append([
        #         gene,
        #         neighbor_name,
        #         str(neighbor_idx),
        #         float(score),
        #         neighbor_cluster
        #     ])

    # 💾 Write CSV
    os.makedirs(output_dir, exist_ok=True)
    neighbors_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_gene_neighbors_epo{args.num_epochs}.csv"
    )
    with open(neighbors_csv_path, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Gene", "Neighbor", "Neighbor_Index", "Relevance_Score", "Neighbor_Cluster"])
        writer.writerows(all_neighbor_rows)
    print(f"📄 Saved gene-neighbor relevance data to: {neighbors_csv_path}")

    source = []
    target = []
    value = []
    link_colors = []

    existing_edges = set()  # Track (source, target) to avoid A→B and B→A cycles

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        # genes_sorted = sorted(genes, key=lambda g: relevance_scores[name_to_index[g]].sum(), reverse=True)[:10]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[topk_name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                # saliency = relevance_scores[rel_idx].sum().item()
                # graph_node_idx = name_to_index[gene]
                # gene_cluster = cluster_assignments[graph_node_idx]
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18)

            gene_idx = label_to_idx[gene]

            # Cluster → Gene link
            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            neighbors = gene_to_neighbors.get(gene, {})
            # for neighbor_idx, neighbor_score in neighbors.items():
            #     neighbor_name = node_id_to_name[neighbor_idx]

            #     # ✅ NEW: Only allow neighbor if in top-k
            #     rel_idx = topk_name_to_index.get(neighbor_name)
            #     if rel_idx is None or rel_idx >= len(row_labels):
            #         continue  # skip: neighbor not top-k or no cluster

            #     neighbor_cluster = row_labels[rel_idx]

            for neighbor_idx, neighbor_score in gene_to_neighbors.get(gene, {}).items():
                neighbor_name = node_names_topk[neighbor_idx]

                if neighbor_name not in topk_name_to_index:
                    continue

                # Skip self-link just in case
                if neighbor_name == gene:
                    continue

                # Add neighbor label if new
                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    saliency = relevance_scores[rel_idx].sum().item()
                    neighbor_cluster = row_labels[neighbor_idx]
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16 if saliency > 0.5 else 10)
                    # if saliency > 0.5:
                    #     highlight_node_indices.append(label_to_idx[neighbor_name])
                    #     highlight_node_saliency.append(saliency)

                neighbor_node_idx = label_to_idx[neighbor_name]
                neighbor_cluster = row_labels[neighbor_idx]

                # Add neighbor cluster label if new
                neighbor_cluster_label = f"nClust {neighbor_cluster}"
                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                # Gene → Neighbor
                if (gene_idx, neighbor_node_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                # Neighbor → Cluster
                if (neighbor_node_idx, neighbor_cluster_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
        showlegend=False,
        paper_bgcolor='white',
        plot_bgcolor='rgba(0,0,0,0)',
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=600, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")

    sankey_stats = analyze_sankey_structure(
        source,
        target,
        value,
        label_to_idx,
        node_names,
        cluster_to_genes,
        gene_to_neighbors,
        row_labels,
        name_to_index,
        relevance_scores 
    )

    entropy = sankey_stats["cluster_entropy"]
    entropy_df = pd.DataFrame(list(entropy.items()), columns=["Cluster", "Entropy"]).sort_values("Entropy", ascending=False)
    plt.figure(figsize=(8, 5))
    sns.barplot(x="Entropy", y="Cluster", data=entropy_df, palette="coolwarm")
    plt.title("Cluster Entropy (Gene Participation Diversity)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_entropy_bar_epo{args.num_epochs}.png"))
    plt.close()

    jaccard = sankey_stats["cluster_jaccard"]
    jaccard_df = pd.DataFrame([
        {"Cluster1": c1, "Cluster2": c2, "Jaccard": val}
        for (c1, c2), val in jaccard.items()
    ])
    pivot_df = jaccard_df.pivot(index="Cluster1", columns="Cluster2", values="Jaccard").fillna(0)
    plt.figure(figsize=(10, 8))
    sns.heatmap(pivot_df, cmap="viridis", annot=True, fmt=".2f", square=True, cbar_kws={'label': 'Jaccard Index'})
    plt.title("Jaccard Similarity Between Confirmed Gene Clusters")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_jaccard_heatmap_epo{args.num_epochs}.png"))
    plt.close()

    centrality_df = pd.DataFrame(list(sankey_stats["gene_degree_centrality"].items()), columns=["Gene", "Centrality"])
    centrality_df = centrality_df.sort_values("Centrality", ascending=False)
    plt.figure(figsize=(10, 15))
    sns.barplot(x="Centrality", y="Gene", data=centrality_df, palette="magma")
    plt.title("Neighbor Centrality (Sum of Relevance)")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{args.model_type}_{args.net_type}_centrality_bar_epo{args.num_epochs}.png"))
    plt.close()

    return sankey_stats

def plot_confirmed_neighbors_bio_ori(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes, #confirmed_genes,
    # scores,
    row_labels,
    total_clusters,
    output_dir,
    relevance_scores):
    # Build safe top-k index mapping
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    for gene in predicted_cancer_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Gene {gene} not in top-k node list.")
            continue

        node_idx = topk_name_to_index[gene]
        # gene_score = relevance_scores[node_idx]
        gene_score = relevance_scores[node_idx].sum().item()  # OR .mean().item()

        gene_cluster = graph.ndata["cluster_bio"][node_idx].item()
        # gene_cluster = int(row_labels[node_idx])

        print(f"{gene} → Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])

        # Filter only those neighbors present in top-k
        neighbor_scores_dict = {}
        for n in neighbors:
            if n == gene:
                continue  # ✅ Skip self
            if n in topk_name_to_index:
                rel_idx = topk_name_to_index[n]
                if rel_idx < relevance_scores.shape[0]:
                    rel_score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores_dict[rel_idx] = rel_score


        if not neighbor_scores_dict:
            print(f"⚠️ No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores_dict.items(), key=lambda x: -x[1])[:10])

        # plot_path = os.path.join(
        #     "results/gene_prediction/bio_neighbor_feature_contributions/",
        #     f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        # )

        # plot_neighbor_relevance(
        #     neighbor_scores=top_neighbors,
        #     gene_name=f"{gene} (Cluster {gene_cluster})",
        #     node_id_to_name=node_id_to_name,
        #     output_path=plot_path,
        #     row_labels=row_labels,
        #     total_clusters=total_clusters,
        #     add_legend=False
        # )
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

def plot_confirmed_neighbors_bio_(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes,
    row_labels,
    total_clusters,
    output_dir,
    relevance_scores,
    top_k=1000  # used in filename
):
    import os
    import csv

    # Mapping
    # topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    # Get neighbor dictionary
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, predicted_cancer_genes)

    # Store all rows to write
    all_neighbor_rows = []

    for gene in predicted_cancer_genes:
        if gene not in name_to_index:
            print(f"⚠️ Gene {gene} not in top-k list.")
            continue

        node_idx = name_to_index[gene]
        graph_node_idx = name_to_index[gene]
        gene_score = relevance_scores[node_idx].sum().item()
        gene_cluster = int(cluster_assignments[graph_node_idx])

        print(f"{gene} → Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for neighbor in neighbors:
            if neighbor == gene:
                continue
            if neighbor in name_to_index:
                rel_idx = name_to_index[neighbor]
                if rel_idx < relevance_scores.shape[0]:
                    score = relevance_scores[rel_idx].sum().item()
                    neighbor_scores[rel_idx] = score

        if not neighbor_scores:
            print(f"⚠️ No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:10])

        for neighbor_idx, score in top_neighbors.items():
            neighbor_name = node_id_to_name.get(neighbor_idx, "UNK")
            neighbor_cluster = int(cluster_assignments[neighbor_idx])
            if neighbor_cluster == -1:
                continue  # Skip unclustered
            all_neighbor_rows.append([
                gene,                      # Source_Gene
                neighbor_name,             # Neighbor_Name
                str(neighbor_idx),         # Neighbor_Index
                round(score, 5),           # Relevance_Score
                neighbor_cluster           # Cluster
            ])

        # Plot relevance bar
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )
        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

    # ✅ Save final CSV
    os.makedirs(output_dir, exist_ok=True)
    combined_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_top{top_k}_confirmed_neighbors_epo{args.num_epochs}.csv"
    )

    with open(combined_csv_path, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Source_Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Cluster"])
        writer.writerows(all_neighbor_rows)

    print(f"📄 Combined confirmed neighbor CSV saved to: {combined_csv_path}")

def plot_confirmed_neighbors_bio(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes,
    node_names_topk,  # <-- used to build index
    row_labels,
    total_clusters,
    output_dir,
    relevance_scores,
    top_k=1000
):
    import os
    import csv

    # Create index mappings
    topk_name_to_index = {name: i for i, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    cluster_assignments = graph.ndata["cluster_bio"].numpy()

    # Sort predicted genes by bio saliency
    top_scored_genes = sorted(
        [g for g in predicted_cancer_genes if g in topk_name_to_index and topk_name_to_index[g] < relevance_scores.shape[0]],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    # Get neighbors
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, top_scored_genes)

    all_neighbor_rows = []

    for gene in top_scored_genes:
        if gene not in name_to_index or gene not in topk_name_to_index:
            print(f"⚠️ Skipping gene not in index: {gene}")
            continue

        node_idx = topk_name_to_index[gene]
        graph_node_idx = name_to_index[gene]
        gene_score = relevance_scores[node_idx].sum().item()
        gene_cluster = int(cluster_assignments[graph_node_idx])

        print(f"{gene} → Node {node_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for neighbor in neighbors:
            if neighbor == gene or neighbor not in topk_name_to_index:
                continue
            rel_idx = topk_name_to_index[neighbor]
            if rel_idx < relevance_scores.shape[0]:
                score = relevance_scores[rel_idx].sum().item()
                neighbor_scores[rel_idx] = score

        if not neighbor_scores:
            print(f"⚠️ No valid neighbors found for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:10])

        # for neighbor_idx, score in top_neighbors.items():
        #     neighbor_name = node_id_to_name.get(neighbor_idx, "UNK")
        #     neighbor_cluster = int(cluster_assignments[neighbor_idx])
        #     if neighbor_cluster == -1:
        #         continue
        #     all_neighbor_rows.append([
        #         gene,
        #         neighbor_name,
        #         str(neighbor_idx),
        #         round(score, 5),
        #         neighbor_cluster
        #     ])

        for neighbor_idx, score in top_neighbors.items():
            # Get the neighbor name safely from top-k
            neighbor_name = next((n for n, i in topk_name_to_index.items() if i == neighbor_idx), None)
            if neighbor_name is None:
                continue  # skip if not top-k mapped

            neighbor_cluster = int(cluster_assignments[name_to_index[neighbor_name]])
            if neighbor_cluster == -1:
                continue

            all_neighbor_rows.append([
                gene,
                neighbor_name,
                str(neighbor_idx),
                round(score, 5),
                neighbor_cluster
            ])

        # Plot relevance bar
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )
        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

    # ✅ Save final CSV
    os.makedirs(output_dir, exist_ok=True)
    combined_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_top{top_k}_confirmed_neighbors_epo{args.num_epochs}.csv"
    )
    with open(combined_csv_path, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Source_Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Cluster"])
        writer.writerows(all_neighbor_rows)

    print(f"📄 Combined confirmed neighbor CSV saved to: {combined_csv_path}")

def apply_biclustering_with_heatmap_plot_topo_0_63(
    graph,
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    topk_node_indices,
    predicted_cancer_genes: list,
    cluster_colors: dict,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):

    os.makedirs(output_dir, exist_ok=True)
    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"

    ############saliency_matrix = saliency_matrix.mean(axis=0) ##reduce to 1D
    ##feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04
    ###saliency_matrix = (saliency_matrix - saliency_matrix.min()) / (saliency_matrix.max() - saliency_matrix.min()) * 20
    ##saliency_matrix_norm = normalize(saliency_matrix, axis=1)
    saliency_matrix = normalize(saliency_matrix, axis=1)

    best_model, best_score = None, np.inf
    valid = False
    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix)
        reordered = saliency_matrix[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix, reordered)

        if mse < best_score:
            best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Valid biclustering not found.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_

    row_order = []
    for cid in np.unique(row_labels):
        cluster_idx = np.where(row_labels == cid)[0]
        row_sums = saliency_matrix[cluster_idx].sum(axis=1)
        sorted_idx = cluster_idx[np.argsort(-row_sums)]
        row_order.extend(sorted_idx)

    # --- SORTING ---
    # # col_order = []
    # col_avgs = saliency_matrix.mean(axis=0)
    # sorted_col_indices = np.argsort(-col_avgs)
    # col_order.extend(sorted_col_indices)

    # --- NO SORTING ---
    col_order = list(range(saliency_matrix.shape[1]))
    col_avgs = saliency_matrix.mean(axis=0)

    clustered_matrix = saliency_matrix[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]

    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(os.path.join(output_dir, "topo_biclustering_row_labels.csv"), index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names)\
        .to_csv(os.path.join(output_dir, "topo_biclustering_heatmap_matrix.csv"))

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    fig = plt.figure(figsize=(20, 16))
    gs = gridspec.GridSpec(2, 2, height_ratios=[0.8, 20], width_ratios=[20, 0.5], hspace=0.0, wspace=0.0)

    ax_top_bar = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[1, 0])
    ax_curve = fig.add_subplot(gs[1, 1], sharey=ax_heatmap)

    ax_top_bar.bar(
        x=np.arange(clustered_matrix.shape[1]) + 0.5,
        height=col_avgs[col_order],#*2,
        width=1.0,
        color='#4682B4',
        edgecolor='black',
        linewidth=0.3
    )
    
    ax_top_bar.set_xlim(0, clustered_matrix.shape[1])
    ax_top_bar.set_ylim(0, clustered_matrix[:, col_order].max() * 1.2)
    # ax_top_bar.set_xticks(np.arange(len(col_order)) + 0.5)
    # ax_top_bar.set_xticklabels([f"{i+1:02d}" for i in range(len(col_order))], rotation=90, fontsize=20)
    ax_heatmap.tick_params(axis='x', which='both', bottom=True, top=False, length=5, pad=1)
    
    ax_top_bar.set_yticks([])
    for spine in ax_top_bar.spines.values():
        spine.set_visible(False)

    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        xticklabels=True,
        yticklabels=False,
        cbar=False,
        ax=ax_heatmap
    )
    for i, cluster in enumerate(reordered_cluster_labels):
        ax_heatmap.add_patch(
            plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')), clip_on=False)
        )

    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax_heatmap.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)
        start += size

    saliency_sums = clustered_matrix.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())

    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.xaxis.set_label_position('top')
    ax_curve.set_xlim([0, 1])
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.invert_yaxis()
    for spine in ax_curve.spines.values():
        spine.set_visible(False)
    ax_curve.tick_params(axis='y', left=False, labelleft=False)

    current_idx = 0
    for cluster_id, cluster_size in zip(unique_clusters, cluster_sizes):
        y_vals = np.arange(current_idx, current_idx + cluster_size)
        cluster_saliency = saliency_sums[current_idx: current_idx + cluster_size]
        cluster_color = cluster_colors.get(cluster_id, '#CCCCCC')
        ax_curve.fill_betweenx(y_vals, 0, cluster_saliency, color=cluster_color, alpha=0.9, linewidth=0)
        current_idx += cluster_size

    fig.subplots_adjust(top=0.99, bottom=0.01, hspace=0.0)
    out_fig_path = os.path.join(output_dir, "topo_biclustering_heatmap_curve.png")
    plt.savefig(out_fig_path, dpi=300, bbox_inches='tight', pad_inches=0.1)
    plt.close()
    print(f"[Saved] Topo Heatmap → {out_fig_path}")

    # === Stats
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo_summary'] = row_labels_tensor

    total_genes_per_cluster = {cid: int((row_labels == cid).sum()) for cid in np.unique(row_labels)}
    pred_counts = {}
    for cid in np.unique(row_labels):
        idx = np.where(row_labels == cid)[0]
        names_in_cluster = [node_names_topk[i] for i in idx]
        pred_counts[cid] = sum(1 for g in names_in_cluster if g in predicted_cancer_genes)

    # Save cluster stats
    pd.DataFrame(list(total_genes_per_cluster.items()), columns=["Cluster", "TotalGenes"])\
        .to_csv(os.path.join(output_dir, "topo_biclustering_total_genes.csv"), index=False)
    pd.DataFrame(list(pred_counts.items()), columns=["Cluster", "PredictedGenes"])\
        .to_csv(os.path.join(output_dir, "topo_biclustering_predicted_counts.csv"), index=False)

    # ✅ RETURN values for unpacking
    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts



def apply_full_spectral_biclustering_bio(
    graph, summary_bio_features, node_names_topk, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):

    print("🧪 Running Spectral Biclustering with fixed (10, 5) clusters...")

    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")

    summary_bio_features_topk = summary_bio_features  # Already filtered to top-k
    assert summary_bio_features_topk.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_topk.shape[1]}"

    n_clusters_row = 10
    n_clusters_col = 5

    best_model = None
    best_score = np.inf

    print("🔁 Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(
            n_clusters=(n_clusters_row, n_clusters_col),
            method='bistochastic',
            svd_method='randomized',
            random_state=i
        )
        model.fit(summary_bio_features_topk)
        reconstructed = summary_bio_features_topk[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features_topk, reconstructed)
        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)

    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names_topk, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === Plotting
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "heatmap_unsort.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    # === Save gene-cluster assignment
    gene_assignment_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_summary_gene_cluster_assignments_epo{args.num_epochs}.csv"
    )

    assignment_rows = []
    for i, (idx, cluster) in enumerate(zip(topk_node_indices, row_labels)):
        gene_name = node_names_topk[i] if i < len(node_names_topk) else f"node_{idx}"
        rel_score = summary_bio_features[i].sum().item()
        assignment_rows.append([gene_name, idx, cluster, round(rel_score, 5)])
    assignment_rows.sort(key=lambda x: (x[2], -x[3]))

    with open(gene_assignment_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Gene_Name", "Index", "Cluster", "Relevance_Score"])
        writer.writerows(assignment_rows)

    print(f"📄 Saved gene-cluster assignment info to: {gene_assignment_path}")

    # === Save gene-neighbor cluster information (top-k only)
    topk_set = set(topk_node_indices)
    node_id_to_local_index = {global_id: i for i, global_id in enumerate(topk_node_indices)}

    neighbor_info_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_topk_gene_neighbors_epo{args.num_epochs}.csv"
    )

    with open(neighbor_info_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            "Gene", "Gene_Index", "Gene_Cluster", "Gene_Score",
            "Neighbor", "Neighbor_Index", "Neighbor_Cluster", "Edge_Weight"
        ])
        for i, cluster in zip(topk_node_indices, row_labels):
            if i not in node_id_to_local_index:
                continue
            local_i = node_id_to_local_index[i]
            gene_name = node_names_topk[local_i] if local_i < len(node_names_topk) else f"node_{i}"
            rel_score = summary_bio_features[local_i].sum().item()

            neighbors = graph.successors(i)
            edge_ids = graph.edge_ids(i, neighbors)

            # for j, e_id in zip(neighbors.tolist(), edge_ids.tolist()):
            #     if j not in topk_set:
            #         continue
            #     local_j = node_id_to_local_index[j]
            #     neighbor_name = node_names_topk[local_j] if local_j < len(node_names_topk) else f"node_{j}"
            #     neighbor_cluster = int(row_labels[local_j])
            #     weight = float(graph.edata['weight'][e_id].item()) if 'weight' in graph.edata else 1.0

            #     writer.writerow([
            #         gene_name, i, cluster, round(rel_score, 5),
            #         neighbor_name, j, neighbor_cluster, round(weight, 4)
            #     ])
            for j, e_id in zip(neighbors.tolist(), edge_ids.tolist()):
                # ✅ Keep only neighbors in topk set
                if j not in topk_set:
                    continue

                # Safe local index
                local_j = node_id_to_local_index.get(j, None)
                if local_j is None or local_j >= len(node_names_topk):
                    continue  # Should not happen, but safe guard

                neighbor_name = node_names_topk[local_j]
                neighbor_cluster = int(row_labels[local_j])

                weight = float(graph.edata['weight'][e_id].item()) if 'weight' in graph.edata else 1.0

                writer.writerow([
                    gene_name, i, cluster, round(rel_score, 5),
                    neighbor_name, j, neighbor_cluster, round(weight, 4)
                ])

    print(f"📄 Saved top-k gene neighbor info to: {neighbor_info_path}")

    # === Save top-k gene cluster info (global + local index, cluster, score)
    assigned_info_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_topk_gene_assignments_epo{args.num_epochs}.csv"
    )

    with open(assigned_info_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Gene_Name", "Global_Index", "Local_Index", "Assigned_Cluster", "Relevance_Score"])
        for local_idx, (global_idx, cluster) in enumerate(zip(topk_node_indices, row_labels)):
            gene_name = node_names_topk[local_idx] if local_idx < len(node_names_topk) else f"node_{global_idx}"
            rel_score = summary_bio_features[local_idx].sum().item()
            writer.writerow([gene_name, global_idx, local_idx, int(cluster), round(rel_score, 5)])

    print(f"📄 Saved top-k gene cluster assignments to: {assigned_info_path}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_full_spectral_biclustering_bio_summary(
    graph, summary_bio_features, node_names_topk, omics_splits,
    predicted_cancer_genes,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    args,
    topk_node_indices=None
):

    print("🧪 Running Spectral Biclustering with fixed (10, 5) clusters...")

    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")

    summary_bio_features_topk = summary_bio_features  # Already filtered to top-k
    assert summary_bio_features_topk.shape[1] == 64, f"Expected 64 summary features, got {summary_bio_features_topk.shape[1]}"

    n_clusters_row = 10
    n_clusters_col = 5

    best_model = None
    best_score = np.inf

    print("🔁 Running biclustering trials:")
    for i in range(10):
        model = SpectralBiclustering(
            n_clusters=(n_clusters_row, n_clusters_col),
            method='bistochastic',
            svd_method='randomized',
            random_state=i
        )
        model.fit(summary_bio_features_topk)
        reconstructed = summary_bio_features_topk[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(summary_bio_features_topk, reconstructed)
        if mse < best_score:
            best_score = mse
            best_model = model

    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering complete.")

    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)

    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names_topk, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === Plotting
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "heatmap_unsort.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=summary_bio_features_topk,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    # === Save gene-cluster assignment
    gene_assignment_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_summary_gene_cluster_assignments_epo{args.num_epochs}.csv"
    )

    assignment_rows = []
    for i, (idx, cluster) in enumerate(zip(topk_node_indices, row_labels)):
        gene_name = node_names_topk[i] if i < len(node_names_topk) else f"node_{idx}"
        rel_score = summary_bio_features[i].sum().item()
        assignment_rows.append([gene_name, idx, cluster, round(rel_score, 5)])
    assignment_rows.sort(key=lambda x: (x[2], -x[3]))

    with open(gene_assignment_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Gene_Name", "Index", "Cluster", "Relevance_Score"])
        writer.writerows(assignment_rows)

    print(f"📄 Saved gene-cluster assignment info to: {gene_assignment_path}")

    # === Save gene-neighbor cluster information (top-k only)
    topk_set = set(topk_node_indices)
    node_id_to_local_index = {global_id: i for i, global_id in enumerate(topk_node_indices)}

    neighbor_info_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_topk_gene_neighbors_epo{args.num_epochs}.csv"
    )

    with open(neighbor_info_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            "Gene", "Gene_Index", "Gene_Cluster", "Gene_Score",
            "Neighbor", "Neighbor_Index", "Neighbor_Cluster", "Edge_Weight"
        ])
        for i, cluster in zip(topk_node_indices, row_labels):
            if i not in node_id_to_local_index:
                continue
            local_i = node_id_to_local_index[i]
            gene_name = node_names_topk[local_i] if local_i < len(node_names_topk) else f"node_{i}"
            rel_score = summary_bio_features[local_i].sum().item()

            neighbors = graph.successors(i)
            edge_ids = graph.edge_ids(i, neighbors)

            for j, e_id in zip(neighbors.tolist(), edge_ids.tolist()):
                if j not in topk_set:
                    continue
                local_j = node_id_to_local_index[j]
                neighbor_name = node_names_topk[local_j] if local_j < len(node_names_topk) else f"node_{j}"
                neighbor_cluster = int(row_labels[local_j])
                weight = float(graph.edata['weight'][e_id].item()) if 'weight' in graph.edata else 1.0

                writer.writerow([
                    gene_name, i, cluster, round(rel_score, 5),
                    neighbor_name, j, neighbor_cluster, round(weight, 4)
                ])

    print(f"📄 Saved top-k gene neighbor info to: {neighbor_info_path}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_collapsed_clusterfirst_multilevel_sankey_bio_pas(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    output_dir,
    CLUSTER_COLORS
):


    def hex_to_rgba(hex_color, alpha):
        hex_color = hex_color.lstrip("#")
        r, g, b = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
        return f"rgba({r},{g},{b},{alpha})"

    # ✅ Proper top-k map
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}  # still useful for full lookup

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "BRCA2"]

    combined_genes = []
    seen = set()
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 100:
            break

    # You can keep your get_neighbors_gene_names helper
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}

    # 🔑 STEP 1 — compute only genes with neighbors
    gene_to_neighbors = {}
    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        rel_idx = topk_name_to_index[gene]
        if rel_idx >= len(row_labels):
            continue

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n == gene:
                continue
            if n in combined_genes:
                continue
            if n not in topk_name_to_index:
                continue

            neighbor_rel_idx = topk_name_to_index[n]
            if neighbor_rel_idx >= len(row_labels):
                continue

            rel_score = relevance_scores[neighbor_rel_idx].sum().item()
            neighbor_scores[neighbor_rel_idx] = rel_score


        k = 3

        if len(neighbor_scores) >= k:
            # Optionally, still sort & limit to top k if you want
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:k])
            gene_to_neighbors[gene] = neighbor_scores

    # 🔑 STEP 2 — build clusters for only valid genes
    cluster_to_genes = {}
    for gene in gene_to_neighbors.keys():
        rel_idx = topk_name_to_index[gene]
        gene_cluster = row_labels[rel_idx]
        cluster_label = f"Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)


    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_gene_neighbors_epo{args.num_epochs}.csv"
    )
    with open(csv_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Gene", "Neighbor", "Neighbor_Index", "Relevance_Score", "Neighbor_Cluster"])
        writer.writerows(cluster_to_genes)
    print(f"✅ Gene neighbor CSV saved: {csv_path}")

    # Sankey
    source = []
    target = []
    value = []
    link_colors = []
    existing_edges = set()

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[topk_name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18)

            gene_idx = label_to_idx[gene]

            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            for neighbor_idx, neighbor_score in gene_to_neighbors.get(gene, {}).items():
                neighbor_name = node_names_topk[neighbor_idx]

                if neighbor_name not in topk_name_to_index:
                    continue

                if neighbor_name == gene:
                    continue

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    neighbor_cluster = row_labels[neighbor_idx]
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16)

                neighbor_node_idx = label_to_idx[neighbor_name]
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nClust {neighbor_cluster}"

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                if (gene_idx, neighbor_node_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                if (neighbor_node_idx, neighbor_cluster_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    save_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    )
    fig.write_html(save_path)
    print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=600, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")


    # save_path = os.path.join(
    #     output_dir,
    #     f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    # )
    # fig.write_html(save_path)
    # print(f"✅ Sankey saved: {save_path}")

    return fig

def plot_confirmed_neighbors_bio(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes,
    node_names_topk,  # used to build index
    row_labels,
    total_clusters,
    output_dir,
    relevance_scores,
    top_k=1000
):

    # ✅ Correct top-k index
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Always use row_labels for cluster source — graph only for fallback
    # Sort predicted genes by summed saliency
    top_scored_genes = sorted(
        [g for g in predicted_cancer_genes if g in topk_name_to_index],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    print(f"✔️ Number of predicted genes in top-k: {len(top_scored_genes)}")

    # Get neighbors
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, top_scored_genes)

    all_neighbor_rows = []

    for gene in top_scored_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Skipping {gene} not in top-k index")
            continue

        rel_idx = topk_name_to_index[gene]
        gene_score = relevance_scores[rel_idx].sum().item()
        gene_cluster = int(row_labels[rel_idx])  # ✅ Use row_labels
        print(f"{gene} → RelIdx {rel_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for neighbor in neighbors:
            if neighbor == gene:
                continue
            if neighbor not in topk_name_to_index:
                continue  # ✅ strict: skip if not top-k
            neighbor_idx = topk_name_to_index[neighbor]
            if neighbor_idx >= relevance_scores.shape[0]:
                continue

            score = relevance_scores[neighbor_idx].sum().item()
            neighbor_scores[neighbor_idx] = score

        if not neighbor_scores:
            print(f"⚠️ No valid top-k neighbors for {gene}.")
            continue

        top_neighbors = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:10])

        for neighbor_idx, score in top_neighbors.items():
            neighbor_name = node_names_topk[neighbor_idx]
            neighbor_cluster = int(row_labels[neighbor_idx])  # ✅ Use row_labels for neighbor too
            if neighbor_cluster == -1:
                continue

            all_neighbor_rows.append([
                gene,
                neighbor_name,
                str(neighbor_idx),
                round(score, 5),
                neighbor_cluster
            ])

        # Plot relevance bar for each gene’s top neighbors
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )
        
        topk_node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=topk_node_id_to_name,  # ✅ correct!

            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

    os.makedirs(output_dir, exist_ok=True)
    combined_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_top{top_k}_confirmed_neighbors_epo{args.num_epochs}.csv"
    )
    with open(combined_csv_path, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Source_Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Cluster"])
        writer.writerows(all_neighbor_rows)

    print(f"📄 Confirmed neighbor CSV saved: {combined_csv_path}")

def plot_biclustering_input_feature_heatmap_bio_sorted_pa(
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    cluster_colors: dict,
    omics_colors: dict,
    output_path: str,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):
    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    saliency_matrix_norm = normalize(saliency_matrix, axis=1)

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:saliency_matrix.shape[1]]

    best_model, best_score = None, np.inf
    valid = False

    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials with Constraint", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix_norm)
        reordered = saliency_matrix_norm[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix_norm, reordered)

        col_labels = model.column_labels_
        col_order = np.argsort(col_labels)
        reordered_feature_labels = [feature_names[i].split(": ")[1] for i in col_order]

        cluster_to_cancers = defaultdict(set)
        for idx, cluster_id in enumerate(col_labels):
            cancer = feature_names[idx].split(": ")[1]
            cluster_to_cancers[cluster_id].add(cancer)

        cluster_ok = all(len(cancers) >= 2 for cancers in cluster_to_cancers.values())

        if cluster_ok:
            if mse < best_score:
                best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Could not find a valid biclustering where all column clusters contain ≥ 2 cancers.")

    print(f"✅ Found valid biclustering with all column clusters containing ≥ 2 cancers.")

    model = best_model
    # row_labels = model.row_labels_
    # col_labels = model.column_labels_
    # row_order = np.argsort(row_labels)
    # col_order = np.argsort(col_labels)
    
    row_labels = model.row_labels_
    col_labels = model.column_labels_

    # === Sort rows by cluster, then by saliency sum within cluster
    cluster_ids = np.unique(row_labels)
    row_order = []

    for cid in np.sort(cluster_ids):
        cluster_idx = np.where(row_labels == cid)[0]
        cluster_saliency = saliency_matrix_norm[cluster_idx]
        row_sums = cluster_saliency.sum(axis=1)
        sorted_idx = cluster_idx[np.argsort(-row_sums)]  # Descending order
        row_order.extend(sorted_idx)

    # Sort columns by cluster (same as before)
    # col_order = np.argsort(col_labels)
    # reordered_feature_labels = [feature_names[i].split(": ")[1] for i in col_order]
    # reordered_omics_labels = [feature_names[i].split(": ")[0] for i in col_order]
    # === Sort columns globally by omics group, then within omics by average saliency
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_avgs = saliency_matrix_norm.mean(axis=0)

    omics_grouped_indices = {omics: [] for omics in omics_order}
    for i, fname in enumerate(feature_names):
        omics = fname.split(":")[0].strip().upper()
        if omics in omics_grouped_indices:
            omics_grouped_indices[omics].append(i)

    col_order = []
    reordered_feature_labels = []
    reordered_omics_labels = []
    feature_colors = []

    for omics in omics_order:
        indices = omics_grouped_indices[omics]
        avg_vals = feature_avgs[indices]
        sorted_within = [i for _, i in sorted(zip(avg_vals, indices), reverse=True)]
        
        col_order.extend(sorted_within)
        reordered_feature_labels.extend([feature_names[i].split(": ")[1] for i in sorted_within])
        reordered_omics_labels.extend([feature_names[i].split(": ")[0] for i in sorted_within])
        feature_colors.extend([omics_colors.get(omics.upper(), 'gray')] * len(sorted_within))

    # Sort column features by omics type
    '''omics_order = ['CNA', 'GE', 'METH', 'MF']  # Preserve this order
    col_label_pairs = [(i, col_labels[i]) for i in range(len(feature_names))]
    omics_grouped = {omics: [] for omics in omics_order}

    for i, label in col_label_pairs:
        omics = feature_names[i].split(":")[0].strip().upper()
        if omics in omics_grouped:
            omics_grouped[omics].append(i)

    col_order = []
    reordered_feature_labels = []
    reordered_omics_labels = []

    for omics in omics_order:
        col_order.extend(omics_grouped[omics])
        reordered_feature_labels.extend([feature_names[i].split(": ")[1] for i in omics_grouped[omics]])
        reordered_omics_labels.extend([feature_names[i].split(": ")[0] for i in omics_grouped[omics]])'''
    #############################


    clustered_matrix = saliency_matrix_norm[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]
    
    # Save label and matrix CSVs
    base_path = output_path.replace(".png", "_with_curve")
    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(base_path + "_cluster_labels.csv", index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names, columns=reordered_feature_labels)\
        .to_csv(base_path + "_heatmap_matrix.csv")
    pd.DataFrame({
        gene: [reordered_feature_labels[i] for i in np.argsort(-clustered_matrix[i])[:top_k]]
        for i, gene in enumerate(reordered_gene_names)
    }).T.rename(columns=lambda i: f"Top{i+1}")\
     .to_csv(base_path + f"_top{top_k}_features_per_gene.csv")

    # Colormap setup
    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(clustered_matrix, 99)


    fig = plt.figure(figsize=(22, 20))

    ##gs = gridspec.GridSpec(2, 1, height_ratios=[0.4, 19], hspace=0.0)
    gs = gridspec.GridSpec(2, 1, height_ratios=[1.2, 18], hspace=0.0)
    
    ax_top_bar = fig.add_subplot(gs[0])
    
    feature_means = saliency_matrix_norm[:, col_order].mean(axis=0)
    feature_means_norm = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ##feature_means_norm = normalize(feature_means, axis=1)

    ax_top_bar.bar(
        x=np.arange(len(feature_means_norm)),
        height=feature_means_norm,
        width=1.0,
        color=feature_colors,
        edgecolor='black',
        linewidth=0.3
    )

    ax_top_bar.set_xlim(0, len(feature_means_norm))
    ax_top_bar.set_ylim(0, 1.05)
    ax_top_bar.set_xticks([])
    ax_top_bar.set_yticks([])
    ax_top_bar.set_frame_on(False)
    ##################################


    # === Add omics-colored top bar showing per-feature relevance
    ax_col_cluster = fig.add_subplot(gs[0])
    ax_top_bar = ax_col_cluster.inset_axes([0.0, 1.05, 1.0, 0.6])  # [x0, y0, width, height]
    feature_means = saliency_matrix_norm[:, col_order].mean(axis=0)
    feature_means_norm = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)

    ax_top_bar.bar(
        x=np.arange(len(feature_means_norm)),
        height=feature_means_norm,
        width=1.0,
        color=feature_colors,
        edgecolor='black',
        linewidth=0.3
    )

    ax_top_bar.set_xlim(0, len(feature_means_norm))
    ax_top_bar.set_ylim(0, 1.05)
    ax_top_bar.axis('off')
    #######################################
        
    # Add a thin omics color bar above cluster color bar
    omics_colors_bar = np.array([[to_rgb(omics_colors.get(o.upper(), "#000000")) for o in reordered_omics_labels]])
    ax_omics_bar = ax_col_cluster.inset_axes([0, 1.05, 1, 0.15])  # [x0, y0, width, height]
    ax_omics_bar.imshow(omics_colors_bar, aspect='auto')
    ax_omics_bar.set_xticks([])
    ax_omics_bar.set_yticks([])
    ax_omics_bar.set_frame_on(False)
        
    
    
    # Top cluster bar
    '''ax_col_cluster = fig.add_subplot(gs[0])
    col_cluster_colors = [cluster_colors.get(c, "#FFFFFF") for c in col_labels[col_order]]
    col_cluster_rgb = np.array([[to_rgb(c) for c in col_cluster_colors]])
    ax_col_cluster.imshow(col_cluster_rgb, aspect='auto', extent=[0, len(col_cluster_colors), 0, 1])
    ax_col_cluster.set_xlim([0, len(col_cluster_colors)])
    ax_col_cluster.set_xticks([])
    ax_col_cluster.set_yticks([])
    ax_col_cluster.set_frame_on(False)'''

    # Split panel: heatmap + curve
    gs_lower = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=gs[1], width_ratios=[20, 1], wspace=0.05)
    ax_heatmap = fig.add_subplot(gs_lower[0])
    ax_curve = fig.add_subplot(gs_lower[1], sharey=ax_heatmap)

    # Draw heatmap
    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax_heatmap
    )

    # Cluster stripes on heatmap
    for i, cluster in enumerate(reordered_cluster_labels):
        ax_heatmap.add_patch(plt.Rectangle((-1.5, i), 1.5, 1,
                                           linewidth=0,
                                           facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                                           clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax_heatmap.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)
        start += size

    # X-tick labels
    ax_heatmap.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax_heatmap.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=24)
    ax_heatmap.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, omics in zip(ax_heatmap.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics.upper(), 'black'))

    # Draw saliency curve per cluster
    saliency_sums = clustered_matrix.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())

    ax_curve.set_xlim([0, 1])
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=18)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.invert_yaxis()
    ax_curve.axis("off")

    current_idx = 0
    for cluster_id, cluster_size in zip(unique_clusters, cluster_sizes):
        y_vals = np.arange(current_idx, current_idx + cluster_size)
        cluster_saliency = saliency_sums[current_idx: current_idx + cluster_size]
        cluster_color = cluster_colors.get(cluster_id, '#CCCCCC')
        ax_curve.fill_betweenx(
            y_vals,
            0,
            cluster_saliency,
            color=cluster_color,
            alpha=0.9,
            linewidth=0
        )
        current_idx += cluster_size

    # Save final figure
    fig.tight_layout()
    fig.subplots_adjust(hspace=0.0)
    out_fig_path = base_path + ".png"
    plt.savefig(out_fig_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"[Saved] Clustered biclustering heatmap with saliency curve → {out_fig_path}")

def plot_collapsed_clusterfirst_multilevel_sankey_bio(
    args,
    graph,
    node_names,
    name_to_index,
    node_names_topk,
    row_labels,
    total_clusters,
    relevance_scores,
    output_dir,
    CLUSTER_COLORS
):


    def hex_to_rgba(hex_color, alpha):
        hex_color = hex_color.lstrip("#")
        r, g, b = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
        return f"rgba({r},{g},{b},{alpha})"

    # ✅ Proper top-k map
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}  # still useful for full lookup

    top_scored_genes = sorted(
        [g for g in node_names_topk if g in topk_name_to_index],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    top_scored_genes = [g for g in top_scored_genes if not g.startswith("MED")]

    selected_known_genes = ["EGFR", "BRCA2"]
    selected_known_genes = ["EGFR", "SRC"]

    combined_genes = []
    seen = set()
    for g in selected_known_genes + top_scored_genes:
        if g in name_to_index and g not in seen:
            combined_genes.append(g)
            seen.add(g)
        if len(combined_genes) == 600:
            break

    # You can keep your get_neighbors_gene_names helper
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, combined_genes)

    label_to_idx = {}
    all_labels = []
    all_colors = []
    font_sizes = []

    cluster_to_genes = {}

    # 🔑 STEP 1 — compute only genes with neighbors
    gene_to_neighbors = {}
    for gene in combined_genes:
        if gene not in topk_name_to_index:
            continue

        rel_idx = topk_name_to_index[gene]
        if rel_idx >= len(row_labels):
            continue

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for n in neighbors:
            if n == gene:
                continue
            if n in combined_genes:
                continue
            if n not in topk_name_to_index:
                continue

            neighbor_rel_idx = topk_name_to_index[n]
            if neighbor_rel_idx >= len(row_labels):
                continue

            rel_score = relevance_scores[neighbor_rel_idx].sum().item()
            neighbor_scores[neighbor_rel_idx] = rel_score


        k = 10

        if len(neighbor_scores) >= k:
            # Optionally, still sort & limit to top k if you want
            neighbor_scores = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:3])
            gene_to_neighbors[gene] = neighbor_scores

    # 🔑 STEP 2 — build clusters for only valid genes
    cluster_to_genes = {}
    for gene in gene_to_neighbors.keys():
        rel_idx = topk_name_to_index[gene]
        gene_cluster = row_labels[rel_idx]
        cluster_label = f"Cluster {gene_cluster}"
        cluster_to_genes.setdefault(cluster_label, []).append(gene)


    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_gene_neighbors_epo{args.num_epochs}.csv"
    )
    with open(csv_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Gene", "Neighbor", "Neighbor_Index", "Relevance_Score", "Neighbor_Cluster"])
        writer.writerows(cluster_to_genes)
    print(f"✅ Gene neighbor CSV saved: {csv_path}")

    # Sankey
    source = []
    target = []
    value = []
    link_colors = []
    existing_edges = set()

    for cluster_label, genes in cluster_to_genes.items():
        if cluster_label not in label_to_idx:
            label_to_idx[cluster_label] = len(all_labels)
            all_labels.append(cluster_label)
            cluster_id = int(cluster_label.split()[-1])
            all_colors.append(CLUSTER_COLORS.get(cluster_id, "#000000"))
            font_sizes.append(24)

        cluster_idx = label_to_idx[cluster_label]
        genes_sorted = sorted(genes, key=lambda g: relevance_scores[topk_name_to_index[g]].sum(), reverse=True)[:10]

        for gene in genes_sorted:
            if gene not in label_to_idx:
                label_to_idx[gene] = len(all_labels)
                all_labels.append(gene)
                rel_idx = topk_name_to_index[gene]
                gene_cluster = row_labels[rel_idx]
                color = CLUSTER_COLORS.get(gene_cluster, "#000000")
                all_colors.append(color)
                font_sizes.append(18)

            gene_idx = label_to_idx[gene]

            if (cluster_idx, gene_idx) not in existing_edges:
                source.append(cluster_idx)
                target.append(gene_idx)
                value.append(1)
                link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(int(cluster_label.split()[-1]), "#000000"), 0.4))
                existing_edges.add((cluster_idx, gene_idx))

            for neighbor_idx, neighbor_score in gene_to_neighbors.get(gene, {}).items():
                neighbor_name = node_names_topk[neighbor_idx]

                if neighbor_name not in topk_name_to_index:
                    continue

                if neighbor_name == gene:
                    continue

                if neighbor_name not in label_to_idx:
                    label_to_idx[neighbor_name] = len(all_labels)
                    all_labels.append(neighbor_name)
                    neighbor_cluster = row_labels[neighbor_idx]
                    color = CLUSTER_COLORS.get(neighbor_cluster, "#000000")
                    all_colors.append(color)
                    font_sizes.append(16)

                neighbor_node_idx = label_to_idx[neighbor_name]
                neighbor_cluster = row_labels[neighbor_idx]
                neighbor_cluster_label = f"nClust {neighbor_cluster}"

                if neighbor_cluster_label not in label_to_idx:
                    label_to_idx[neighbor_cluster_label] = len(all_labels)
                    all_labels.append(neighbor_cluster_label)
                    all_colors.append(CLUSTER_COLORS.get(neighbor_cluster, "#000000"))
                    font_sizes.append(18)

                neighbor_cluster_idx = label_to_idx[neighbor_cluster_label]

                if (gene_idx, neighbor_node_idx) not in existing_edges:
                    source.append(gene_idx)
                    target.append(neighbor_node_idx)
                    value.append(neighbor_score)
                    link_colors.append("rgba(160,160,160,0.5)")
                    existing_edges.add((gene_idx, neighbor_node_idx))

                if (neighbor_node_idx, neighbor_cluster_idx) not in existing_edges:
                    source.append(neighbor_node_idx)
                    target.append(neighbor_cluster_idx)
                    value.append(neighbor_score)
                    link_colors.append(hex_to_rgba(CLUSTER_COLORS.get(neighbor_cluster, "#000000"), 0.6))
                    existing_edges.add((neighbor_node_idx, neighbor_cluster_idx))

    fig = go.Figure(data=[go.Sankey(
        arrangement="snap",
        node=dict(
            pad=30,
            thickness=30,
            line=dict(color="black", width=0.5),
            label=all_labels,
            color=all_colors
        ),
        link=dict(
            source=source,
            target=target,
            value=value,
            color=link_colors
        )
    )])

    fig.update_layout(
        font_size=16,
        margin=dict(l=20, r=20, t=20, b=20),
        width=1200,
        height=1200,
    )

    output_dir = "results/gene_prediction/bio_collapsed_clusterfirst_multilevel_sankey/"
    os.makedirs(output_dir, exist_ok=True)

    # save_path = os.path.join(
    #     output_dir,
    #     f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    # )
    # fig.write_html(save_path)
    # print(f"✅ Collapsed Cluster-First Multi-level Sankey saved: {save_path}")

    try:
        import plotly.io as pio
        png_save_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.png"
        )
        fig.write_image(png_save_path, scale=2, width=600, height=1200)
        print(f"🖼️ PNG also saved to: {png_save_path}")
    except Exception as e:
        print(f"⚠️ Failed to save PNG: {e}")
        print("Tip: Install 'kaleido' via pip to enable static image export: pip install kaleido")


    # save_path = os.path.join(
    #     output_dir,
    #     f"{args.model_type}_{args.net_type}_bio_collapsed_clusterfirst_multilevel_sankey_epo{args.num_epochs}.html"
    # )
    # fig.write_html(save_path)
    # print(f"✅ Sankey saved: {save_path}")

    return fig

def filter_topk_midrange_relevance_genes(
    node_names,
    labels,
    scores,
    relevance_scores,
    output_dir,
    top_k=1000,
    relevance_q_low=0.1,
    relevance_q_high=0.9
):
    """
    Retains exactly top_k predicted genes (unlabeled) with relevance in mid quantile range.
    Plots score vs. relevance and returns names and relevance vectors.
    """

    import os
    import numpy as np
    import matplotlib.pyplot as plt

    os.makedirs(output_dir, exist_ok=True)

    # Compute relevance sum and normalize
    relevance_sums = relevance_scores.sum(dim=1).cpu().numpy()
    relevance_sums_norm = (relevance_sums - relevance_sums.min()) / (relevance_sums.max() - relevance_sums.min() + 1e-8)

    # Define relevance thresholds
    low_q, high_q = np.quantile(relevance_sums_norm, [relevance_q_low, relevance_q_high])

    # Only use unlabeled nodes
    non_labeled_nodes = [i for i, label in enumerate(labels) if label == -1]
    scored_relevance_data = [
        (i, node_names[i], scores[i], relevance_sums_norm[i])
        for i in non_labeled_nodes
    ]

    # Sort all by prediction score
    scored_relevance_data.sort(key=lambda x: x[2], reverse=True)

    # Retain top-k with mid-range relevance
    retained = []
    for i, name, score, rel in scored_relevance_data:
        if low_q <= rel <= high_q:
            retained.append((i, name, score, rel))
        if len(retained) == top_k:
            break

    if len(retained) < top_k:
        print(f"[Warning] Only {len(retained)} genes found in mid-relevance range.")

    # Plot
    discarded = [d for d in scored_relevance_data if d not in retained]

    plt.figure(figsize=(10, 6))
    plt.scatter([r for _, _, _, r in discarded], [s for _, _, s, _ in discarded], alpha=0.3, label='Discarded', color='lightgray')
    plt.scatter([r for _, _, _, r in retained], [s for _, _, s, _ in retained], alpha=0.8, label='Retained', color='#0077B6')
    plt.xlabel("Normalized Relevance Sum")
    plt.ylabel("Predicted Score (Sigmoid)")
    plt.title(f"Top-{top_k} Predicted Genes (Mid-range Relevance)")
    plt.legend()
    plot_path = os.path.join(output_dir, f"top{top_k}_mid_relevance_scatter.png")
    plt.tight_layout()
    plt.savefig(plot_path, dpi=300)
    plt.close()
    print(f"[Saved] Scatter plot → {plot_path}")

    # Return results
    retained_gene_names = [name for _, name, _, _ in retained]
    retained_indices = [i for i, _, _, _ in retained]
    retained_relevance = relevance_scores[retained_indices]

    return retained_gene_names, retained_relevance, retained_indices

def analyze_relevance_vs_score_save_plot(
    relevance_scores,
    scores,
    labels,
    node_names,
    output_dir,
    args,
    quantiles=(0.2, 0.8),
    top_n=1000
):
    """
    Filter non-labeled nodes by LRP relevance, select mid-range quantile,
    rank by prediction score, plot retained vs discarded and save figure.

    Parameters
    ----------
    relevance_scores : torch.Tensor or np.ndarray
        Node x Feature saliency matrix.
    scores : np.ndarray
        Prediction scores for each node.
    labels : np.ndarray or list
        Node labels (-1 means non-labeled).
    node_names : list
        Names for each node.
    output_dir : str
        Directory to save output plot.
    args : Namespace
        Should have model_type, net_type, num_epochs for filename.
    quantiles : tuple
        Lower and upper quantile for midrange filter.
    top_n : int
        Number of top retained genes to keep.

    Returns
    -------
    retained_gene_names : list
        Names of top retained genes.
    retained_gene_indices : list
        Indices of top retained genes.
    """

    # ➤ Convert and normalize
    if hasattr(relevance_scores, "cpu"):
        relevance_scores = relevance_scores.cpu().numpy()
    relevance_sums = relevance_scores.sum(axis=1)
    relevance_sums_norm = normalize(relevance_sums.reshape(1, -1)).flatten()

    # ➤ Non-labeled mask
    non_labeled_nodes = [i for i, label in enumerate(labels) if label == -1]
    is_non_labeled = np.zeros_like(relevance_sums_norm, dtype=bool)
    is_non_labeled[non_labeled_nodes] = True

    # ➤ Quantile cutoffs
    low_q, high_q = np.quantile(relevance_sums_norm, quantiles)
    is_midrange = (relevance_sums_norm >= low_q) & (relevance_sums_norm <= high_q)

    # ➤ Rank
    non_labeled_scores = [
        (node_names[i], scores[i], i)
        for i in non_labeled_nodes if is_midrange[i]
    ]
    ranking_retained = sorted(non_labeled_scores, key=lambda x: x[1], reverse=True)
    top_retained = ranking_retained[:top_n]

    retained_gene_names = [name for name, _, _ in top_retained]
    retained_gene_indices = [idx for _, _, idx in top_retained]

    # ➤ Masks for plot
    retained_mask = is_midrange & is_non_labeled
    discarded_mask = ~is_midrange & is_non_labeled

    # ➤ Plot
    plt.figure(figsize=(10, 7))
    plt.scatter(
        relevance_sums_norm[discarded_mask],
        scores[discarded_mask],
        color='lightgray',
        alpha=0.6,
        label='Discarded Genes'
    )
    plt.scatter(
        relevance_sums_norm[retained_mask],
        scores[retained_mask],
        color='crimson',
        alpha=0.6,
        label='Retained Genes'
    )
    plt.axvline(x=low_q, color='blue', linestyle='--', label=f'{int(quantiles[0]*100)}th Percentile')
    plt.axvline(x=high_q, color='green', linestyle='--', label=f'{int(quantiles[1]*100)}th Percentile')
    plt.xlabel("Normalized LRP Relevance Sum")
    plt.ylabel("Predicted Cancer Gene Score (sigmoid)")
    plt.title("Predicted Score vs. Relevance — Retained vs. Discarded Genes")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    # ➤ Save
    plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_score_vs_relevance_epo{args.num_epochs}.png"
    )
    plt.savefig(plot_path, dpi=300)
    plt.close()

    print(f"✅ Saved Relevance–Score scatter plot → {plot_path}")

    return retained_gene_names, retained_gene_indices

def plot_bio_biclustering_clustermap(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    cluster_colors,
    omics_colors=None,
    row_labels=None,
    col_labels=None,
    gene_names=None,
    top_k=10
):
    # === Normalize
    relevance_scores = normalize(relevance_scores, axis=1)
    saliency_matrix_norm = relevance_scores

    if omics_colors is None:
        omics_colors = {
            'CNA': '#9370DB', 'GE': '#228B22', 'METH': '#00008B', 'MF': '#b22222'
        }

    # === Feature names
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:relevance_scores.shape[1]]

    # === Reorder rows/columns
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
        row_labels = row_labels[row_order]
        if gene_names is not None:
            gene_names = [gene_names[i] for i in row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        col_labels = col_labels[col_order]

    reordered_omics_labels = [f.split(":")[0].strip().upper() for f in feature_names]
    reordered_feature_labels = [f.split(":")[1].strip() for f in feature_names]

    # === Save top-k features
    if gene_names is not None:
        topk_dir = os.path.join(os.path.dirname(output_path), "topk_features_per_gene")
        os.makedirs(topk_dir, exist_ok=True)
        for i, gene in enumerate(gene_names):
            topk_indices = np.argsort(-relevance_scores[i])[:top_k]
            topk_features = [feature_names[j] for j in topk_indices]
            topk_scores = relevance_scores[i][topk_indices]
            df_gene = pd.DataFrame({"Feature": topk_features, "Score": topk_scores})
            df_gene.to_csv(os.path.join(topk_dir, f"{gene}_top{top_k}_features.csv"), index=False)

    # === Setup figure with 3 rows: omics bar, cluster bar, heatmap
    fig = plt.figure(figsize=(20, 20))
    gs = gridspec.GridSpec(2, 1, height_ratios=[0.5, 19], hspace=0.0)
    ##gs = gridspec.GridSpec(2, 1, height_ratios=[1.2, 18], hspace=0.05)


    # === Top bar: column cluster stripe
    ax_col_cluster = fig.add_subplot(gs[0])
    col_cluster_colors = [CLUSTER_COLORS.get(c, '#FFFFFF') for c in col_labels]
    col_cluster_rgb = np.array([[to_rgb(c) for c in col_cluster_colors]])
    ax_col_cluster.imshow(col_cluster_rgb, aspect='auto', extent=[0, len(col_cluster_colors), 0, 1])
    ax_col_cluster.set_xlim([0, len(col_cluster_colors)])
    ax_col_cluster.set_xticks([])
    ax_col_cluster.set_yticks([])
    ax_col_cluster.set_frame_on(False)

    # === Main heatmap
    ax = fig.add_subplot(gs[1])

    ###################################


    # === Main heatmap
    #ax = fig.add_subplot(gs[2])
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(relevance_scores, 99)

    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax
    )

    # === Row cluster color bars
    if row_labels is not None:
        for i, cluster in enumerate(row_labels):
            ax.add_patch(plt.Rectangle(
                (-1.5, i), 1.5, 1,
                linewidth=0,
                facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                clip_on=False
            ))

        unique_clusters, cluster_sizes = np.unique(row_labels, return_counts=True)
        start_idx = 0
        for cluster, size in zip(unique_clusters, cluster_sizes):
            center_y = start_idx + size / 2
            ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)#, fontweight='bold')
            start_idx += size

    # === Feature x-labels, color-coded by omics
    ax.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=24)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, omics in zip(ax.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics, 'black'))

    # === Finalize
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    fig.subplots_adjust(hspace=0.0)
    plt.close()
    print(f"[Saved] Clustermap with top omics bar → {output_path}")

def plot_biclustering_input_feature_heatmap(
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    cluster_colors: dict,
    omics_colors: dict,
    output_path: str,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):
    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    saliency_matrix_norm = normalize(saliency_matrix, axis=1)

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:saliency_matrix.shape[1]]

    best_model, best_score = None, np.inf
    valid = False

    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials with Constraint", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix_norm)
        reordered = saliency_matrix_norm[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix_norm, reordered)

        # Validate: does every column cluster contain >=4 unique cancers?
        col_labels = model.column_labels_
        col_order = np.argsort(col_labels)
        reordered_feature_labels = [feature_names[i].split(": ")[1] for i in col_order]

        from collections import defaultdict
        cluster_to_cancers = defaultdict(set)
        for idx, cluster_id in enumerate(col_labels):
            cancer = feature_names[idx].split(": ")[1]
            cluster_to_cancers[cluster_id].add(cancer)

        cluster_ok = all(len(cancers) >= 2 for cancers in cluster_to_cancers.values())

        if cluster_ok:
            if mse < best_score:
                best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Could not find a valid biclustering where all column clusters contain >=4 cancers.")

    print(f"✅ Found valid biclustering with all column clusters containing >=4 cancers.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_
    row_order = np.argsort(row_labels)
    col_order = np.argsort(col_labels)

    clustered_matrix = saliency_matrix_norm[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]
    reordered_feature_labels = [feature_names[i].split(": ")[1] for i in col_order]
    reordered_omics_labels = [feature_names[i].split(": ")[0] for i in col_order]

    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(output_path.replace(".png", "_cluster_labels.csv"), index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names, columns=reordered_feature_labels)\
        .to_csv(output_path.replace(".png", "_heatmap_matrix.csv"))
    pd.DataFrame({
        gene: [reordered_feature_labels[i] for i in np.argsort(-clustered_matrix[i])[:top_k]]
        for i, gene in enumerate(reordered_gene_names)
    }).T.rename(columns=lambda i: f"Top{i+1}").to_csv(output_path.replace(".png", f"_top{top_k}_features_per_gene.csv"))

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(clustered_matrix, 99)

    fig = plt.figure(figsize=(22, 20))
    gs = gridspec.GridSpec(2, 1, height_ratios=[0.45, 19], hspace=0.0)

    ax_col_cluster = fig.add_subplot(gs[0])
    col_cluster_colors = [cluster_colors.get(c, "#FFFFFF") for c in col_labels[col_order]]
    col_cluster_rgb = np.array([[to_rgb(c) for c in col_cluster_colors]])
    ax_col_cluster.imshow(col_cluster_rgb, aspect='auto', extent=[0, len(col_cluster_colors), 0, 1])
    ax_col_cluster.set_xlim([0, len(col_cluster_colors)])
    ax_col_cluster.set_xticks([])
    ax_col_cluster.set_yticks([])
    ax_col_cluster.set_frame_on(False)

    ax = fig.add_subplot(gs[1])
    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax
    )

    for i, cluster in enumerate(reordered_cluster_labels):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0,
                                   facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                                   clip_on=False))

    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)#, fontweight='bold')
        start += size

    ax.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=24)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, omics in zip(ax.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics.upper(), 'black'))

    plt.tight_layout()
    fig.subplots_adjust(hspace=0.0)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"[Saved] Biclustering heatmap → {output_path}")

def apply_full_spectral_biclustering_bio(
    graph, saliency_matrix, node_names_topk, omics_splits,
    predicted_cancer_genes, n_clusters_row, n_clusters_col,
    save_path, save_row_labels_path,
    save_total_genes_per_cluster_path, 
    save_predicted_counts_path,
    output_path_genes_clusters, 
    output_path_heatmap,
    output_dir,
    cluster_colors,
    args,
    topk_node_indices=None,
    n_trials=10,
    top_k=5
):
    print("🧪 Running Spectral Biclustering with FIXED clusters (10, 4)...")

    if topk_node_indices is None:
        raise ValueError("`topk_node_indices` must be provided")

    assert saliency_matrix.shape[1] == 64, f"Expected 64 summary features, got {saliency_matrix.shape[1]}"
    
    matrix = saliency_matrix

    # === Fixed cluster counts

    print(f"→ Using fixed n_clusters = ({n_clusters_row}, {n_clusters_col})")

    # === Run trials
    best_model = None
    best_score = np.inf
    print("🔁 Running biclustering trials:")
    for i in range(n_trials):
        model = SpectralBiclustering(
            n_clusters=(n_clusters_row, n_clusters_col),
            method='bistochastic',
            svd_method='randomized',
            random_state=i
        )
        model.fit(matrix)
        reconstructed = matrix[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(matrix, reconstructed)
        if mse < best_score:
            best_score = mse
            best_model = model

    # === Labels
    bicluster = best_model
    row_labels = bicluster.row_labels_
    col_labels = bicluster.column_labels_

    # === Assign to graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    print("✅ Spectral Biclustering complete.")

    # === Save metadata
    save_graph_with_clusters(graph, save_path)
    save_row_labels(row_labels, save_row_labels_path)
    total_genes_per_cluster = compute_total_genes_per_cluster(row_labels, n_clusters_row)
    save_total_genes_per_cluster(total_genes_per_cluster, save_total_genes_per_cluster_path)

    pred_counts, predicted_indices = count_predicted_genes_per_cluster(
        row_labels, node_names_topk, predicted_cancer_genes, n_clusters_row
    )
    save_predicted_counts(pred_counts, save_predicted_counts_path)

    # === Plots
    plot_bio_biclustering_heatmap_unsort(
        args=args,
        relevance_scores=matrix,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "heatmap_unsort.png"),
        row_labels=row_labels,
        col_labels=col_labels
    )

    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=matrix,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "spectral_biclustering_bio_fixed_clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels,
        cluster_colors=cluster_colors
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    # === Assignments
    gene_assignment_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_summary_gene_cluster_assignments_epo{args.num_epochs}.csv"
    )

    assignment_rows = []
    for i, (idx, cluster) in enumerate(zip(topk_node_indices, row_labels)):
        gene_name = node_names_topk[i] if i < len(node_names_topk) else f"node_{idx}"
        rel_score = matrix[i].sum().item()
        assignment_rows.append([gene_name, idx, cluster, round(rel_score, 5)])
    assignment_rows.sort(key=lambda x: (x[2], -x[3]))
    with open(gene_assignment_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Gene_Name", "Index", "Cluster", "Relevance_Score"])
        writer.writerows(assignment_rows)

    print(f"📄 Saved gene–cluster assignment: {gene_assignment_path}")

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_confirmed_neighbors_bio(
    args,
    graph,
    node_names,
    name_to_index,
    predicted_cancer_genes,
    node_names_topk,  # used to build index
    row_labels,
    total_clusters,
    output_dir,
    relevance_scores,
    top_k=1000
):
    # ✅ Correct top-k index
    topk_name_to_index = {name: i for i, name in enumerate(node_names_topk)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}

    # Always use row_labels for cluster source — graph only for fallback
    # Sort predicted genes by summed saliency
    top_scored_genes = sorted(
        [g for g in predicted_cancer_genes if g in topk_name_to_index],
        key=lambda g: relevance_scores[topk_name_to_index[g]].sum(),
        reverse=True
    )

    print(f"✔️ Number of predicted genes in top-k: {len(top_scored_genes)}")

    # Get neighbors
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, top_scored_genes)

    all_neighbor_rows = []

    for gene in top_scored_genes:
        if gene not in topk_name_to_index:
            print(f"⚠️ Skipping {gene} not in top-k index")
            continue

        rel_idx = topk_name_to_index[gene]
        gene_score = relevance_scores[rel_idx].sum().item()
        gene_cluster = int(row_labels[rel_idx])  # ✅ Use row_labels
        print(f"{gene} → RelIdx {rel_idx} | Bio score: {gene_score:.4f} | Cluster: {gene_cluster}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_scores = {}

        for neighbor in neighbors:
            if neighbor == gene:
                continue
            if neighbor not in topk_name_to_index:
                continue  # ✅ strict: skip if not top-k
            neighbor_idx = topk_name_to_index[neighbor]
            if neighbor_idx >= relevance_scores.shape[0]:
                continue

            score = relevance_scores[neighbor_idx].sum().item()
            neighbor_scores[neighbor_idx] = score

        if not neighbor_scores:
            print(f"⚠️ No valid top-k neighbors for {gene}.")
            continue

        # ✅ Store ALL valid neighbors for CSV
        all_neighbors = neighbor_scores

        # ✅ Pick TOP 10 for plotting
        top_neighbors = dict(sorted(neighbor_scores.items(), key=lambda x: -x[1])[:10])

        # === SAVE ALL neighbors to CSV
        for neighbor_idx, score in all_neighbors.items():
            neighbor_name = node_names_topk[neighbor_idx]
            neighbor_cluster = int(row_labels[neighbor_idx])  # ✅ Use row_labels for neighbor too
            if neighbor_cluster == -1:
                continue

            all_neighbor_rows.append([
                gene,
                neighbor_name,
                str(neighbor_idx),
                round(score, 5),
                neighbor_cluster
            ])

        # === PLOT ONLY TOP 10 neighbors
        plot_path = os.path.join(
            "results/gene_prediction/bio_neighbor_feature_contributions/",
            f"{args.model_type}_{args.net_type}_{gene}_bio_confirmed_neighbor_relevance_epo{args.num_epochs}.png"
        )

        os.makedirs(os.path.dirname(plot_path), exist_ok=True)

        topk_node_id_to_name = {i: name for i, name in enumerate(node_names_topk)}

        plot_neighbor_relevance(
            neighbor_scores=top_neighbors,
            gene_name=f"{gene} (Cluster {gene_cluster})",
            node_id_to_name=topk_node_id_to_name,
            output_path=plot_path,
            row_labels=row_labels,
            total_clusters=total_clusters,
            add_legend=False
        )

    os.makedirs(output_dir, exist_ok=True)
    combined_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_top{top_k}_confirmed_neighbors_epo{args.num_epochs}.csv"
    )
    with open(combined_csv_path, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Source_Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Cluster"])
        writer.writerows(all_neighbor_rows)

    print(f"📄 Confirmed neighbor CSV saved: {combined_csv_path}")

def plot_biclustering_input_feature_heatmap(
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    cluster_colors: dict,
    omics_colors: dict,
    output_path: str,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):
    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    saliency_matrix_norm = normalize(saliency_matrix, axis=1)

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:saliency_matrix.shape[1]]

    best_model, best_score = None, np.inf
    valid = False

    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials with Constraint", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix_norm)
        reordered = saliency_matrix_norm[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix_norm, reordered)

        # Validate: does every column cluster contain >=4 unique cancers?
        col_labels = model.column_labels_
        col_order = np.argsort(col_labels)
        reordered_feature_labels = [feature_names[i].split(": ")[1] for i in col_order]

        from collections import defaultdict
        cluster_to_cancers = defaultdict(set)
        for idx, cluster_id in enumerate(col_labels):
            cancer = feature_names[idx].split(": ")[1]
            cluster_to_cancers[cluster_id].add(cancer)

        cluster_ok = all(len(cancers) >= 2 for cancers in cluster_to_cancers.values())

        if cluster_ok:
            if mse < best_score:
                best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Could not find a valid biclustering where all column clusters contain >=4 cancers.")

    print(f"✅ Found valid biclustering with all column clusters containing >=4 cancers.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_
    ######row_order = np.argsort(row_labels)
    # Sort genes within each cluster by total saliency
    cluster_to_indices = defaultdict(list)
    for idx, label in enumerate(model.row_labels_):
        cluster_to_indices[label].append(idx)

    sorted_row_indices = []
    sorted_cluster_labels = []

    for cluster_id in sorted(cluster_to_indices.keys()):  # ensure consistent order
        cluster_indices = cluster_to_indices[cluster_id]
        # Sort by row sum (or other metric)
        cluster_saliencies = saliency_matrix_norm[cluster_indices]
        intra_order = np.argsort(-cluster_saliencies.sum(axis=1))  # descending
        sorted_cluster = [cluster_indices[i] for i in intra_order]
        sorted_row_indices.extend(sorted_cluster)
        sorted_cluster_labels.extend([cluster_id] * len(sorted_cluster))

    row_order = sorted_row_indices
    reordered_cluster_labels = np.array(sorted_cluster_labels)

    col_order = np.argsort(col_labels)############################

    clustered_matrix = saliency_matrix_norm[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]
    reordered_feature_labels = [feature_names[i].split(": ")[1] for i in col_order]
    reordered_omics_labels = [feature_names[i].split(": ")[0] for i in col_order]

    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(output_path.replace(".png", "_cluster_labels.csv"), index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names, columns=reordered_feature_labels)\
        .to_csv(output_path.replace(".png", "_heatmap_matrix.csv"))
    pd.DataFrame({
        gene: [reordered_feature_labels[i] for i in np.argsort(-clustered_matrix[i])[:top_k]]
        for i, gene in enumerate(reordered_gene_names)
    }).T.rename(columns=lambda i: f"Top{i+1}").to_csv(output_path.replace(".png", f"_top{top_k}_features_per_gene.csv"))

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(clustered_matrix, 99)

    fig = plt.figure(figsize=(22, 20))
    gs = gridspec.GridSpec(2, 1, height_ratios=[0.4, 19], hspace=0.0)

    ax_col_cluster = fig.add_subplot(gs[0])
    col_cluster_colors = [cluster_colors.get(c, "#FFFFFF") for c in col_labels[col_order]]
    col_cluster_rgb = np.array([[to_rgb(c) for c in col_cluster_colors]])
    ax_col_cluster.imshow(col_cluster_rgb, aspect='auto', extent=[0, len(col_cluster_colors), 0, 1])
    ax_col_cluster.set_xlim([0, len(col_cluster_colors)])
    ax_col_cluster.set_xticks([])
    ax_col_cluster.set_yticks([])
    ax_col_cluster.set_frame_on(False)

    ax = fig.add_subplot(gs[1])
    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax
    )

    for i, cluster in enumerate(reordered_cluster_labels):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0,
                                   facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                                   clip_on=False))

    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)#, fontweight='bold')
        start += size

    ax.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=24)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, omics in zip(ax.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics.upper(), 'black'))

    plt.tight_layout()
    fig.subplots_adjust(hspace=0.0)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"[Saved] Biclustering heatmap → {output_path}")

def plot_bio_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    omics_splits,
    output_path,
    omics_colors=None,
    gene_names=None,
    col_labels=None
):
    # Normalize relevance scores
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min()) * 10

    if omics_colors is None:
        omics_colors = {
            'cna': '#9370DB',
            'ge': '#228B22',
            'meth': '#00008B',
            'mf': '#b22222',
        }

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['cna', 'ge', 'meth', 'mf']
    feature_names = [f"{omics.upper()}: {cancer}" for omics in omics_order for cancer in cancer_names]

    # Column sorting (omics and per-feature)
    feature_avgs = relevance_scores.mean(axis=0)
    omics_group_means = {}
    for omics in omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_mean = feature_avgs[group_indices].mean()
        omics_group_means[omics] = group_mean
    sorted_omics_order = sorted(omics_order, key=lambda x: omics_group_means[x], reverse=True)

    sorted_col_indices = []
    sorted_feature_names = []
    sorted_feature_colors = []
    new_omics_splits = {}
    col_cursor = 0

    for omics in sorted_omics_order:
        start, end = omics_splits[omics]
        group_indices = list(range(start, end + 1))
        group_avgs = feature_avgs[group_indices]
        group_sorted = [i for _, i in sorted(zip(group_avgs, group_indices), reverse=True)]

        new_omics_splits[omics] = (col_cursor, col_cursor + len(group_sorted) - 1)
        col_cursor += len(group_sorted)

        sorted_col_indices.extend(group_sorted)
        sorted_feature_names.extend([feature_names[i] for i in group_sorted])
        sorted_feature_colors.extend([omics_colors[omics]] * len(group_sorted))

    relevance_scores = relevance_scores[:, sorted_col_indices]
    feature_names = sorted_feature_names
    feature_colors = sorted_feature_colors

    # Row sorting: by cluster, then by saliency within cluster
    cluster_ids = np.unique(row_labels)
    ordered_row_indices = []
    for cluster_id in np.sort(cluster_ids):  # ⬅ flip cluster order
    ##for cluster_id in np.sort(cluster_ids):
        cluster_mask = (row_labels == cluster_id)
        cluster_scores = relevance_scores[cluster_mask]
        saliency_sums = cluster_scores.sum(axis=1)
        intra_cluster_order = np.argsort(-saliency_sums)
        cluster_indices = np.where(cluster_mask)[0][intra_cluster_order]
        ordered_row_indices.extend(cluster_indices)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]

    # Plotting
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(sorted_scores, 99)



    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")



    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)
    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])

    # Top bar
    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min() + 1e-6)
    ax_bar.axis("off")
    ax_bar.set_xlim(0, len(feature_names))
    ax_bar.set_ylim(0, 1.6)

    for i, (val, color) in enumerate(zip(feature_means, feature_colors)):
        ax_bar.bar(
            x=i + 0.5, 
            height=val, 
            width=1.0,
            color=color, 
            edgecolor='black', 
            linewidth=0.5, 
            alpha=0.3 + 0.7 * val
        )

    # Heatmap
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={"label": "Relevance Score", "shrink": 0.1, "aspect": 12, "pad": 0.02, "orientation": "vertical", "location": "right"},
        ax=ax
    )
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=18)
    ax_cbar.yaxis.label.set_size(18)

    # Cluster stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')), clip_on=False))

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(sorted_clusters, return_counts=True)
    start_idx = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start_idx + size / 2
        ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=18, fontweight='bold')
        start_idx += size

    # X-axis labels
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels([f.split(": ")[1] for f in feature_names], rotation=90, fontsize=14)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, color in zip(ax.get_xticklabels(), feature_colors):
        label.set_color(color)

    # Saliency curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.fill_betweenx(
        np.arange(len(saliency_sums)), 
        0, 
        saliency_sums, 
        color='#a9cce3', 
        alpha=0.8, 
        linewidth=3)

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.hlines(y=1.01, xmin=0, xmax=1, color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform())
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')

    # Omics group bars
    for omics in sorted_omics_order:
        start, end = new_omics_splits[omics]
        group_center = (start + end) / 2 + 0.5
        mean_val = sorted_scores[:, start:end+1].mean()
        norm_mean = (mean_val - np.min(feature_means)) / (np.max(feature_means) - np.min(feature_means) + 1e-6)
        ax.bar(
            x=group_center,
            height=0.15,
            width=end - start + 1,
            bottom=len(sorted_scores) + 1.5,
            color=omics_colors[omics],
            edgecolor='black',
            linewidth=1,
            alpha=min(1.0, 0.3 + 0.7 * norm_mean)
        )

    fig.tight_layout()
    fig.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    # 🔹 Optional: Cluster-wise contributions
    plot_bio_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,
        row_labels=row_labels,
        feature_names=feature_names,
        per_cluster_feature_contributions_output_dir=os.path.join(os.path.dirname(output_path), "per_cluster_feature_contributions_bio"),
        omics_colors=omics_colors
    )

def plot_biclustering_input_feature_heatmap_bio_sorted_pa(
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    cluster_colors: dict,
    omics_colors: dict,
    output_path: str,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):
    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    ##saliency_matrix_norm = normalize(saliency_matrix, axis=1)
    saliency_matrix = (saliency_matrix - saliency_matrix.min()) / (saliency_matrix.max() - saliency_matrix.min()) * 10

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:saliency_matrix.shape[1]]

    best_model, best_score = None, np.inf
    valid = False

    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials with Constraint", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix)
        reordered = saliency_matrix[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix, reordered)

        col_labels = model.column_labels_
        cluster_to_cancers = defaultdict(set)
        for idx, cluster_id in enumerate(col_labels):
            cancer = feature_names[idx].split(": ")[1]
            cluster_to_cancers[cluster_id].add(cancer)

        cluster_ok = all(len(cancers) >= 2 for cancers in cluster_to_cancers.values())

        if cluster_ok and mse < best_score:
            best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Could not find a valid biclustering where all column clusters contain ≥ 2 cancers.")

    print(f"✅ Found valid biclustering with all column clusters containing ≥ 2 cancers.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_

    # Sort rows within clusters
    row_order = []
    for cid in np.unique(row_labels):
        cluster_idx = np.where(row_labels == cid)[0]
        row_sums = saliency_matrix[cluster_idx].sum(axis=1)
        sorted_idx = cluster_idx[np.argsort(-row_sums)]
        row_order.extend(sorted_idx)

    # Sort columns by omics group and within each group by average saliency
    feature_avgs = saliency_matrix.mean(axis=0)
    omics_grouped_indices = {omics: [] for omics in omics_order}
    for i, fname in enumerate(feature_names):
        omics = fname.split(":")[0].strip().upper()
        if omics in omics_grouped_indices:
            omics_grouped_indices[omics].append(i)

    col_order = []
    reordered_feature_labels = []
    reordered_omics_labels = []
    feature_colors = []

    for omics in omics_order:
        indices = omics_grouped_indices[omics]
        sorted_within = [i for _, i in sorted(zip(feature_avgs[indices], indices), reverse=True)]
        col_order.extend(sorted_within)
        reordered_feature_labels.extend([feature_names[i].split(": ")[1] for i in sorted_within])
        reordered_omics_labels.extend([feature_names[i].split(": ")[0] for i in sorted_within])
        feature_colors.extend([omics_colors.get(omics.upper(), 'gray')] * len(sorted_within))

    clustered_matrix = saliency_matrix[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]

    base_path = output_path.replace(".png", "_with_curve")
    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(base_path + "_cluster_labels.csv", index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names, columns=reordered_feature_labels)\
        .to_csv(base_path + "_heatmap_matrix.csv")
    pd.DataFrame({
        gene: [reordered_feature_labels[i] for i in np.argsort(-clustered_matrix[i])[:top_k]]
        for i, gene in enumerate(reordered_gene_names)
    }).T.rename(columns=lambda i: f"Top{i+1}")\
     .to_csv(base_path + f"_top{top_k}_features_per_gene.csv")

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(clustered_matrix, 99)

    fig = plt.figure(figsize=(22, 20))
    gs = gridspec.GridSpec(2, 1, height_ratios=[1.2, 18], hspace=0.0)
    ax_col_cluster = fig.add_subplot(gs[0])

    # Omics color bar
    '''omics_colors_bar = np.array([[to_rgb(omics_colors.get(o.upper(), "#000000")) for o in reordered_omics_labels]])
    ax_omics_bar = ax_col_cluster.inset_axes([0, 1.05, 1, 0.15])
    ax_omics_bar.imshow(omics_colors_bar, aspect='auto')
    ax_omics_bar.set_xticks([])
    ax_omics_bar.set_yticks([])
    ax_omics_bar.set_frame_on(False)'''

    # Top bar
    ax_top_bar = ax_col_cluster.inset_axes([0.0, 1.25, 1.0, 0.8])
    feature_means = saliency_matrix[:, col_order].mean(axis=0)
    feature_means_norm = feature_means#normalize(feature_means, axis=1)
    ax_top_bar.bar(
        x=np.arange(len(feature_means_norm)),
        height=feature_means_norm,
        width=1.0,
        color=feature_colors,
        edgecolor='black',
        linewidth=0.3
    )
    ax_top_bar.set_xlim(0, len(feature_means_norm))
    ax_top_bar.set_ylim(0, 1.05)
    ax_top_bar.axis('off')

    # Heatmap and saliency curve
    gs_lower = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=gs[1], width_ratios=[20, 1], wspace=0.05)
    ax_heatmap = fig.add_subplot(gs_lower[0])
    ax_curve = fig.add_subplot(gs_lower[1], sharey=ax_heatmap)

    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax_heatmap
    )

    for i, cluster in enumerate(reordered_cluster_labels):
        ax_heatmap.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0,
                                           facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                                           clip_on=False))

    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax_heatmap.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)
        start += size

    ax_heatmap.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax_heatmap.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=24)
    ax_heatmap.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, omics in zip(ax_heatmap.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics.upper(), 'black'))

    saliency_sums = clustered_matrix.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())

    ax_curve.set_xlim([0, 1])
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=18)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.invert_yaxis()
    ax_curve.axis("off")

    current_idx = 0
    for cluster_id, cluster_size in zip(unique_clusters, cluster_sizes):
        y_vals = np.arange(current_idx, current_idx + cluster_size)
        cluster_saliency = saliency_sums[current_idx: current_idx + cluster_size]
        cluster_color = cluster_colors.get(cluster_id, '#CCCCCC')
        ax_curve.fill_betweenx(
            y_vals,
            0,
            cluster_saliency,
            color=cluster_color,
            alpha=0.9,
            linewidth=0
        )
        current_idx += cluster_size

    fig.tight_layout()
    fig.subplots_adjust(hspace=0.0)
    out_fig_path = base_path + ".png"
    plt.savefig(out_fig_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"[Saved] Clustered biclustering heatmap with saliency curve → {out_fig_path}")

def plot_biclustering_input_feature_heatmap_bio_sorted_pas(
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    cluster_colors: dict,
    omics_colors: dict,
    output_path: str,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):
    import os
    import numpy as np
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    from matplotlib import gridspec
    from matplotlib.colors import LinearSegmentedColormap, to_rgba
    from sklearn.cluster import SpectralBiclustering
    from sklearn.metrics import mean_squared_error
    from collections import defaultdict
    from tqdm import tqdm

    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    saliency_matrix = (saliency_matrix - saliency_matrix.min()) / (saliency_matrix.max() - saliency_matrix.min()) * 10

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:saliency_matrix.shape[1]]

    best_model, best_score = None, np.inf
    valid = False

    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials with Constraint", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix)
        reordered = saliency_matrix[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix, reordered)

        col_labels = model.column_labels_
        cluster_to_cancers = defaultdict(set)
        for idx, cluster_id in enumerate(col_labels):
            cancer = feature_names[idx].split(": ")[1]
            cluster_to_cancers[cluster_id].add(cancer)

        cluster_ok = all(len(cancers) >= 2 for cancers in cluster_to_cancers.values())

        if cluster_ok and mse < best_score:
            best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Could not find a valid biclustering where all column clusters contain ≥ 2 cancers.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_

    # Sort rows by cluster and total saliency
    row_order = []
    for cid in np.unique(row_labels):
        cluster_idx = np.where(row_labels == cid)[0]
        row_sums = saliency_matrix[cluster_idx].sum(axis=1)
        sorted_idx = cluster_idx[np.argsort(-row_sums)]
        row_order.extend(sorted_idx)

    # Sort columns by omics then feature importance
    feature_avgs = saliency_matrix.mean(axis=0)
    omics_grouped_indices = {omics: [] for omics in omics_order}
    for i, fname in enumerate(feature_names):
        omics = fname.split(":")[0].strip().upper()
        if omics in omics_grouped_indices:
            omics_grouped_indices[omics].append(i)

    col_order = []
    reordered_feature_labels, reordered_omics_labels, feature_colors = [], [], []
    for omics in omics_order:
        indices = omics_grouped_indices[omics]
        sorted_within = [i for _, i in sorted(zip(feature_avgs[indices], indices), reverse=True)]
        col_order.extend(sorted_within)
        reordered_feature_labels.extend([feature_names[i].split(": ")[1] for i in sorted_within])
        reordered_omics_labels.extend([feature_names[i].split(": ")[0] for i in sorted_within])
        feature_colors.extend([omics_colors.get(omics.upper(), 'gray')] * len(sorted_within))

    clustered_matrix = saliency_matrix[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]

    base_path = output_path.replace(".png", "_with_curve")
    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(base_path + "_cluster_labels.csv", index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names, columns=reordered_feature_labels)\
        .to_csv(base_path + "_heatmap_matrix.csv")
    pd.DataFrame({
        gene: [reordered_feature_labels[i] for i in np.argsort(-clustered_matrix[i])[:top_k]]
        for i, gene in enumerate(reordered_gene_names)
    }).T.rename(columns=lambda i: f"Top{i+1}")\
     .to_csv(base_path + f"_top{top_k}_features_per_gene.csv")

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(clustered_matrix, 99)

    fig = plt.figure(figsize=(22, 20))

    # Top bar for feature saliency
    gs = gridspec.GridSpec(3, 1, height_ratios=[1.0, 0.0, 20], hspace=0.0)
    ax_top_bar = fig.add_subplot(gs[0])
    
    ax_top_bar.bar(
        x=np.arange(len(feature_avgs[col_order])),
        height=feature_avgs[col_order],
        width=1.0,
        color=feature_colors,
        edgecolor='black',
        linewidth=0.3
    )
    ax_top_bar.set_xlim(0, len(feature_avgs[col_order]))
    ax_top_bar.set_ylim(0, 1.05)
    ax_top_bar.axis('off')

    # Heatmap + saliency curve
    gs_lower = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=gs[2], width_ratios=[20, 1], wspace=0.0)
    ax_heatmap = fig.add_subplot(gs_lower[0])
    ax_curve = fig.add_subplot(gs_lower[1], sharey=ax_heatmap)

    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax_heatmap
    )

    for i, cluster in enumerate(reordered_cluster_labels):
        ax_heatmap.add_patch(plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0,
                                           facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                                           clip_on=False))

    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax_heatmap.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)
        start += size

    ax_heatmap.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax_heatmap.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=22)
    ax_heatmap.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, omics in zip(ax_heatmap.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics.upper(), 'black'))

    saliency_sums = clustered_matrix.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())

    ax_curve.set_xlim([0, 1])
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=18)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.invert_yaxis()
    ax_curve.axis("off")

    current_idx = 0
    for cluster_id, cluster_size in zip(unique_clusters, cluster_sizes):
        y_vals = np.arange(current_idx, current_idx + cluster_size)
        cluster_saliency = saliency_sums[current_idx: current_idx + cluster_size]
        cluster_color = cluster_colors.get(cluster_id, '#CCCCCC')
        ax_curve.fill_betweenx(
            y_vals,
            0,
            cluster_saliency,
            color=cluster_color,
            alpha=0.9,
            linewidth=0
        )
        current_idx += cluster_size

    fig.tight_layout()
    out_fig_path = base_path + ".png"
    ##plt.savefig(out_fig_path, dpi=300, bbox_inches='tight')
    plt.savefig(out_fig_path, dpi=300, bbox_inches='tight', pad_inches=0.1)

    plt.close()
    print(f"[Saved] Clustered biclustering heatmap with saliency curve → {out_fig_path}")

def plot_biclustering_input_feature_heatmap_bio_sorted(
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    cluster_colors: dict,
    omics_colors: dict,
    output_path: str,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):

    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # saliency_matrix = (saliency_matrix - saliency_matrix.min()) / (saliency_matrix.max() - saliency_matrix.min()) * 5
    saliency_matrix_norm = normalize(saliency_matrix, axis=1)

    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:saliency_matrix.shape[1]]

    best_model, best_score = None, np.inf
    valid = False

    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials with Constraint", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix)
        reordered = saliency_matrix[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix, reordered)

        col_labels = model.column_labels_
        cluster_to_cancers = defaultdict(set)
        for idx, cluster_id in enumerate(col_labels):
            cancer = feature_names[idx].split(": ")[1]
            cluster_to_cancers[cluster_id].add(cancer)

        cluster_ok = all(len(cancers) >= 2 for cancers in cluster_to_cancers.values())

        if cluster_ok and mse < best_score:
            best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Could not find a valid biclustering where all column clusters contain ≥ 2 cancers.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_

    # Sort rows by cluster and total saliency
    row_order = []
    for cid in np.unique(row_labels):
        cluster_idx = np.where(row_labels == cid)[0]
        row_sums = saliency_matrix[cluster_idx].sum(axis=1)
        sorted_idx = cluster_idx[np.argsort(-row_sums)]
        row_order.extend(sorted_idx)

    # Sort columns by omics then feature importance
    feature_avgs = saliency_matrix.mean(axis=0)
    omics_grouped_indices = {omics: [] for omics in omics_order}
    for i, fname in enumerate(feature_names):
        omics = fname.split(":")[0].strip().upper()
        if omics in omics_grouped_indices:
            omics_grouped_indices[omics].append(i)

    col_order = []
    reordered_feature_labels, reordered_omics_labels, feature_colors = [], [], []
    for omics in omics_order:
        indices = omics_grouped_indices[omics]
        sorted_within = [i for _, i in sorted(zip(feature_avgs[indices], indices), reverse=True)]
        col_order.extend(sorted_within)
        reordered_feature_labels.extend([feature_names[i].split(": ")[1] for i in sorted_within])
        reordered_omics_labels.extend([feature_names[i].split(": ")[0] for i in sorted_within])
        feature_colors.extend([omics_colors.get(omics.upper(), 'gray')] * len(sorted_within))

    clustered_matrix = saliency_matrix[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]

    base_path = output_path.replace(".png", "_with_curve")
    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(base_path + "_cluster_labels.csv", index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names, columns=reordered_feature_labels)\
        .to_csv(base_path + "_heatmap_matrix.csv")
    pd.DataFrame({
        gene: [reordered_feature_labels[i] for i in np.argsort(-clustered_matrix[i])[:top_k]]
        for i, gene in enumerate(reordered_gene_names)
    }).T.rename(columns=lambda i: f"Top{i+1}")\
     .to_csv(base_path + f"_top{top_k}_features_per_gene.csv")

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(clustered_matrix, 99)

    # === Final fixed figure ===
    fig = plt.figure(figsize=(22, 18))

    # Unified GridSpec
    gs = gridspec.GridSpec(3, 1, height_ratios=[1.0, 0.1, 20], hspace=0.0)

    # Top bar + lower: same [heatmap | curve]
    gs_top = gridspec.GridSpecFromSubplotSpec(
        1, 2, subplot_spec=gs[0], width_ratios=[20, 1], wspace=0.0
    )
    gs_lower = gridspec.GridSpecFromSubplotSpec(
        1, 2, subplot_spec=gs[2], width_ratios=[20, 1], wspace=0.0
    )

    # Axes
    ax_top_bar = fig.add_subplot(gs_top[0])
    ax_heatmap = fig.add_subplot(gs_lower[0])
    ax_curve = fig.add_subplot(gs_lower[1], sharey=ax_heatmap)

    # === Top feature bar ===
    ax_top_bar.bar(
        x=np.arange(len(feature_avgs[col_order])) + 0.5,
        height=feature_avgs[col_order],
        #width=1.0,
        width=0.5,
        color=feature_colors,
        edgecolor='black',
        linewidth=0.3
    )
    ax_top_bar.set_xlim(0, len(feature_avgs[col_order]))
    ax_top_bar.set_ylim(0, 1.05)
    ax_top_bar.set_xticks([])
    ax_top_bar.set_yticks([])
    for spine in ax_top_bar.spines.values():
        spine.set_visible(False)

    # === Heatmap ===
    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax_heatmap
    )

    # Add row cluster stripe
    for i, cluster in enumerate(reordered_cluster_labels):
        ax_heatmap.add_patch(
            plt.Rectangle(
                (-1.5, i), 1.5, 1,
                linewidth=0,
                facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                clip_on=False
            )
        )

    # Cluster size labels
    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax_heatmap.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)
        start += size

    # X-axis feature tick labels
    ax_heatmap.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax_heatmap.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=22)
    ax_heatmap.tick_params(axis='x', which='both', bottom=True, top=False, length=5, pad=1)
    for label, omics in zip(ax_heatmap.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics.upper(), 'black'))

    # === Saliency curve ===
    saliency_sums = clustered_matrix.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())

    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.xaxis.set_label_position('top')
    ax_curve.set_xlim([0, 1])
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=18)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.invert_yaxis()

    # Remove all spines
    for spine in ax_curve.spines.values():
        spine.set_visible(False)

    # Remove y ticks
    ax_curve.tick_params(axis='y', left=False, labelleft=False)

    # Fill curve per cluster
    current_idx = 0
    for cluster_id, cluster_size in zip(unique_clusters, cluster_sizes):
        y_vals = np.arange(current_idx, current_idx + cluster_size)
        cluster_saliency = saliency_sums[current_idx: current_idx + cluster_size]
        cluster_color = cluster_colors.get(cluster_id, '#CCCCCC')
        ax_curve.fill_betweenx(
            y_vals,
            0,
            cluster_saliency,
            color=cluster_color,
            alpha=0.9,
            linewidth=0
        )
        current_idx += cluster_size

    fig.subplots_adjust(top=0.98, bottom=0.02, hspace=0.0)
    out_fig_path = base_path + ".png"
    plt.savefig(out_fig_path, dpi=300, bbox_inches='tight', pad_inches=0.1)
    plt.close()
    print(f"[Saved] Clustered biclustering heatmap with saliency curve → {out_fig_path}")
    
    # Calculate total counts per cluster
    bio_total_counts = {cid: np.sum(row_labels == cid) for cid in np.unique(row_labels)}

    # Example: count how many genes in each cluster have sum saliency above threshold
    saliency_sums = clustered_matrix.sum(axis=1)
    threshold = np.percentile(saliency_sums, 80)  # top 20% as example
    bio_pred_counts = {}
    for cid in np.unique(row_labels):
        idx = np.where(row_labels == cid)[0]
        passing = np.sum(saliency_sums[idx] > threshold)
        bio_pred_counts[cid] = passing

    # Return the objects you want
    return clustered_matrix, row_labels, col_labels, bio_total_counts, bio_pred_counts

def apply_biclustering_with_heatmap_plot_bio(
    graph,
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    topk_node_indices,
    predicted_cancer_genes: list,
    cluster_colors: dict,
    omics_colors: dict,
    omics_splits,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):
    """
    Full biclustering, graph annotation, saving, and heatmap plot.
    Mirrors `apply_full_spectral_biclustering_bio` but with heatmap figure.
    """
    os.makedirs(output_dir, exist_ok=True)
    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"

    # === Normalize
    ##saliency_matrix_norm = (saliency_matrix - saliency_matrix.min()) / (saliency_matrix.max() - saliency_matrix.min()) * 10
    ## Darker plot $##################################################################################
    saliency_matrix_norm = normalize(saliency_matrix, axis=1)
    
    # === Plot unsorted (raw) heatmap with input-style layout
    unsorted_output_path = os.path.join(output_dir, "bio_biclustering_unsorted_input_style_heatmap.png")

    ##saliency_matrix_norm = normalize(saliency_matrix, axis=1)
    vmin, vmax = 0, np.percentile(saliency_matrix_norm, 99)

    # Reuse or define omics + feature labels again for consistency
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:saliency_matrix.shape[1]]
    reordered_feature_labels = [f.split(": ")[1] for f in feature_names]
    reordered_omics_labels = [f.split(": ")[0] for f in feature_names]

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    fig = plt.figure(figsize=(22, 18))
    gs = gridspec.GridSpec(2, 1, height_ratios=[0.4, 19], hspace=0.0)

    # Dummy top bar (white, just for layout alignment)
    ax_col_dummy = fig.add_subplot(gs[0])
    ax_col_dummy.set_xlim([0, saliency_matrix.shape[1]])
    ax_col_dummy.set_xticks([])
    ax_col_dummy.set_yticks([])
    ax_col_dummy.set_frame_on(False)
    ax_col_dummy.imshow(
        np.ones((1, saliency_matrix.shape[1], 3)),
        extent=[0, saliency_matrix.shape[1], 0, 1],
        aspect='auto'
    )

    # Raw heatmap
    ax_raw = fig.add_subplot(gs[1])
    sns.heatmap(
        saliency_matrix_norm,
        cmap=cmap,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax_raw
    )

    # X-axis labels (cancer types) and color by omics
    ax_raw.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax_raw.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=24)
    ax_raw.tick_params(axis='x', which='both', bottom=True, top=False, length=5)

    for label, omics in zip(ax_raw.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics.upper(), 'black'))

    plt.tight_layout()
    fig.subplots_adjust(hspace=0.0)
    plt.savefig(unsorted_output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"[Saved] Unsorted input-style heatmap → {unsorted_output_path}")

    # === Feature labels
    # cancer_names = [
    #     'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
    #     'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    # ]
    # omics_order = ['CNA', 'GE', 'METH', 'MF']
    # feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    # feature_names = feature_names[:saliency_matrix.shape[1]]
    
    saliency_matrix = normalize(saliency_matrix, axis=1)

    # === Run biclustering trials
    best_model, best_score = None, np.inf
    valid = False
    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix)
        reordered = saliency_matrix[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix, reordered)

        col_labels = model.column_labels_
        cluster_to_cancers = defaultdict(set)
        for idx, cid in enumerate(col_labels):
            cancer = feature_names[idx].split(": ")[1]
            cluster_to_cancers[cid].add(cancer)

        if all(len(cancers) >= 2 for cancers in cluster_to_cancers.values()) and mse < best_score:
            best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Valid biclustering not found.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_

    # === Sort
    row_order = []
    for cid in np.unique(row_labels):
        cluster_idx = np.where(row_labels == cid)[0]
        row_sums = saliency_matrix[cluster_idx].sum(axis=1)
        sorted_idx = cluster_idx[np.argsort(-row_sums)]
        row_order.extend(sorted_idx)

    feature_avgs = saliency_matrix.mean(axis=0)
    
    # === Build group → indices mapping
    omics_groups = {}
    for i, fname in enumerate(feature_names):
        omics = fname.split(":")[0].strip().upper()
        if omics not in omics_groups:
            omics_groups[omics] = []
        omics_groups[omics].append(i)

    # === Compute mean per omics group
    omics_means = {}
    for omics, indices in omics_groups.items():
        omics_means[omics] = feature_avgs[indices].mean()

    # === Sort omics groups descending by mean
    sorted_omics = [o for o, _ in sorted(omics_means.items(), key=lambda x: -x[1])]

    print(f"🔍 Omics groups sorted by mean: {sorted_omics}")

    # === Now build final col_order
    col_order = []
    reordered_feature_labels, reordered_omics_labels, feature_colors = [], [], []
    for omics in sorted_omics:
        indices = omics_groups[omics]
        sorted_within = [i for _, i in sorted(zip(feature_avgs[indices], indices), reverse=True)]
        col_order.extend(sorted_within)
        reordered_feature_labels.extend([feature_names[i].split(": ")[1] for i in sorted_within])
        reordered_omics_labels.extend([feature_names[i].split(": ")[0] for i in sorted_within])
        feature_colors.extend([omics_colors.get(omics.upper(), 'gray')] * len(sorted_within))


    clustered_matrix = saliency_matrix[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]

    # === Save output tables
    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(os.path.join(output_dir, "bio_biclustering_row_labels.csv"), index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names, columns=reordered_feature_labels)\
        .to_csv(os.path.join(output_dir, "bio_biclustering_heatmap_matrix.csv"))
    pd.DataFrame({
        gene: [reordered_feature_labels[i] for i in np.argsort(-clustered_matrix[i])[:top_k]]
        for i, gene in enumerate(reordered_gene_names)
    }).T.rename(columns=lambda i: f"Top{i+1}")\
     .to_csv(os.path.join(output_dir, f"bio_biclustering_top{top_k}_features.csv"))

    # === Figure
    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    ##vmin, vmax = 0, np.percentile(clustered_matrix, 99)

    '''fig = plt.figure(figsize=(20, 20))
    gs = gridspec.GridSpec(3, 1, height_ratios=[1.0, 0.1, 20], hspace=0.0)
    gs_top = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=gs[0], width_ratios=[20, 1], wspace=0.0)
    gs_lower = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=gs[2], width_ratios=[20, 1], wspace=0.0)
    ax_top_bar = fig.add_subplot(gs_top[0])
    ax_heatmap = fig.add_subplot(gs_lower[0])
    ax_curve = fig.add_subplot(gs_lower[1], sharey=ax_heatmap)'''
    # fig = plt.figure(figsize=(22, 16))
    # ##gs = gridspec.GridSpec(2, 2, height_ratios=[1, 20], width_ratios=[20, 1], hspace=0.0, wspace=0.0)
    # gs = gridspec.GridSpec(2, 2, height_ratios=[2, 20], width_ratios=[20, 1], hspace=0.0, wspace=0.0)
    fig = plt.figure(figsize=(22, 16))
    gs = gridspec.GridSpec(2, 2, height_ratios=[0.8, 20], width_ratios=[20, 0.5], hspace=0.0, wspace=0.0)
    
    ax_top_bar = fig.add_subplot(gs[0, 0])
    ax_heatmap = fig.add_subplot(gs[1, 0])
    ax_curve = fig.add_subplot(gs[1, 1], sharey=ax_heatmap)

    # ax_top_bar.bar(
    #     x=np.arange(len(feature_avgs[col_order])) + 0.5,
    #     height=feature_avgs[col_order],
    #     width=1.0,
    #     color=feature_colors,
    #     edgecolor='black',
    #     linewidth=0.3
    # )
    
    # Normalize feature averages to [0, 1]
    heights = feature_avgs[col_order]
    norm_heights = (heights - heights.min()) / (heights.max() - heights.min() + 1e-6)

    # Plot with alpha-scaled bars
    for i, (norm_h, color) in enumerate(zip(norm_heights, feature_colors)):
        ax_top_bar.bar(
            x=i + 0.5,
            height=norm_h,#*0.5,
            width=1.0,
            color=color,
            edgecolor='none',
            linewidth=0,
            alpha=0.3 + 0.7 * norm_h
        )

    ax_top_bar.set_ylim(0, 1.1)
    # Safer xlim
    ax_top_bar.set_xlim(-0.5, len(feature_avgs[col_order]) + 0.5)
    # Turn off clutter
    ax_top_bar.axis("off")

    # ax_top_bar.set_xlim(0, len(feature_avgs[col_order]))
    # #ax_top_bar.set_ylim(0, 1.05)
    # ax_top_bar.set_ylim(0, feature_avgs[col_order].max() * 1.2)
    ax_top_bar.set_xticks([])
    ax_top_bar.set_yticks([])
    for spine in ax_top_bar.spines.values():
        spine.set_visible(False)

    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        # vmin=vmin,
        # vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax_heatmap
    )
    for i, cluster in enumerate(reordered_cluster_labels):
        ax_heatmap.add_patch(
            plt.Rectangle(
                (-1.5, i), 1.5, 1,
                linewidth=0,
                facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                clip_on=False
            )
        )
    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax_heatmap.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)
        start += size

    ax_heatmap.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax_heatmap.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=20)
    ax_heatmap.tick_params(axis='x', which='both', bottom=True, top=False, length=5, pad=1)
    for label, omics in zip(ax_heatmap.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics.upper(), 'black'))

    saliency_sums = clustered_matrix.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.xaxis.set_label_position('top')
    ax_curve.set_xlim([0, 1])
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=18)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.invert_yaxis()
    for spine in ax_curve.spines.values():
        spine.set_visible(False)
    ax_curve.tick_params(axis='y', left=False, labelleft=False)

    current_idx = 0
    for cluster_id, cluster_size in zip(unique_clusters, cluster_sizes):
        y_vals = np.arange(current_idx, current_idx + cluster_size)
        cluster_saliency = saliency_sums[current_idx: current_idx + cluster_size]
        cluster_color = cluster_colors.get(cluster_id, '#CCCCCC')
        ax_curve.fill_betweenx(y_vals, 0, cluster_saliency, color=cluster_color, alpha=0.9, linewidth=0)
        current_idx += cluster_size

    fig.subplots_adjust(top=0.99, bottom=0.01, hspace=0.0)
    out_fig_path = os.path.join(output_dir, "bio_biclustering_heatmap_curve.png")
    plt.savefig(out_fig_path, dpi=300, bbox_inches='tight', pad_inches=0.1)
    plt.close()
    print(f"[Saved] Heatmap → {out_fig_path}")

    # === Graph + stats
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_bio_summary'] = row_labels_tensor

    total_genes_per_cluster = {cid: int((row_labels == cid).sum()) for cid in np.unique(row_labels)}
    pred_counts = {}
    for cid in np.unique(row_labels):
        idx = np.where(row_labels == cid)[0]
        names_in_cluster = [node_names_topk[i] for i in idx]
        pred_counts[cid] = sum(1 for g in names_in_cluster if g in predicted_cancer_genes)

    pd.DataFrame(list(total_genes_per_cluster.items()), columns=["Cluster", "TotalGenes"])\
        .to_csv(os.path.join(output_dir, "bio_biclustering_total_genes.csv"), index=False)
    pd.DataFrame(list(pred_counts.items()), columns=["Cluster", "PredictedGenes"])\
        .to_csv(os.path.join(output_dir, "bio_biclustering_predicted_counts.csv"), index=False)

    #################
    # === Save detailed cluster assignment ===
    assignment_rows = []
    for i, (idx, cluster) in enumerate(zip(topk_node_indices, row_labels)):
        gene_name = node_names_topk[i] if i < len(node_names_topk) else f"node_{idx}"
        rel_score = saliency_matrix[i].sum().item()
        assignment_rows.append([gene_name, gene_name, idx, round(rel_score, 5), cluster])

    assignment_rows.sort(key=lambda x: (x[4], -x[3]))

    assignment_df = pd.DataFrame(
        assignment_rows,
        columns=["Source_Gene", "Neighbor_Name", "Neighbor_Index", "Relevance_Score", "Cluster"]
    )
    assignment_csv_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_bio_biclustering_gene_assignments.csv"
    )
    assignment_df.to_csv(assignment_csv_path, index=False)
    print(f"📄 Saved gene–neighbor cluster assignments: {assignment_csv_path}")

    # === Extra plots ===
    plot_bio_biclustering_clustermap(
        args=args,
        relevance_scores=saliency_matrix,
        omics_splits=omics_splits,
        output_path=os.path.join(output_dir, "spectral_biclustering_bio_fixed_clustermap.png"),
        row_labels=row_labels,
        col_labels=col_labels,
        cluster_colors=cluster_colors
    )

    plot_predicted_genes_distribution(
        pred_counts=pred_counts,
        output_path=os.path.join(output_dir, "predicted_genes_per_cluster.png")
    )

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def plot_topo_biclustering_heatmap(
    args,
    relevance_scores,
    row_labels,
    output_path,
    gene_names=None,
    col_labels=None
    ):
    

    """
    Plots a spectral biclustering heatmap for topological embeddings (1024–2047),
    with within-cluster gene sorting and column sorting by global relevance.

    Args:
        args: CLI or config object with settings.
        relevance_scores (np.ndarray): shape [num_nodes, 2048], full embedding.
        row_labels (np.ndarray): shape [num_nodes], integer cluster assignments.
        output_path (str): Path to save the figure.
        gene_names (list of str, optional): Gene name labels for heatmap index.

    Returns:
        pd.DataFrame: heatmap matrix with genes as rows and topo features as columns.
    """

    # 🔹 Extract 64D summary of topological features
    #relevance_scores = extract_summary_features_np_topo(relevance_scores)
    # Normalize features-----------------------------------------------------------------------------------------------------------------
    # relevance_scores = StandardScaler().fit_transform(relevance_scores)*10
    relevance_scores = (relevance_scores - relevance_scores.min()) / (relevance_scores.max() - relevance_scores.min())
    
    # 🔹 Create topo feature names (01–64)
    feature_names = [f"{i+1:02d}" for i in range(relevance_scores.shape[1])]

    # 🔹 Sort columns (features) by total relevance across all genes
    col_sums = relevance_scores.sum(axis=0)
    col_order = np.argsort(-col_sums)
    relevance_scores = relevance_scores[:, col_order]
    feature_names = [feature_names[i] for i in col_order]
    if col_labels is not None:
        col_labels = np.array(col_labels)[col_order]

    # 🔹 Sort by cluster → then by gene-wise relevance within cluster
    ordered_row_indices = []
    row_labels = np.array(row_labels)
    unique_clusters = np.unique(row_labels)

    for cluster in unique_clusters:
        cluster_idx = np.where(row_labels == cluster)[0]
        cluster_scores = relevance_scores[cluster_idx]
        cluster_gene_sums = cluster_scores.sum(axis=1)
        sorted_cluster = cluster_idx[np.argsort(-cluster_gene_sums)]
        ordered_row_indices.extend(sorted_cluster)

    sorted_scores = relevance_scores[ordered_row_indices]
    sorted_clusters = row_labels[ordered_row_indices]
    if gene_names is not None:
        gene_names = [gene_names[i] for i in ordered_row_indices]

    # 🔹 Compute cluster boundaries and centers
    _, counts = np.unique(sorted_clusters, return_counts=True)
    cluster_boundaries = np.cumsum(counts)
    cluster_start_indices = [0] + list(cluster_boundaries[:-1])
    cluster_centers = [(start + start + count - 1) / 2 for start, count in zip(cluster_start_indices, counts)]

    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())
    
    # 🔹 Set colormap
    bluish_gray_gradient = LinearSegmentedColormap.from_list(
        "bluish_gray_gradient", ["#F0F3F4", "#85929e"]
    )

    
    # Construct row labels: use gene names if available, else fallback to generic
    row_gene_names = [gene_names[i] if gene_names is not None else f"Gene_{i}" for i in ordered_row_indices]

    # Build DataFrame for ordered scores
    df_ordered_scores = pd.DataFrame(sorted_scores, index=row_gene_names, columns=feature_names)

    # Add cluster information
    df_ordered_scores.insert(0, "Cluster", sorted_clusters)
    
    base_dir = os.path.dirname(output_path)
    cluster_output_dir = os.path.join(base_dir, "cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    cluster_output_dir = output_path.replace(".png", "_cluster_csvs")
    os.makedirs(cluster_output_dir, exist_ok=True)

    for cluster_id in np.unique(sorted_clusters):
        cluster_df = df_ordered_scores[df_ordered_scores["Cluster"] == cluster_id]
        cluster_csv_path = os.path.join(cluster_output_dir, f"cluster_{cluster_id}_genes.csv")
        cluster_df.to_csv(cluster_csv_path)
        print(f"Saved cluster {cluster_id} gene scores to {cluster_csv_path}")

    # Save to CSV
    csv_output_path = output_path.replace(".png", "_ordered_scores.csv")
    df_ordered_scores.to_csv(csv_output_path)
    print(f"Ordered relevance score matrix with cluster info saved to {csv_output_path}")


    # 🔹 Setup figure layout
    fig = plt.figure(figsize=(18, 17))
    gs = fig.add_gridspec(nrows=15, ncols=50, wspace=0.0, hspace=0.0)


    ax_bar = fig.add_subplot(gs[0, 2:45])
    ax = fig.add_subplot(gs[1:13, 2:45])
    ax_curve = fig.add_subplot(gs[1:13, 45:48], sharey=ax)
    ax_cbar = fig.add_subplot(gs[5:9, 49])
    #ax_legend = fig.add_subplot(gs[14, 2:45])

    # 🔹 Compute dynamic vmax
    vmin = np.percentile(sorted_scores, 5)
    vmax = np.percentile(sorted_scores, 99)


    feature_means = sorted_scores.mean(axis=0)
    feature_means = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min()) * 0.04

    ax_bar.bar(
        np.arange(len(feature_means)) + 0.5, 
        feature_means,
        width=1.0,
        color="#B0BEC5",
        linewidth=0,
        alpha=0.6
    )

    ax_bar.set_xticks([0, len(feature_means)])
    ax_bar.set_xticklabels(['0', '1'], fontsize=16)
    ax_bar.tick_params(axis='x', direction='out', pad=1)
        
    ax_bar.set_xlim(0, len(feature_means))  # align with heatmap width
    ax_bar.set_ylim(0, 0.04)
    ax_bar.set_yticks([])
    ax_bar.set_yticklabels([])
    ax_bar.tick_params(axis='y', length=0)  # removes tick marks
    ax_bar.set_xticks([])


    for spine in ['left', 'bottom', 'top', 'right']:
        ax_bar.spines[spine].set_visible(False)
    '''for spine in ['left', 'bottom']:
        ax_bar.spines[spine].set_visible(True)
        ax_bar.spines[spine].set_linewidth(1.0)
        ax_bar.spines[spine].set_color("black")'''


    # 🔹 Apply log transformation to enhance low-intensity features
    sorted_scores = np.log1p(sorted_scores)  # This will emphasize smaller values

    # 🔹 Normalize scores (optional but improves contrast)
    #sorted_scores = (sorted_scores - sorted_scores.min()) / (sorted_scores.max() - sorted_scores.min())

    # 🔹 Compute vmin and vmax dynamically
    vmin = 0#np.percentile(sorted_scores, 1)   # Stretch the color range from low values
    vmax = np.percentile(sorted_scores, 99)  # Cap extreme values

    # 🔹 Choose a perceptually clear colormap
    #colormap = "mako"  # or try "viridis", "plasma", "rocket", etc.

    # 🔹 Plot heatmap with new settings
    sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=True,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Log-Scaled Relevance",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )

    # 🔹 Plot heatmap
    '''sns.heatmap(
        sorted_scores,
        cmap=bluish_gray_gradient,
        vmin=15,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar_ax=ax_cbar,
        cbar_kws={
            "label": "Relevance Score",
            "shrink": 0.1,
            "aspect": 12,
            "pad": 0.02,
            "orientation": "vertical",
            "location": "right"
        },
        ax=ax
    )'''
    ax_cbar.yaxis.label.set_color("#85929e")
    ax_cbar.tick_params(colors="#85929e", labelsize=16)
    ax_cbar.yaxis.label.set_size(18)

    # 🔹 Add cluster color stripes
    for i, cluster in enumerate(sorted_clusters):
        ax.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(CLUSTER_COLORS.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # 🔹 Cluster size labels
    for cluster_id, center_y, count in zip(unique_clusters, cluster_centers, counts):
        ax.text(
            -2.0, center_y, f"{count}",
            va='center', ha='right', fontsize=18, fontweight='bold'
        )

    # 🔹 X-tick labels below heatmap
    ax.set_xticks(np.arange(len(feature_names)) + 0.5)
    ax.set_xticklabels(feature_names, rotation=90, fontsize=16)
    ax.tick_params(axis='x', bottom=True, labelbottom=True)

    ax.set_xlabel("")
    ax.set_ylabel("")
    ax.set_title("")

    # 🔹 Omics + LRP Legend
    '''ax_legend.axis("off")
    lrp_patch = Patch(facecolor='#a9cce3', alpha=0.8, label='Saliency Sum')
    ax_legend.legend(
        handles=[lrp_patch],
        loc="center",
        ncol=1,
        frameon=False,
        fontsize=16,
        handleheight=1.5,
        handlelength=3
    )'''

    # 🔹 Saliency Sum curve
    saliency_sums = sorted_scores.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    y = np.arange(len(saliency_sums))

    ax_curve.fill_betweenx(
        y, 0, saliency_sums,
        color='#a9cce3',
        alpha=0.8,
        linewidth=3
    )

    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.tick_params(axis='x', direction='out', pad=1)
    ax_curve.hlines(
        y=1.05, xmin=0, xmax=1,
        color='black', linewidth=1.5, transform=ax_curve.get_xaxis_transform()
    )
    ax_curve.xaxis.set_label_position('top')
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.spines['right'].set_visible(False)
    ax_curve.spines['left'].set_visible(False)
    ax_curve.spines['bottom'].set_visible(False)
    ax_curve.spines['top'].set_visible(False)
    ax_curve.tick_params(axis='y', length=0)

    # 🔹 Final layout + save
    plt.subplots_adjust(wspace=0, hspace=0)
    plt.tight_layout(rect=[0, 0.03, 1, 1])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved spectral clustering heatmap to {output_path}")

    # 🔹 Cluster-wise contribution breakdown
    plot_topo_clusterwise_feature_contributions(
        args=args,
        relevance_scores=relevance_scores,  # Not sorted for per-cluster breakdown
        row_labels=row_labels,
        feature_names=[f"{i+1:02d}" for i in range(relevance_scores.shape[1])],
        per_cluster_feature_contributions_output_dir=os.path.join(
            os.path.dirname(output_path), "per_cluster_feature_contributions_topo"
        )
    )

    return pd.DataFrame(sorted_scores, index=gene_names, columns=feature_names)

def plot_bio_biclustering_clustermap(
    args,
    relevance_scores,
    omics_splits,
    output_path,
    cluster_colors,
    omics_colors=None,
    row_labels=None,
    col_labels=None,
    gene_names=None,
    top_k=10
):
    # === Normalize
    relevance_scores = normalize(relevance_scores, axis=1)
    saliency_matrix_norm = relevance_scores

    if omics_colors is None:
        omics_colors = {
            'CNA': '#9370DB', 'GE': '#228B22', 'METH': '#00008B', 'MF': '#b22222'
        }

    # === Feature names
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'Kidney', 'KidneyPap',
        'Liver', 'LungAd', 'LungSc', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    omics_order = ['CNA', 'GE', 'METH', 'MF']
    feature_names = [f"{omics}: {cancer}" for omics in omics_order for cancer in cancer_names]
    feature_names = feature_names[:relevance_scores.shape[1]]

    # === Reorder rows/columns
    if row_labels is not None:
        row_order = np.argsort(row_labels)
        relevance_scores = relevance_scores[row_order]
        row_labels = row_labels[row_order]
        if gene_names is not None:
            gene_names = [gene_names[i] for i in row_order]
    if col_labels is not None:
        col_order = np.argsort(col_labels)
        relevance_scores = relevance_scores[:, col_order]
        feature_names = [feature_names[i] for i in col_order]
        col_labels = col_labels[col_order]

    reordered_omics_labels = [f.split(":")[0].strip().upper() for f in feature_names]
    reordered_feature_labels = [f.split(":")[1].strip() for f in feature_names]

    # === Save top-k features
    if gene_names is not None:
        topk_dir = os.path.join(os.path.dirname(output_path), "topk_features_per_gene")
        os.makedirs(topk_dir, exist_ok=True)
        for i, gene in enumerate(gene_names):
            topk_indices = np.argsort(-relevance_scores[i])[:top_k]
            topk_features = [feature_names[j] for j in topk_indices]
            topk_scores = relevance_scores[i][topk_indices]
            df_gene = pd.DataFrame({"Feature": topk_features, "Score": topk_scores})
            df_gene.to_csv(os.path.join(topk_dir, f"{gene}_top{top_k}_features.csv"), index=False)

    # === Setup figure with 3 rows: omics bar, cluster bar, heatmap
    fig = plt.figure(figsize=(20, 20))
    gs = gridspec.GridSpec(2, 1, height_ratios=[0.5, 19], hspace=0.0)
    ##gs = gridspec.GridSpec(2, 1, height_ratios=[1.2, 18], hspace=0.05)


    # === Top bar: column cluster stripe
    ax_col_cluster = fig.add_subplot(gs[0])
    col_cluster_colors = [CLUSTER_COLORS.get(c, '#FFFFFF') for c in col_labels]
    col_cluster_rgb = np.array([[to_rgb(c) for c in col_cluster_colors]])
    ax_col_cluster.imshow(col_cluster_rgb, aspect='auto', extent=[0, len(col_cluster_colors), 0, 1])
    ax_col_cluster.set_xlim([0, len(col_cluster_colors)])
    ax_col_cluster.set_xticks([])
    ax_col_cluster.set_yticks([])
    ax_col_cluster.set_frame_on(False)

    # === Main heatmap
    ax = fig.add_subplot(gs[1])

    ###################################


    # === Main heatmap
    #ax = fig.add_subplot(gs[2])
    bluish_gray_gradient = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])
    vmin, vmax = 0, np.percentile(relevance_scores, 99)

    sns.heatmap(
        relevance_scores,
        cmap=bluish_gray_gradient,
        vmin=vmin,
        vmax=vmax,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax
    )

    # === Row cluster color bars
    if row_labels is not None:
        for i, cluster in enumerate(row_labels):
            ax.add_patch(plt.Rectangle(
                (-1.5, i), 1.5, 1,
                linewidth=0,
                facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
                clip_on=False
            ))

        unique_clusters, cluster_sizes = np.unique(row_labels, return_counts=True)
        start_idx = 0
        for cluster, size in zip(unique_clusters, cluster_sizes):
            center_y = start_idx + size / 2
            ax.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)#, fontweight='bold')
            start_idx += size

    # === Feature x-labels, color-coded by omics
    ax.set_xticks(np.arange(len(reordered_feature_labels)) + 0.5)
    ax.set_xticklabels(reordered_feature_labels, rotation=90, fontsize=24)
    ax.tick_params(axis='x', which='both', bottom=True, top=False, length=5)
    for label, omics in zip(ax.get_xticklabels(), reordered_omics_labels):
        label.set_color(omics_colors.get(omics, 'black'))

    # === Finalize
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    fig.subplots_adjust(hspace=0.0)
    plt.close()
    print(f"[Saved] Clustermap with top omics bar → {output_path}")

def apply_biclustering_with_heatmap_plot_topo(
    graph,
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    topk_node_indices,
    predicted_cancer_genes: list,
    cluster_colors: dict,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):
    import os
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import matplotlib.gridspec as gridspec
    from matplotlib.colors import to_rgb, to_rgba, LinearSegmentedColormap
    import seaborn as sns
    from tqdm import tqdm
    from sklearn.preprocessing import normalize
    from sklearn.metrics import mean_squared_error
    from sklearn.cluster import SpectralBiclustering
    import torch

    os.makedirs(output_dir, exist_ok=True)
    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"

    saliency_matrix = normalize(saliency_matrix, axis=1)

    # === Biclustering
    best_model, best_score = None, np.inf
    valid = False
    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix)
        reordered = saliency_matrix[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix, reordered)
        if mse < best_score:
            best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Valid biclustering not found.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_

    row_order = []
    for cid in np.unique(row_labels):
        cluster_idx = np.where(row_labels == cid)[0]
        row_sums = saliency_matrix[cluster_idx].sum(axis=1)
        sorted_idx = cluster_idx[np.argsort(-row_sums)]
        row_order.extend(sorted_idx)

    col_order = []
    for cid in np.unique(col_labels):
        cluster_idx = np.where(col_labels == cid)[0]
        col_sums = saliency_matrix[:, cluster_idx].sum(axis=0)
        sorted_idx = cluster_idx[np.argsort(-col_sums)]
        col_order.extend(sorted_idx)

    clustered_matrix = saliency_matrix[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]
    reordered_col_labels = col_labels[col_order]

    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(os.path.join(output_dir, "topo_biclustering_row_labels.csv"), index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names)\
        .to_csv(os.path.join(output_dir, "topo_biclustering_heatmap_matrix.csv"))

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # === Plotting
    fig = plt.figure(figsize=(20, 16))
    gs = gridspec.GridSpec(3, 2, height_ratios=[0.3, 0.8, 20], width_ratios=[20, 0.5], hspace=0.0, wspace=0.0)
    ax_stripe = fig.add_subplot(gs[0, 0])
    ax_top_bar = fig.add_subplot(gs[1, 0])
    ax_heatmap = fig.add_subplot(gs[2, 0])
    ax_curve = fig.add_subplot(gs[2, 1], sharey=ax_heatmap)

    # === Column Cluster Stripe
    col_cluster_colors = [cluster_colors.get(c, '#CCCCCC') for c in reordered_col_labels]
    stripe_rgb = np.array([[to_rgb(c) for c in col_cluster_colors]])
    ax_stripe.imshow(stripe_rgb, aspect='auto', extent=[0, len(col_cluster_colors), 0, 1])
    ax_stripe.set_xlim(0, len(col_cluster_colors))
    ax_stripe.set_xticks([])
    ax_stripe.set_yticks([])
    ax_stripe.set_frame_on(False)

    # === Column value bar
    col_avgs = saliency_matrix[:, col_order].mean(axis=0)
    ax_top_bar.bar(
        x=np.arange(len(col_order)) + 0.5,
        height=col_avgs,
        width=1.0,
        color='#4682B4',
        edgecolor='black',
        linewidth=0.3
    )
    ax_top_bar.set_xlim(0, len(col_order))
    ax_top_bar.set_ylim(0, col_avgs.max() * 1.2)
    ax_top_bar.set_xticks([])
    ax_top_bar.set_yticks([])
    for spine in ax_top_bar.spines.values():
        spine.set_visible(False)

    # === Heatmap
    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        xticklabels=False,
        yticklabels=False,
        cbar=False,
        ax=ax_heatmap
    )
    for i, cluster in enumerate(reordered_cluster_labels):
        ax_heatmap.add_patch(plt.Rectangle(
            (-1.5, i), 1.5, 1,
            linewidth=0,
            facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')),
            clip_on=False
        ))

    # === Row cluster sizes
    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax_heatmap.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)
        start += size

    # === Curve plot
    saliency_sums = clustered_matrix.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())
    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.set_xlim(0, 1)
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=16)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.invert_yaxis()
    ax_curve.tick_params(axis='y', left=False, labelleft=False)
    for spine in ax_curve.spines.values():
        spine.set_visible(False)

    current_idx = 0
    for cluster_id, cluster_size in zip(unique_clusters, cluster_sizes):
        y_vals = np.arange(current_idx, current_idx + cluster_size)
        cluster_saliency = saliency_sums[current_idx: current_idx + cluster_size]
        cluster_color = cluster_colors.get(cluster_id, '#CCCCCC')
        ax_curve.fill_betweenx(y_vals, 0, cluster_saliency, color=cluster_color, alpha=0.9, linewidth=0)
        current_idx += cluster_size

    fig.subplots_adjust(top=0.99, bottom=0.01)
    out_fig_path = os.path.join(output_dir, "topo_biclustering_heatmap_curve.png")
    plt.savefig(out_fig_path, dpi=300, bbox_inches='tight', pad_inches=0.1)
    plt.close()
    print(f"[Saved] Topo Heatmap → {out_fig_path}")

    # === Add cluster assignments to graph
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo_summary'] = row_labels_tensor

    # === Save stats
    total_genes_per_cluster = {cid: int((row_labels == cid).sum()) for cid in np.unique(row_labels)}
    pred_counts = {
        cid: sum(1 for i in np.where(row_labels == cid)[0] if node_names_topk[i] in predicted_cancer_genes)
        for cid in np.unique(row_labels)
    }

    pd.DataFrame(list(total_genes_per_cluster.items()), columns=["Cluster", "TotalGenes"])\
        .to_csv(os.path.join(output_dir, "topo_biclustering_total_genes.csv"), index=False)
    pd.DataFrame(list(pred_counts.items()), columns=["Cluster", "PredictedGenes"])\
        .to_csv(os.path.join(output_dir, "topo_biclustering_predicted_counts.csv"), index=False)

    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts

def apply_biclustering_with_heatmap_plot_topo(
    graph,
    saliency_matrix: np.ndarray,
    node_names_topk: list,
    topk_node_indices,
    predicted_cancer_genes: list,
    cluster_colors: dict,
    output_dir: str,
    args,
    n_clusters_row: int = 4,
    n_clusters_col: int = 4,
    top_k: int = 10,
    n_trials: int = 10,
    cmap: str = None
):
    # best_model, best_score = None, np.inf
    # valid = False

    # for i in tqdm(range(n_trials * 10), desc="Biclustering Trials", ncols=80):
    #     model = SpectralBiclustering(
    #         n_clusters=(n_clusters_row, n_clusters_col),
    #         method='log',
    #         random_state=i
    #     )
    #     model.fit(saliency_matrix)

    #     row_labels = model.row_labels_
    #     col_labels = model.column_labels_

    #     # === Check: Does every COLUMN cluster have at least 2 predicted genes/features? ===
    #     pass_check = True
    #     for cid in np.unique(col_labels):
    #         cluster_idx = np.where(col_labels == cid)[0]
    #         # For columns, you usually check the feature *names* — here I assume you have that:
    #         # If your columns have names, supply them.
    #         names_in_cluster = [f"feat_{j}" for j in cluster_idx]  # Replace with real names if you have them!
    #         pred_count = sum(1 for f in names_in_cluster if f in predicted_cancer_genes)
    #         if pred_count < 2:
    #             pass_check = False
    #             break

    #     if not pass_check:
    #         continue  # Reject and try next

    #     # === Evaluate MSE for tie-breaking ===
    #     reordered = saliency_matrix[np.argsort(row_labels)][:, np.argsort(col_labels)]
    #     mse = mean_squared_error(saliency_matrix, reordered)

    #     if mse < best_score:
    #         best_model, best_score = model, mse
    #         valid = True

    # if not valid:
    #     raise RuntimeError("❌ Valid biclustering not found that satisfies min predicted genes/features per column cluster.")

    os.makedirs(output_dir, exist_ok=True)
    assert saliency_matrix.shape[0] == len(node_names_topk), "Row count mismatch"

    saliency_matrix = normalize(saliency_matrix, axis=1)

    best_model, best_score = None, np.inf
    valid = False
    for i in tqdm(range(n_trials * 3), desc="Biclustering Trials", ncols=80):
        model = SpectralBiclustering(n_clusters=(n_clusters_row, n_clusters_col), method='log', random_state=i)
        model.fit(saliency_matrix)
        reordered = saliency_matrix[np.argsort(model.row_labels_)][:, np.argsort(model.column_labels_)]
        mse = mean_squared_error(saliency_matrix, reordered)
        if mse < best_score:
            best_model, best_score = model, mse
            valid = True

    if not valid:
        raise RuntimeError("❌ Valid biclustering not found.")

    model = best_model
    row_labels = model.row_labels_
    col_labels = model.column_labels_

    row_order = []
    for cid in np.unique(row_labels):
        cluster_idx = np.where(row_labels == cid)[0]
        row_sums = saliency_matrix[cluster_idx].sum(axis=1)
        sorted_idx = cluster_idx[np.argsort(-row_sums)]
        row_order.extend(sorted_idx)

    col_order = []
    for cid in np.unique(col_labels):
        cluster_idx = np.where(col_labels == cid)[0]
        col_sums = saliency_matrix[:, cluster_idx].sum(axis=0)
        sorted_idx = cluster_idx[np.argsort(-col_sums)]
        col_order.extend(sorted_idx)

    clustered_matrix = saliency_matrix[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]
    reordered_col_labels = col_labels[col_order]

    pd.DataFrame({"Gene": reordered_gene_names, "Cluster": reordered_cluster_labels})\
        .to_csv(os.path.join(output_dir, "topo_biclustering_row_labels.csv"), index=False)
    pd.DataFrame(clustered_matrix, index=reordered_gene_names)\
        .to_csv(os.path.join(output_dir, "topo_biclustering_heatmap_matrix.csv"))

    if cmap is None:
        cmap = LinearSegmentedColormap.from_list("bluish_gray_gradient", ["#F0F3F4", "#85929e"])

    # Get col_order by grouping features by their column clusters
    col_order = []
    col_cluster_order = []
    for cid in np.unique(col_labels):
        idxs = np.where(col_labels == cid)[0]
        col_order.extend(idxs)
        col_cluster_order.extend([cid] * len(idxs))

    col_avgs = saliency_matrix.mean(axis=0)

    # Reorder matrix and cluster labels
    clustered_matrix = saliency_matrix[row_order][:, col_order]
    reordered_gene_names = [node_names_topk[i] for i in row_order]
    reordered_cluster_labels = row_labels[row_order]

    fig = plt.figure(figsize=(20, 16))
    # gs = gridspec.GridSpec(3, 2, height_ratios=[0.6, 0.5, 20], width_ratios=[20, 0.5], hspace=0.0, wspace=0.0)
    
    gs = gridspec.GridSpec(3, 2,
        height_ratios=[0.8, 0.3, 20],
        width_ratios=[20, 0.5],
        hspace=0.0, wspace=0.0)

    # === Top bar goes in row 0 ===
    ax_top_bar = fig.add_subplot(gs[0, 0])

    # === Color stripe goes in row 1 ===
    ax_col_cluster = fig.add_subplot(gs[1, 0])



    # === Top stripe showing column cluster colors
    # ax_col_cluster = fig.add_subplot(gs[0, 0])
    col_cluster_colors = [cluster_colors.get(cid, '#FFFFFF') for cid in col_cluster_order]
    col_cluster_rgb = np.array([[to_rgb(c) for c in col_cluster_colors]])
    ax_col_cluster.imshow(col_cluster_rgb, aspect='auto', extent=[0, len(col_order), 0, 1])
    ax_col_cluster.set_xlim([0, len(col_order)])
    ax_col_cluster.set_xticks([])
    ax_col_cluster.set_yticks([])
    ax_col_cluster.set_frame_on(False)

    # === Top bar with column feature values
    # ax_top_bar = fig.add_subplot(gs[1, 0])
    ax_top_bar.bar(
        x=np.arange(clustered_matrix.shape[1]) + 0.5,
        height=col_avgs[col_order],
        width=1.0,
        color='#4682B4',
        edgecolor='black',
        linewidth=0.3
    )
    ax_top_bar.set_xlim(0, clustered_matrix.shape[1])
    ax_top_bar.set_ylim(0, clustered_matrix[:, col_order].max() * 1.2)
    ax_top_bar.set_xticks([])
    ax_top_bar.set_yticks([])
    ax_top_bar.set_frame_on(False)

    # === Main heatmap
    ax_heatmap = fig.add_subplot(gs[2, 0])
    sns.heatmap(
        clustered_matrix,
        cmap=cmap,
        xticklabels=True,
        yticklabels=False,
        cbar=False,
        ax=ax_heatmap
    )

    # Add row cluster color patches
    for i, cluster in enumerate(reordered_cluster_labels):
        ax_heatmap.add_patch(
            plt.Rectangle((-1.5, i), 1.5, 1, linewidth=0, facecolor=to_rgba(cluster_colors.get(cluster, '#FFFFFF')), clip_on=False)
        )

    # Add cluster size labels on left
    unique_clusters, cluster_sizes = np.unique(reordered_cluster_labels, return_counts=True)
    start = 0
    for cluster, size in zip(unique_clusters, cluster_sizes):
        center_y = start + size / 2
        ax_heatmap.text(-2.0, center_y, f"{size}", va='center', ha='right', fontsize=22)
        start += size

    # === X-tick: Original feature indices, colored by cluster
    ax_heatmap.set_xticks(np.arange(len(col_order)) + 0.5)
    ax_heatmap.set_xticklabels([str(i) for i in col_order], rotation=90, fontsize=20)
    for label, cid in zip(ax_heatmap.get_xticklabels(), col_cluster_order):
        label.set_color(cluster_colors.get(cid, 'black'))

    ax_heatmap.tick_params(axis='x', which='both', bottom=True, top=False, length=5, pad=2)

    # === Right saliency curves
    ax_curve = fig.add_subplot(gs[2, 1], sharey=ax_heatmap)
    saliency_sums = clustered_matrix.sum(axis=1)
    saliency_sums = (saliency_sums - saliency_sums.min()) / (saliency_sums.max() - saliency_sums.min())

    ax_curve.xaxis.set_ticks_position('top')
    ax_curve.xaxis.set_label_position('top')
    ax_curve.set_xlim([0, 1])
    ax_curve.set_xticks([0, 1])
    ax_curve.set_xticklabels(['0', '1'], fontsize=14)
    ax_curve.set_ylim(0, len(saliency_sums))
    ax_curve.invert_yaxis()
    ax_curve.tick_params(axis='y', left=False, labelleft=False)
    for spine in ax_curve.spines.values():
        spine.set_visible(False)

    current_idx = 0
    for cluster_id, cluster_size in zip(unique_clusters, cluster_sizes):
        y_vals = np.arange(current_idx, current_idx + cluster_size)
        cluster_saliency = saliency_sums[current_idx: current_idx + cluster_size]
        cluster_color = cluster_colors.get(cluster_id, '#CCCCCC')
        ax_curve.fill_betweenx(y_vals, 0, cluster_saliency, color=cluster_color, alpha=0.9, linewidth=0)
        current_idx += cluster_size

    # === Final adjustments
    fig.subplots_adjust(top=0.98, bottom=0.01)
    out_fig_path = os.path.join(output_dir, "topo_biclustering_heatmap_curve.png")
    plt.savefig(out_fig_path, dpi=300, bbox_inches='tight', pad_inches=0.1)
    plt.close()
    print(f"[Saved] Topo Heatmap → {out_fig_path}")

    # === Stats
    row_labels_tensor = torch.full((graph.num_nodes(),), -1, dtype=torch.long)
    row_labels_tensor[topk_node_indices] = torch.tensor(row_labels, dtype=torch.long)
    graph.ndata['cluster_topo_summary'] = row_labels_tensor

    total_genes_per_cluster = {cid: int((row_labels == cid).sum()) for cid in np.unique(row_labels)}
    pred_counts = {}
    for cid in np.unique(row_labels):
        idx = np.where(row_labels == cid)[0]
        names_in_cluster = [node_names_topk[i] for i in idx]
        pred_counts[cid] = sum(1 for g in names_in_cluster if g in predicted_cancer_genes)

    pd.DataFrame(list(total_genes_per_cluster.items()), columns=["Cluster", "TotalGenes"])\
        .to_csv(os.path.join(output_dir, "topo_biclustering_total_genes.csv"), index=False)
    pd.DataFrame(list(pred_counts.items()), columns=["Cluster", "PredictedGenes"])\
        .to_csv(os.path.join(output_dir, "topo_biclustering_predicted_counts.csv"), index=False)

    # ✅✅✅ You MUST have:
    return graph, row_labels, col_labels, total_genes_per_cluster, pred_counts






def train(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    output_dir = 'results/gene_prediction/'
    os.makedirs(output_dir, exist_ok=True)

    # File path for saving model architecture
    model_csv_path = os.path.join(output_dir, f'{args.model_type}_model_structure.csv')

    n_clusters_row = 10
    n_clusters_col = 6
    
    omics_types = ['cna', 'ge', 'meth', 'mf']
    
    omics_colors = {
        'CNA': '#9370DB', 'GE': '#228B22', 'METH': '#00008B', 'MF': '#b22222'
    }
    
    cancer_names = [
        'Bladder', 'Breast', 'Cervix', 'Colon', 'Esophagus', 'HeadNeck', 'KidneyCC', 'KidneyPC',
        'Liver', 'LungAD', 'LungSC', 'Prostate', 'Rectum', 'Stomach', 'Thyroid', 'Uterus'
    ]
    bio_feat_names = [
        f"{cancer}_{omics}"
        for omics in omics_types
        for cancer in cancer_names
    ]    
    
    topo_feat_names = [f"Topo_{i}" for i in range(64)]

    # Define feature groups
    feature_groups = {
        "bio": (0, 1024),
        "topo": (1024, 2048)
    }
    
    omics_splits = {
        'cna': (0, 15),
        'ge': (16, 31),
        'meth': (32, 47),
        'mf': (48, 63),
    }
    
    epoch_times, cpu_usages, gpu_usages = [], [], []

    data_path = os.path.join('../gat/data/multiomics_meth/', f'{args.net_type}_omics_ppi_embeddings_graph_2048.json')
    ##data_path = os.path.join('../gat/data/json_graphs_omics_mf/', f'{cancer_type}_graph.json')
    ##data_path = '../___KG-PE/embedding/data/merged_gene_embeddings.json'
    
    nodes, edges, embeddings, labels = load_graph_data(data_path)

    graph = dgl.graph(edges)
    graph.ndata['feat'] = embeddings
    graph.ndata['label'] = labels
    graph.ndata['train_mask'] = labels != -1  # Mask for labeled nodes
    graph.ndata['test_mask'] = torch.ones_like(labels, dtype=torch.bool)  # Mask for testing on all nodes
    graph = dgl.add_self_loop(graph)
    in_feats = embeddings.shape[1]
    hidden_feats = args.hidden_feats
    out_feats = 1  # Binary classification (driver gene or not)
    
    
    # Compute node degrees
    graph.ndata['degree'] = graph.in_degrees().float().unsqueeze(1)  # Reshape to (N, 1)

    # Concatenate degree feature with node embeddings
    features = torch.cat([graph.ndata['feat'], graph.ndata['degree']], dim=1)

    # Update input feature dimension
    in_feats = features.shape[1]  # Update to reflect added degree feature

    # Model, loss, optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    

    node_names = list(nodes.keys())

    name_to_index = {name: idx for idx, name in enumerate(node_names)}
    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    graph = dgl.graph(edges, num_nodes=len(nodes))

    graph.ndata['feat'] = embeddings
    graph.ndata['label'] = labels
    graph.ndata['train_mask'] = labels != -1
    graph.ndata['test_mask'] = torch.ones_like(labels, dtype=torch.bool)
    graph = dgl.add_self_loop(graph)

    assert len(node_names) == graph.num_nodes(), "Node names length mismatch!"

    # ✅ Load ground truth cancer gene names
    ground_truth_cancer_genes = load_ground_truth_cancer_genes('../acgnn/data/ncg_8886.txt')
    
    in_feats = embeddings.shape[1]
    model = choose_model(args.model_type, in_feats, args.hidden_feats, 1).to(device)
    if hasattr(model, 'set_graph'):
        model.set_graph(graph)

    loss_fn = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    graph = graph.to(device)
    features = embeddings.to(device)
    labels = labels.to(device).float()
    train_mask = graph.ndata['train_mask'].to(device)


    for epoch in tqdm(range(args.num_epochs), desc="Training Progress", unit="epoch"):
        epoch_start = time.time()
        cpu_usage = psutil.cpu_percent(interval=None)
        gpu_usage = torch.cuda.memory_allocated(device) / 2048**2 if torch.cuda.is_available() else 0.0

        model.train()
        logits = model(graph, features).squeeze()
        loss = loss_fn(logits[train_mask], labels[train_mask])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_times.append(time.time() - epoch_start)
        cpu_usages.append(cpu_usage)
        gpu_usages.append(gpu_usage)

        tqdm.write(f"Epoch {epoch + 1}/{args.num_epochs}, Loss: {loss.item():.4f}, CPU: {cpu_usage}%, GPU: {gpu_usage:.2f} MB")

    model.eval()
    with torch.no_grad():
        ##trained_embeddings = model(graph, features, return_embeddings=True)  # shape: [num_nodes, hidden_feats]
        logits = model(graph, features).squeeze()
        ##node_embeddings = logits.unsqueeze(1)  # shape: [num_nodes, 1] for binary classification
        scores = torch.sigmoid(logits).cpu().numpy()

                
    # Convert epoch_times to floats during summation
    total_time = sum(epoch_times)  # Total time in seconds
    average_time_per_epoch = total_time / args.num_epochs  # Average time per epoch
    # Calculate average memory usage for CPU and GPU
    average_cpu_usage = sum(cpu_usages) / args.num_epochs  # CPU usage in MB
    average_gpu_usage = sum(gpu_usages) / args.num_epochs  # GPU usage in MB
    # Perform clustering and visualization on predicted cancer driver genes
    save_model_details(model, args, model_csv_path, in_feats, hidden_feats, out_feats)
    label_scores = save_predicted_scores(scores, labels, nodes, args)
    save_average_scores(label_scores, args)
    plot_average_scores(label_scores, args)
    plot_score_distributions(label_scores, args)
    save_performance_metrics(epoch_times, cpu_usages, gpu_usages, args)
    # After calculating total_time, average_time_per_epoch, etc.
    save_overall_metrics(total_time, average_time_per_epoch, average_cpu_usage, average_gpu_usage, args, output_dir)



    ##scores = train_with_relevance_tracking(args, model, graph, embeddings, labels, train_mask, output_dir)

    non_labeled_nodes = [i for i, label in enumerate(labels) if label == -1]
    non_labeled_scores = [(node_names[i], scores[i]) for i in non_labeled_nodes]
    ranking = sorted(non_labeled_scores, key=lambda x: x[1], reverse=True)

    process_predictions(
        ranking, args,
        "../acgnn/data/796_drivers.txt",
        "../acgnn/data/oncokb_1172.txt",
        "../acgnn/data/ongene_803.txt",
        "../acgnn/data/ncg_8886.txt",
        "../acgnn/data/intogen_23444.txt",
        node_names,
        non_labeled_nodes
    )
    ##plot_and_analyze(args)

    predicted_cancer_genes = [i for i, _ in ranking[:1000]]
    #random.shuffle(predicted_cancer_genes)

    top_gene_indices = [name_to_index[name] for name in predicted_cancer_genes if name in name_to_index]
    graph.ndata['degree'] = graph.in_degrees().float().unsqueeze(1)

    if top_gene_indices:
        avg_degree = graph.ndata['degree'][top_gene_indices].float().mean().item()
        print(f"Average degree of top predicted nodes: {avg_degree:.2f}")
    else:
        print("No top nodes predicted above the threshold.")


    ###########################################################################################################################################
    #
    # Biclustering for Bio and Topo 
    # 
    ###########################################################################################################################################
    
    print("Generating feature importance plots...")

    top_node_features = embeddings[top_gene_indices].cpu().numpy()  # shape: [num_top_genes, feature_dim]

    # Call the function to find optimal k
    plot_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_silhouette_score_plot_epo{args.num_epochs}.png")
    n_clusters_row_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_n_clusters_row_epo{args.num_epochs}.txt")

    # Make sure output_dir exists
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs('data/', exist_ok=True)

    # Extract node names of top-k predicted genes
    node_names_topk = [node_names[i] for i in top_gene_indices]

    # Use dynamic naming based on model/net type and epochs
    bio_output_img = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_BIO_clusters_epo{args.num_epochs}.png")
    topo_output_img = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_TOPO_clusters_epo{args.num_epochs}.png")

    bio_output_path_heatmap = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_BIO_heatmap_epo{args.num_epochs}.png")
    topo_output_path_heatmap = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_TOPO_heatmap_epo{args.num_epochs}.png")
    
    bio_graph_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_BIO_clustered_graph.pth")
    topo_graph_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_TOPO_clustered_graph.pth")

    bio_row_labels_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_BIO_row_labels.npy")
    topo_row_labels_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_TOPO_row_labels.npy")

    bio_total_per_cluster_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_BIO_total_genes_per_cluster.npy")
    topo_total_per_cluster_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_TOPO_total_genes_per_cluster.npy")

    bio_pred_counts_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_BIO_pred_counts.npy")
    topo_pred_counts_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_TOPO_pred_counts.npy")

    plot_silhouette_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_silhouette_score_plot_epo{args.num_epochs}.png")
    n_clusters_row_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_n_clusters_row_epo{args.num_epochs}.txt")
    #n_clusters_row = 10#find_optimal_k(top_node_features, k_range=(5, 20), plot_path=plot_silhouette_path, save_n_clusters_row_path=n_clusters_row_path)
    # Automatically determine n_clusters_row via eigengap
    plot_eigengap_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_epo{args.num_epochs}.png")
    #n_clusters_row=eigengap_analysis(summary_bio_topk, max_clusters=25, normalize=True, plot_path=plot_eigengap_path)
    
    #n_clusters_row=10


    bio_feats = graph.ndata['feat'][:, :1024]  # biological features
    topo_feats = graph.ndata['feat'][:, 1024:] # topology features

    bio_embeddings_np = bio_feats.cpu().numpy()
    summary_bio_features = extract_summary_features_np_bio(bio_embeddings_np)  # shape [num_nodes, 64]

    topo_embeddings_np = topo_feats.cpu().numpy()
    summary_topo_features = extract_summary_features_np_topo(topo_embeddings_np)  # shape [num_nodes, 64]

    relevance_scores = compute_relevance_scores(model, graph, features)
    

    ###################################################################################################################
    #
    ###################################################################################################################    
    retained_gene_names, retained_relevance, retained_gene_indices = filter_topk_midrange_relevance_genes(
        node_names=node_names,
        labels=labels.cpu().numpy(),
        scores=scores,
        relevance_scores=relevance_scores,
        output_dir=output_dir,
        top_k=1000,
        relevance_q_low=0.01,
        #relevance_q_high=0.85,
        relevance_q_high=0.99
    )
    
    # ➤ Extract info
    # retained_gene_names = [name for name, _, _ in top_retained]
    # retained_gene_indices = [idx for _, _, idx in top_retained]
    topk_node_indices_tensor = torch.tensor(retained_gene_indices, dtype=torch.int64).view(-1)

    top_gene_indices = retained_gene_indices#[name_to_index[name] for name in retained_gene_names if name in name_to_index]
    node_names_topk = retained_gene_names#[node_names[i] for i in top_gene_indices]
    predicted_cancer_genes = retained_gene_names
    
    # === Prepare relevance score matrices
    ##retained_relevance = relevance_scores[retained_gene_indices]  # [1000, 2048]
    relevance_scores_bio = retained_relevance[:, :1024]
    relevance_scores_topo = retained_relevance[:, 1024:]

    # === Compute summary relevance matrices
    relevance_matrix_bio = extract_summary_features_np_bio(relevance_scores_bio.cpu().numpy())
    relevance_matrix_topo = extract_summary_features_np_topo(relevance_scores_topo.cpu().numpy())
    ###################################################################################################################
    #
    ###################################################################################################################
    # Slice relevance scores for biological and topological embeddings
    relevance_scores_bio = relevance_scores[:, :1024]
    relevance_scores_topo = relevance_scores[:, 1024:]
    topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
    
    # Step 1: Slice top-k saliency scores for bio features (1024)
    relevance_scores_bio = relevance_scores[:, :1024]
    topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
    relevance_scores_topk_bio = relevance_scores_bio[topk_node_indices_tensor]
    relevance_matrix_bio = extract_summary_features_np_bio(relevance_scores_topk_bio.detach().cpu().numpy())

    print(relevance_matrix_bio.shape)

    relevance_scores_topk_topo = relevance_scores_topo[topk_node_indices_tensor]
    relevance_matrix_topo = extract_summary_features_np_topo(relevance_scores_topk_topo.detach().cpu().numpy())
     
    
    n_clusters_row_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_n_clusters_row_epo{args.num_epochs}.txt")
    #n_clusters_row = 10#find_optimal_k(top_node_features, k_range=(5, 20), plot_path=plot_silhouette_path, save_n_clusters_row_path=n_clusters_row_path)
    # Automatically determine n_clusters_row via eigengap
    plot_eigengap_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_eigengap_epo{args.num_epochs}.png")
    assert relevance_matrix_bio.shape[1] == 64, f"Expected 64 summary features, got {relevance_matrix_bio.shape[1]}"
    assert relevance_matrix_topo.shape[1] == 64, f"Expected 64 summary features, got {relevance_matrix_topo.shape[1]}"
    #n_clusters_row, eigenvals = eigengap_analysis(relevance_matrix, max_clusters=11, normalize=True, plot_path=plot_eigengap_path)

    n_clusters_row=10
    print(f"Best number of clusters (k > 1): {n_clusters_row}")

    # node_names_topk = [node_names[i] for i in top_gene_indices]
    
    output_path_genes_clusters = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_clusters_epo{args.num_epochs}.png')
    output_path_predicted_genes = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_genes.csv')

    spectral_biclustering_graph_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_clustered_graph.pth")
    row_labels_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_row_labels.npy")
    total_genes_per_cluster_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_total_genes_per_cluster.npy")
    pred_counts_path = os.path.join('data/', f"{args.model_type}_{args.net_type}_saved_pred_counts.npy")

    '''output_path_sorted_ = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_biclustering_input_heatmap_epo{args.num_epochs}_sorted_.png"
    )'''
    
    graph_bio, row_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_biclustering_with_heatmap_plot_bio(
        graph=graph,
        saliency_matrix=relevance_matrix_bio,
        node_names_topk=node_names_topk, #[node_names[i] for i in top_gene_indices],
        topk_node_indices=top_gene_indices,  # <-- add this!
        predicted_cancer_genes=predicted_cancer_genes,
        cluster_colors=CLUSTER_COLORS,
        omics_colors=omics_colors,
        omics_splits=omics_splits,
        n_clusters_row=n_clusters_row,
        n_clusters_col=n_clusters_col,
        ##output_path=output_path_sorted_,
        output_dir=output_dir,
        args=args,
    )
    # graph_bio, row_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_full_spectral_biclustering_bio(
    #     graph=graph,
    #     summary_bio_features=relevance_matrix_bio,  # shape (1000, 64)
    #     node_names=node_names,
    #     predicted_cancer_genes=predicted_cancer_genes,
    #     n_clusters=n_clusters_row,
    #     save_path=bio_graph_path,
    #     save_row_labels_path=bio_row_labels_path,
    #     save_total_genes_per_cluster_path=bio_total_per_cluster_path,
    #     save_predicted_counts_path=bio_pred_counts_path,
    #     output_path_genes_clusters=bio_output_img,
    #     output_path_heatmap=bio_output_path_heatmap,
    #     topk_node_indices=top_gene_indices  # <-- add this!
    # )

    '''graph_bio, row_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_full_spectral_biclustering_bio(
        graph=graph,
        saliency_matrix=relevance_matrix_bio,
        node_names_topk=node_names_topk,
        omics_splits=omics_splits,
        predicted_cancer_genes=predicted_cancer_genes,
        n_clusters_row = n_clusters_row,
        n_clusters_col = n_clusters_col,
        save_path=bio_graph_path,
        save_row_labels_path=bio_row_labels_path,
        save_total_genes_per_cluster_path=bio_total_per_cluster_path,
        save_predicted_counts_path=bio_pred_counts_path,
        output_path_genes_clusters=bio_output_img,
        output_path_heatmap=bio_output_path_heatmap,
        topk_node_indices=top_gene_indices,
        output_dir=output_dir,  
        cluster_colors=CLUSTER_COLORS,
        args=args               # <-- new argument
    )'''

    '''graph_bio, row_labels_bio, col_labels_bio, bio_total_counts, bio_pred_counts = apply_full_spectral_biclustering_bio(
        graph=graph,
        summary_bio_features=relevance_matrix_bio,
        node_names_topk=node_names_topk,
        omics_splits=omics_splits,
        predicted_cancer_genes=predicted_cancer_genes,
        save_path=bio_graph_path,
        save_row_labels_path=bio_row_labels_path,
        save_total_genes_per_cluster_path=bio_total_per_cluster_path,
        save_predicted_counts_path=bio_pred_counts_path,
        output_path_genes_clusters=bio_output_img,
        output_path_heatmap=bio_output_path_heatmap,
        topk_node_indices=top_gene_indices,
        output_dir=output_dir,  # <-- new argument
        args=args               # <-- new argument
    )
    '''
    # First create a full tensor of -1 (default for unclustered)
    full_row_labels_bio = torch.full((graph_bio.num_nodes(),), -1, dtype=torch.long)

    # Then assign the cluster labels only to the relevant subset
    full_row_labels_bio[top_gene_indices] = torch.tensor(row_labels_bio, dtype=torch.long)

    # Now safely assign it to the graph
    graph_bio.ndata['cluster_bio'] = full_row_labels_bio
    
    G_bio = graph_bio.to_networkx(node_attrs=['cluster_bio'])
 
    # plot_relevance_tsne_umap(
    #     relevance_scores,
    #     G_bio,
    #     cluster_key='cluster_bio',
    #     method='umap',  # or 'tsne'
    #     title_suffix="(Biological)"
    # )
    
    # === TOPO BICLUSTERING ===
    graph_topo, row_labels_topo, col_labels_topo, topo_total_counts, topo_pred_counts = apply_biclustering_with_heatmap_plot_topo(
        graph=graph,
        #topo_embeddings=topo_feats,
        saliency_matrix=relevance_matrix_topo,
        node_names_topk=node_names_topk,
        topk_node_indices=top_gene_indices,  # <-- add this!
        predicted_cancer_genes=predicted_cancer_genes,
        cluster_colors=CLUSTER_COLORS,
        n_clusters_row=n_clusters_row,
        n_clusters_col=n_clusters_col,
        ##output_path=output_path_sorted_,
        output_dir=output_dir,
        args=args,
    )

    '''graph_topo, row_labels_topo, col_labels_topo, topo_total_counts, topo_pred_counts = apply_full_spectral_biclustering_topo(
        graph=graph,
        #topo_embeddings=topo_feats,
        summary_topo_features=relevance_matrix_topo,
        node_names_topk=node_names_topk,
        #feature_names=None,
        predicted_cancer_genes=node_names_topk,
        n_clusters=n_clusters_row,
        save_path=topo_graph_path,
        save_row_labels_path=topo_row_labels_path,
        save_total_genes_per_cluster_path=topo_total_per_cluster_path,
        save_predicted_counts_path=topo_pred_counts_path,
        output_path_genes_clusters=topo_output_img,
        output_path_heatmap=topo_output_path_heatmap,
        topk_node_indices=top_gene_indices  # <-- add this!
    )
    '''
    
    full_row_labels_topo = torch.full((graph_topo.num_nodes(),), -1, dtype=torch.long)

    # Then assign the cluster labels only to the relevant subset
    full_row_labels_topo[top_gene_indices] = torch.tensor(row_labels_topo, dtype=torch.long)

    # Now safely assign it to the graph
    graph_topo.ndata['cluster_topo'] = full_row_labels_topo

    # Plot known cancer gene percentages
    gt_indices = set(name_to_index[name] for name in ground_truth_cancer_genes if name in name_to_index)
    kcg_counts_bio = {
        i: sum((np.array(row_labels_bio) == i) & np.isin(range(len(row_labels_bio)), list(gt_indices)))
        for i in range(n_clusters_row)
    }
    

    # === BIO INTERACTION PLOTS ===
    degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()
    row_labels_np_bio = np.array(row_labels_bio)

    kcg_nodes = [i for i, name in enumerate(node_names_topk) if name in ground_truth_cancer_genes]
    kcg_data_bio = pd.DataFrame({
        "Cluster": row_labels_np_bio[kcg_nodes],
        "Interactions": degrees_np[kcg_nodes]
    })

    pcg_data_bio = pd.DataFrame({
        "Cluster": row_labels_np_bio,#[top_gene_indices],
        "Interactions": degrees_np[top_gene_indices]
    })

    suffix = 'bio'
    output_path_interactions_kcgs_bio = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcgs_interaction_{suffix}_epo{args.num_epochs}.png')
    output_path_interactions_pcgs_bio = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_pcgs_interaction_{suffix}_epo{args.num_epochs}.png')

    plot_interactions_with_kcgs(kcg_data_bio, output_path_interactions_kcgs_bio)
    plot_interactions_with_pcgs(pcg_data_bio, output_path_interactions_pcgs_bio)

    # Plot PCG cluster percentages
    plot_pcg_cancer_genes(
        clusters=range(n_clusters_row),
        predicted_cancer_genes_count=bio_pred_counts,
        total_genes_per_cluster=bio_total_counts,
        node_names=node_names_topk,
        row_labels=row_labels_bio,
        output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_pcg_percent_bio_epo{args.num_epochs}.png')
    )

    # Plot known cancer gene percentages
    kcg_counts_bio = {
        i: sum((np.array(row_labels_bio) == i) & np.isin(range(len(row_labels_bio)), list(gt_indices)))
        for i in range(n_clusters_row)
    }
    
    plot_kcg_cancer_genes(
        clusters=range(n_clusters_row),
        kcg_count=kcg_counts_bio,
        total_genes_per_cluster=bio_total_counts,
        node_names=node_names_topk,
        row_labels=row_labels_bio,
        output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcg_percent_bio_epo{args.num_epochs}.png')
    )

    # ➕ Novel PCG bar plot
    
    # Output path
    novel_pcg_plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_novel_pcg_cluster_bar_{suffix}_epo{args.num_epochs}.png"
    )
        
    novel_predicted_genes = [n for n in predicted_cancer_genes if n not in ground_truth_cancer_genes]

    plot_novel_predicted_cancer_genes(
        clusters=list(bio_total_counts.keys()),
        novel_predicted_cancer_genes=novel_predicted_genes,
        total_genes_per_cluster=bio_total_counts,
        node_names=node_names_topk,
        row_labels=row_labels_bio,
        output_path=novel_pcg_plot_path
    )
            
            
    # === TOPO INTERACTION PLOTS ===
    row_labels_np_topo = np.array(row_labels_topo)

    kcg_data_topo = pd.DataFrame({
        "Cluster": row_labels_np_topo[kcg_nodes],
        "Interactions": degrees_np[kcg_nodes]
    })

    pcg_data_topo = pd.DataFrame({
        "Cluster": row_labels_np_topo,#[top_gene_indices],
        "Interactions": degrees_np[top_gene_indices]
    })

    suffix = 'topo'
    output_path_interactions_kcgs_topo = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcgs_interaction_{suffix}_epo{args.num_epochs}.png')
    output_path_interactions_pcgs_topo = os.path.join(output_dir, f'{args.model_type}_{args.net_type}_pcgs_interaction_{suffix}_epo{args.num_epochs}.png')

    plot_interactions_with_kcgs(kcg_data_topo, output_path_interactions_kcgs_topo)
    plot_interactions_with_pcgs(pcg_data_topo, output_path_interactions_pcgs_topo)

    novel_pcg_plot_path = os.path.join(
        output_dir,
        f"{args.model_type}_{args.net_type}_novel_pcg_cluster_bar_{suffix}_epo{args.num_epochs}.png"
    )
        
    novel_predicted_genes = [n for n in predicted_cancer_genes if n not in ground_truth_cancer_genes]

    plot_novel_predicted_cancer_genes(
        clusters=list(topo_total_counts.keys()),
        novel_predicted_cancer_genes=novel_predicted_genes,
        total_genes_per_cluster=topo_total_counts,
        node_names=node_names_topk,
        row_labels=row_labels_topo,
        output_path=novel_pcg_plot_path
    )
    
    neighbor_indices = set()
    for idx in top_gene_indices:
        neighbors = graph.successors(idx).tolist()
        neighbor_indices.update(neighbors)

    confirmed_file = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_confirmed_genes_epo{args.num_epochs}.csv")
    # with open(confirmed_file, "r") as f:
    #     confirmed_genes = [line.strip() for line in f if line.strip()]

    for cluster_type, row_labels, relevance_scores_subset, tag in [
        ("bio", full_row_labels_bio, relevance_matrix_bio, "bio"),
        ("topo", full_row_labels_topo, relevance_matrix_topo, "topo")
    ]:
        print(f"📊 Processing cluster type: {tag}")
             
        # Assign cluster labels to graph
        graph.ndata[f'cluster_{tag}'] = torch.tensor(row_labels)

        # Build cluster-to-gene mappings
        predicted_cancer_genes_indices = set(name_to_index[name] for name in predicted_cancer_genes if name in name_to_index)
        cluster_to_genes = {i: [] for i in range(n_clusters_row)}
        for idx in predicted_cancer_genes_indices:
            cluster_id = int(row_labels[idx])  # Ensure it's a Python int
            cluster_to_genes[cluster_id].append(node_names[idx])

        # Prepare cluster-degrees dataframe for known and predicted genes
        row_labels_np = graph.ndata[f'cluster_{tag}'].cpu().numpy()
        degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()

        # Known cancer genes
        kcg_nodes = [i for i, name in enumerate(node_names) if name in ground_truth_cancer_genes]
        kcg_data = pd.DataFrame({
            "Cluster": row_labels_np[kcg_nodes],
            "Interactions": degrees_np[kcg_nodes]
        })

        # Predicted cancer genes
        pcg_nodes = top_gene_indices
        pcg_data = pd.DataFrame({
            "Cluster": row_labels_np[pcg_nodes],
            "Interactions": degrees_np[pcg_nodes]
        })

        # ➕ Plot novel predicted PCGs per cluster
        # novel_predicted_genes = [
        #     name for name in predicted_cancer_genes
        #     if name not in ground_truth_cancer_genes
        # ]

        # Compute total genes per cluster
        row_labels_np = row_labels.cpu().numpy() if isinstance(row_labels, torch.Tensor) else row_labels
        total_genes_per_cluster = {c: np.sum(row_labels_np == c) for c in np.unique(row_labels_np)}

        # ➕ Plot known cancer genes (KCGs) per cluster
        kcg_count_per_cluster = {c: 0 for c in np.unique(row_labels)}
        for i, name in enumerate(node_names):
            if name in ground_truth_cancer_genes:
                cluster = int(row_labels[i])
                kcg_count_per_cluster[cluster] += 1

        # Output path
        kcg_plot_path = os.path.join(
            output_dir,
            f"{args.model_type}_{args.net_type}_kcg_cluster_bar_{tag}_epo{args.num_epochs}.png"
        )


        # Save path for confirmed gene list
        confirmed_genes_save_path = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_confirmed_genes_{tag}_epo{args.num_epochs}.txt"
        )

        # Prepare for heatmap
        '''topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64).view(-1)
        relevance_scores_topk = relevance_scores_subset[topk_node_indices_tensor]'''
        row_labels_topk = graph.ndata[f'cluster_{tag}'][topk_node_indices_tensor]

        heatmap_path = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}.png"
        )

        heatmap_path_kcg = os.path.join(
                    output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}_kcg.png"
                )

        heatmap_path_npcg = os.path.join(
                    output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}_npcg.png"
                )
        
        heatmap_path_unsort = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}_unsort.png"
        )

        heatmap_path_clusters_unsort = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_spectral_biclustering_heatmap_{tag}_epo{args.num_epochs}_clusters_unsort.png"
        )
        saliency_ridge_path = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_saliency_ridge_{tag}_epo{args.num_epochs}.png"
        )

        ridge_all_clusters_path = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_ridge_all_clusters_{tag}_epo{args.num_epochs}.png"
        )

        ridge_from_bio_path_across = os.path.join(
            output_dir, f"{args.model_type}_{args.net_type}_ridge_from_{tag}_across_epo{args.num_epochs}"
        ) 
        
        sanity_heatmap_path = os.path.join(output_dir, f"{args.model_type}_{args.net_type}_input_bio_heatmap_epo{args.num_epochs}.png")
                   


        print(f"📊 Processing cluster type: {tag}")

        if tag == "bio":

            # plot_biclustering_input_feature_heatmap(
            #     saliency_matrix=relevance_matrix_bio,
            #     node_names_topk=node_names_topk, #[node_names[i] for i in top_gene_indices],
            #     cluster_colors=CLUSTER_COLORS,
            #     omics_colors=omics_colors,
            #     n_clusters_row=n_clusters_row,
            #     n_clusters_col=n_clusters_col,
            #     output_path=sanity_heatmap_path,
            #     output_dir=output_dir,
            #     args=args,
            #     # title="Sanity Check: Input Bio Features (Top-k)"
            # )
            '''output_path_sorted = os.path.join(
                output_dir,
                f"{args.model_type}_{args.net_type}_biclustering_input_heatmap_epo{args.num_epochs}_sorted.png"
            )

            plot_biclustering_input_feature_heatmap_bio_sorted(
                saliency_matrix=relevance_matrix_bio,
                node_names_topk=node_names_topk, #[node_names[i] for i in top_gene_indices],
                cluster_colors=CLUSTER_COLORS,
                omics_colors=omics_colors,
                n_clusters_row=n_clusters_row,
                n_clusters_col=n_clusters_col,
                output_path=output_path_sorted,
                output_dir=output_dir,
                args=args,
            )'''


            plot_collapsed_clusterfirst_multilevel_sankey_bio(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                node_names_topk=node_names_topk,
                # novel_predicted_genes=novel_predicted_genes,
                # scores=scores,
                row_labels=row_labels_bio,
                total_clusters=n_clusters_row,
                output_dir=output_dir,
                relevance_scores=relevance_matrix_bio,
                CLUSTER_COLORS=CLUSTER_COLORS 
            )
              
            ## Plot bio neighbor relevance for confirmed genes
            # plot_confirmed_neighbors_bio(
            #     args=args,
            #     graph=graph,
            #     node_names=node_names,
            #     name_to_index=name_to_index,
            #     node_names_topk=node_names_topk,
            #     predicted_cancer_genes=predicted_cancer_genes,
            #     # scores=scores,
            #     row_labels=row_labels_bio,#graph.ndata["cluster_bio"]
            #     total_clusters=n_clusters_row,
            #     output_dir=output_dir,
            #     relevance_scores=relevance_matrix_bio,
            #     # relevance_scores=relevance_scores_bio
            # )     

                                    
            '''plot_biclustering_input_feature_heatmap(
                feature_matrix=relevance_matrix_bio,
                node_names_topk=[node_names[i] for i in top_gene_indices],
                output_path=sanity_heatmap_path,
                title="Sanity Check: Input Bio Features (Top-k)"
            )'''

            
            plot_bio_biclustering_heatmap_kcg(
                args=args,
                relevance_scores=relevance_matrix_bio,
                row_labels=row_labels_bio,
                omics_splits=omics_splits,
                output_path=heatmap_path_kcg,
                gene_names=node_names_topk,
                kcg_list=ground_truth_cancer_genes
            )

            plot_bio_biclustering_heatmap_npcg(
                args=args,
                relevance_scores=relevance_matrix_bio,
                row_labels=row_labels_bio,
                omics_splits=omics_splits,
                output_path=heatmap_path_npcg,
                gene_names=node_names_topk,
                kcg_list=ground_truth_cancer_genes,                  
                plot_only_novel=True
            )
            
            plot_bio_biclustering_heatmap(
                args=args,
                relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                omics_splits=omics_splits,
                #cancer_names=cancer_names,
                output_path=heatmap_path,
                col_labels=col_labels_bio
            )

            # plot_bio_biclustering_heatmap_unsort(
            #     args=args,
            #     relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
            #     #row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
            #     #gene_names=node_names_topk,
            #     omics_splits=omics_splits,
            #     output_path=heatmap_path_unsort,
            #     col_labels=col_labels_bio
            # )

            # plot_bio_biclustering_clustermap(
            #     args=args,
            #     relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
            #     #row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
            #     #gene_names=node_names_topk,
            #     omics_splits=omics_splits,
            #     output_path=heatmap_path_unsort,
            #     col_labels=col_labels_bio
            # )

            plot_bio_biclustering_heatmap_unsort(
                args=args,
                relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                omics_splits=omics_splits,
                output_path=heatmap_path_unsort,
                col_labels=col_labels_bio
            )

            plot_bio_biclustering_heatmap_clusters_unsort(
                args=args,
                relevance_scores=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_bio,#row_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                omics_splits=omics_splits,
                output_path=heatmap_path_clusters_unsort,
                col_labels=col_labels_bio
            )
            
            #sankey_bio_output_path=os.path.join(output_dir, f"{args.model_type}_{args.net_type}_dynamic_sankey_bio_epo{args.num_epochs}.html")
            ##selected_genes = ["PLK1", "SKP2", "SRC", "TRAF2"]
          
            # plot_bio_biclustering_heatmap_clusters_unsort(
            #     args=args,
            #     relevance_scores=relevance_matrix_bio,
            #     row_labels=row_labels_bio,
            #     omics_splits=omics_splits,
            #     output_path=heatmap_path_clusters_unsort,
            #     gene_names=None,
            #     col_labels=col_labels_bio
            #     #sorted_node_indices=np.array(topk_node_indices)[high_saliency_indices]
            # )
        
            cluster_dict = cluster_to_genes  # from earlier code
            
            save_and_plot_confirmed_genes_bio(
                args=args,
                node_names_topk=node_names_topk,
                node_scores_topk=relevance_matrix_bio,#relevance_scores_topk.detach().cpu().numpy(),
                summary_feature_relevance=relevance_matrix_bio,
                output_dir=output_dir,
                confirmed_genes_save_path=confirmed_file,
                row_labels_topk=row_labels_bio,
                tag="bio",
                confirmed_gene_path="../acgnn/data/ncg_8886.txt"
                ##confirmed_gene_path="data/796_drivers.txt"
            )

            
            with open(confirmed_file, "r") as f:
                confirmed_genes = [line.strip() for line in f if line.strip()]
                

            save_and_plot_novel_genes_bio(
                args=args,
                node_names_topk=node_names_topk,
                node_scores_topk=relevance_matrix_bio,
                summary_feature_relevance=relevance_matrix_bio,
                output_dir="results/gene_prediction",
                novel_genes_save_path="results/gene_prediction/bio_novel_genes.txt",
                row_labels_topk=row_labels_bio,
                tag="bio",
                confirmed_gene_path="../acgnn/data/ncg_8886.txt"
            )
            
        elif tag == "topo":
            # 🔵 Apply topo spectral biclustering
            # graph_topo, row_labels, col_labels, total_genes_per_cluster, predicted_counts = apply_full_spectral_biclustering_topo(
            #     graph=graph,
            #     summary_topo_features=relevance_scores_subset,
            #     node_names_topk=node_names_topk,
            #     predicted_cancer_genes=node_names_topk,
            #     n_clusters=n_clusters_row,
            #     save_path=topo_graph_path,
            #     save_row_labels_path=topo_row_labels_path,
            #     save_total_genes_per_cluster_path=topo_total_per_cluster_path,
            #     save_predicted_counts_path=topo_pred_counts_path,
            #     output_path_genes_clusters=topo_output_img,
            #     output_path_heatmap=topo_output_path_heatmap,
            #     topk_node_indices=top_gene_indices
            # )

            # Assign cluster labels
            # graph_topo.ndata[f'cluster_{tag}'] = torch.full((graph_topo.num_nodes(),), -1, dtype=torch.long)
            # graph_topo.ndata[f'cluster_{tag}'][top_gene_indices] = torch.tensor(row_labels, dtype=torch.long)

            # ➕ Plot t-SNE/UMAP
            # G_tmp = graph_topo.to_networkx(node_attrs=[f'cluster_{tag}'])
            # plot_relevance_tsne_umap(
            #     relevance_scores_subset,
            #     G_tmp,
            #     cluster_key=f'cluster_{tag}',
            #     method='umap',
            #     title_suffix="(Topo)"
            # )

            # ➕ Bar plots
            plot_pcg_cancer_genes(
                clusters=range(n_clusters_row),
                predicted_cancer_genes_count=topo_pred_counts,
                total_genes_per_cluster=topo_total_counts,
                node_names=node_names_topk,
                row_labels=row_labels,
                output_path=os.path.join(
                    output_dir,
                    f'{args.model_type}_{args.net_type}_pcg_percent_{tag}_epo{args.num_epochs}.png'
                )
            )

            # ➕ Interaction plots
            # topk_node_indices_tensor = torch.tensor(top_gene_indices, dtype=torch.int64)
            # row_labels_np = np.array(row_labels)
            # degrees_np = graph.ndata['degree'].squeeze().cpu().numpy()

            # kcg_nodes = [i for i, name in enumerate(node_names_topk) if name in ground_truth_cancer_genes]
            # kcg_data = pd.DataFrame({
            #     "Cluster": row_labels_np[kcg_nodes],
            #     "Interactions": degrees_np[kcg_nodes]
            # })

            # pcg_data = pd.DataFrame({
            #     "Cluster": row_labels_np,
            #     "Interactions": degrees_np[top_gene_indices]
            # })

            # plot_interactions_with_kcgs(
            #     kcg_data,
            #     os.path.join(output_dir, f"{args.model_type}_{args.net_type}_kcgs_interaction_{tag}_epo{args.num_epochs}.png")
            # )

            # plot_interactions_with_pcgs(
            #     pcg_data,
            #     os.path.join(output_dir, f"{args.model_type}_{args.net_type}_pcgs_interaction_{tag}_epo{args.num_epochs}.png")
            # )

            # # ➕ KCG percent bar plot
            # gt_indices = set(name_to_index[name] for name in ground_truth_cancer_genes if name in name_to_index)
            # kcg_counts = {
            #     i: sum((np.array(row_labels) == i) & np.isin(range(len(row_labels)), list(gt_indices)))
            #     for i in range(n_clusters_row)
            # }

            # plot_kcg_cancer_genes(
            #     clusters=range(n_clusters_row),
            #     kcg_count=kcg_counts,
            #     total_genes_per_cluster=topo_total_counts,
            #     node_names=node_names_topk,
            #     row_labels=row_labels,
            #     output_path=os.path.join(output_dir, f'{args.model_type}_{args.net_type}_kcg_percent_{tag}_epo{args.num_epochs}.png')
            # )

        
            plot_topo_biclustering_heatmap(
                args=args,
                relevance_scores=relevance_matrix_topo,# relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_topo,#topk.cpu().numpy(),
                gene_names=node_names_topk,
                output_path=heatmap_path,
                col_labels=col_labels_topo
            )

            plot_topo_biclustering_heatmap_unsorted(
                args=args,
                relevance_scores=relevance_matrix_topo,
                row_labels=row_labels_topo,
                gene_names=node_names_topk,
                output_path=heatmap_path_unsort,
                col_labels=col_labels_topo
            )
            
            plot_topo_biclustering_heatmap_clusters_unsort(
                args=args,
                relevance_scores=relevance_matrix_topo,#relevance_scores_topk.detach().cpu().numpy(),
                row_labels=row_labels_topo,#row_labels_topk.cpu().numpy(),
                gene_names=node_names_topk,
                #omics_splits=omics_splits,
                output_path=heatmap_path_clusters_unsort,
                col_labels=col_labels_bio
            )
                        
            save_and_plot_confirmed_genes_topo(
                args=args,
                node_names_topk=node_names_topk,
                node_scores_topk=relevance_matrix_topo,#relevance_scores_topk.detach().cpu().numpy(),
                summary_feature_relevance=relevance_matrix_topo,  # full 1024D, will be reduced inside
                output_dir=output_dir,
                confirmed_genes_save_path=confirmed_file,
                row_labels_topk=row_labels_topo,
                tag="topo",
                confirmed_gene_path="../acgnn/data/ncg_8886.txt"
                ##confirmed_gene_path="data/796_drivers.txt"
            )

            # with open(confirmed_file, "r") as f:
            #     confirmed_genes = [line.strip() for line in f if line.strip()]

            #plot_collapsed_clusterfirst_multilevel_sankey_topo_sorted(
            plot_collapsed_clusterfirst_multilevel_sankey_topo(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                #confirmed_genes=confirmed_genes,
                novel_predicted_genes=novel_predicted_genes,
                scores=scores,
                row_labels=row_labels_topo,
                total_clusters=n_clusters_row,
                relevance_scores=relevance_matrix_topo,
                CLUSTER_COLORS=CLUSTER_COLORS                 
            )
                            
            # ✅ Plot topo neighbor relevance for confirmed genes
            plot_confirmed_neighbors_topo(
                args=args,
                graph=graph,
                node_names=node_names,
                name_to_index=name_to_index,
                predicted_cancer_genes=predicted_cancer_genes,
                # scores=scores,
                row_labels=row_labels_topo,
                total_clusters=n_clusters_row,
                relevance_scores=relevance_matrix_topo
            )
            

    ###########################################################################################################################################
    #
    # pathway enrichment
    # 
    ###########################################################################################################################################
    
    # === Compare BIO and TOPO cluster assignments ===
    print("Comparing BIO vs TOPO cluster assignments on top predicted genes...")

    # Ensure tensors are on CPU
    row_labels_bio_topk = graph.ndata['cluster_bio'][topk_node_indices_tensor].cpu().numpy()
    row_labels_topo_topk = graph.ndata['cluster_topo'][topk_node_indices_tensor].cpu().numpy()

    # Compute ARI and NMI
    ari_score = adjusted_rand_score(row_labels_bio_topk, row_labels_topo_topk)
    nmi_score = normalized_mutual_info_score(row_labels_bio_topk, row_labels_topo_topk)

    print(f"Adjusted Rand Index (ARI): {ari_score:.4f}")
    print(f"Normalized Mutual Information (NMI): {nmi_score:.4f}")

    # === Contingency matrix ===
    # Plot contingency matrix using modular function
    plot_contingency_matrix(
        row_labels_bio_topk,
        row_labels_topo_topk,
        ari_score,
        nmi_score,
        output_dir,
        args
    )
    
    #######################################################################################################################################
    
    # ==== Cluster-wise Functional Enrichment ====

    def get_marker_genes_by_cluster(node_names_topk, row_labels_topk):
        cluster_to_genes = defaultdict(list)
        for name, label in zip(node_names_topk, row_labels_topk):
            cluster_to_genes[label.item()].append(name)
        return cluster_to_genes

    def run_enrichment(cluster_to_genes, organism='hsapiens'):
        gp = GProfiler(return_dataframe=True)
        enrichment_results = {}
        for cluster_id, genes in cluster_to_genes.items():
            if len(genes) >= 5:  # Skip tiny clusters
                enriched = gp.profile(
                    organism=organism,
                    query=genes,
                    sources=['KEGG', 'REAC', 'HP', 'GO:BP'],
                    user_threshold=0.05
                )
                enrichment_results[cluster_id] = enriched
        return enrichment_results

    def summarize_enrichment(enrichment_dict, label):
        for cluster_id, df in enrichment_dict.items():
            top_terms = df[['name', 'p_value']].sort_values('p_value').head(5)
            print(f"\n{label} Cluster {cluster_id} — Top Enriched Terms:")
            print(top_terms.to_string(index=False))

    # Enrichment for bio clusters
    bio_clusters = get_marker_genes_by_cluster(node_names_topk, row_labels_bio)
    bio_enrichment = run_enrichment(bio_clusters)
    summarize_enrichment(bio_enrichment, label='Bio')

    # Enrichment for topo clusters
    topo_clusters = get_marker_genes_by_cluster(node_names_topk, row_labels_topo)
    topo_enrichment = run_enrichment(topo_clusters)
    summarize_enrichment(topo_enrichment, label='Topo')
    
    ###########################################################################################################    

    # Convert defaultdict to DataFrame
    bio_clusters_df = pd.DataFrame(
        [(cluster, gene) for cluster, genes in bio_clusters.items() for gene in genes],
        columns=["bio_cluster", "gene"]
    )

    topo_clusters_df = pd.DataFrame(
        [(cluster, gene) for cluster, genes in topo_clusters.items() for gene in genes],
        columns=["topo_cluster", "gene"]
    )

    # Then build cluster_gene_map
    cluster_gene_map = {
        'bio': bio_clusters_df.groupby('bio_cluster')['gene'].apply(list).to_dict(),
        'topo': topo_clusters_df.groupby('topo_cluster')['gene'].apply(list).to_dict()
    }

    # Initialize g:Profiler client
    gp = GProfiler(return_dataframe=True)

    # Setup: assuming enrichment_results = {}
    enrichment_results = {}

    for cluster_type, cluster_map in cluster_gene_map.items():
        enrichment_results[cluster_type] = {}
        for cluster_id, genes in cluster_map.items():
            # Skip empty clusters
            if not genes:
                continue
            
            # Perform real enrichment query
            try:
                result_df = gp.profile(
                    organism='hsapiens',
                    query=genes,
                    #sources=['GO:BP', 'REAC', 'KEGG', 'HP'],
                    sources=['KEGG', 'GO:BP', 'REAC', 'HP'],
                    user_threshold=0.05,
                    significance_threshold_method="fdr"
                )

                # Filter and store only significant results
                sig_results = result_df[result_df['p_value'] < 0.05]

                enrichment_results[cluster_type][cluster_id] = sig_results

            except Exception as e:
                print(f"Enrichment failed for {cluster_type.capitalize()} cluster {cluster_id}: {e}")
                enrichment_results[cluster_type][cluster_id] = pd.DataFrame()

    plot_enriched_term_counts(
        enrichment_results=enrichment_results,
        output_path=output_dir,
        model_type=args.model_type,
        net_type=args.net_type,
        num_epochs=args.num_epochs,
        #bio_color='#1f77b4', 
        #topo_color='#ff7f0e'
        #bio_color = '#0077B6',
        #topo_color = '#F15BB5'
        #bio_color = '#F08080',
        #topo_color = '#006400'
        ##topo_color = '#90EE90'
    )
    
    plot_shared_enriched_pathways_venn(
        enrichment_results=enrichment_results,
        output_path=output_dir,
        model_type=args.model_type,
        net_type=args.net_type,
        num_epochs=args.num_epochs
    )

    ########################################################################################################################################    

    #save_and_plot_enriched_pathways(enrichment_results, args, output_dir)
    heatmap_df, topo_terms_df, bio_terms_df = save_and_plot_enriched_pathways(enrichment_results, args, output_dir)

    ########################################################################################################################################    

    '''top_node_embeddings_bio = bio_feats[top_gene_indices].cpu().numpy()

    # Assuming embedding is of shape [N, 1024] or similar
    feature_dim_bio = top_node_embeddings_bio.shape[1]
    top_node_feature_names_bio = [f'feat_{i}' for i in range(feature_dim_bio)]

    relevance_df_bio = pd.DataFrame(
        #relevance_scores_topk.numpy(),
        relevance_matrix_bio,
        index=node_names_topk,                   # list of gene names
        columns=top_node_feature_names_bio           # list of string feature names
    )

    top_node_embeddings_topo = topo_feats[top_gene_indices].cpu().numpy()

    # Assuming embedding is of shape [N, 1024] or similar
    feature_dim_topo = top_node_embeddings_topo.shape[1]
    top_node_feature_names_topo = [f'feat_{i}' for i in range(feature_dim_topo)]

    relevance_df_topo = pd.DataFrame(
        #relevance_scores_topk.numpy(),
        relevance_matrix_topo,
        index=node_names_topk,                   # list of gene names
        columns=top_node_feature_names_topo           # list of string feature names
    )
    

    def cluster_dict_to_df(cluster_dict, cluster_label_name):
        rows = []
        for cluster_id, genes in cluster_dict.items():
            rows.extend([(gene, cluster_id) for gene in genes])
        return pd.DataFrame(rows, columns=['gene', cluster_label_name])

    bio_clusters = get_marker_genes_by_cluster(node_names_topk, row_labels_bio)
    topo_clusters = get_marker_genes_by_cluster(node_names_topk, row_labels_topo)

    bio_clusters_df = cluster_dict_to_df(bio_clusters, 'bio_cluster')
    topo_clusters_df = cluster_dict_to_df(topo_clusters, 'topo_cluster')
    
    # Create heatmaps per cluster set

    visualize_feature_relevance_heatmaps(relevance_df_bio, bio_clusters_df, "results/bio_heatmaps")
    visualize_feature_relevance_heatmaps(relevance_df_topo, topo_clusters_df, "results/topo_heatmaps")'''

    print("✅ Clustered feature relevance heatmaps saved in 'results/bio_heatmaps' and 'results/topo_heatmaps'")

    ###########################################################################################################################################
    #
    # neighbor relevance
    # 
    ###########################################################################################################################################
    
    # with open(confirmed_file, "r") as f:
    #     confirmed_genes = [line.strip() for line in f if line.strip()]

    node_id_to_name = {i: name for i, name in enumerate(node_names)}
    neighbors_dict = get_neighbors_gene_names(graph, node_names, name_to_index, confirmed_genes)

    for gene in confirmed_genes:
        plot_neighbor_relevance_by_mode(
            gene=gene,
            relevance_scores=relevance_scores_bio,
            mode='BIO',
            neighbor_scores=scores,
            neighbors_dict=neighbors_dict,
            name_to_index=name_to_index,
            node_id_to_name=node_id_to_name,
            graph=graph,
            row_labels=graph.ndata["cluster_bio"], 
            total_clusters=n_clusters_row,            
            args=args
        )

        plot_neighbor_relevance_by_mode(
            gene=gene,
            relevance_scores=relevance_scores_topo,
            mode='TOPO',
            neighbor_scores=scores,
            neighbors_dict=neighbors_dict,
            name_to_index=name_to_index,
            node_id_to_name=node_id_to_name,
            graph=graph,
            row_labels=graph.ndata["cluster_topo"], 
            total_clusters=n_clusters_row,   
            args=args
        )

    ###########################################################################################################################################
    #
    # feature importance
    # 
    ###########################################################################################################################################
    
    gene_indices = [name_to_index[gene] for gene in confirmed_genes if gene in name_to_index]

    # Compute relevance scores using saliency (Integrated Gradients)
    relevance_scores = compute_relevance_scores_norm(
        model=model,
        graph=graph,
        features=features,
        node_indices=gene_indices,
        normalize=True,
        feature_groups=feature_groups
    )
            
    '''save_dir = os.path.join(output_dir, "gnnexplainer/")
    os.makedirs(save_dir, exist_ok=True)

    for gene in confirmed_genes:
        if gene not in name_to_index:
            print(f"⚠️ Gene {gene} not found in the graph.")
            continue

        node_idx = name_to_index[gene]

        if node_idx not in relevance_scores:
            print(f"⚠️ No relevance found for {gene} (node {node_idx})")
            continue

        if not isinstance(relevance_scores[node_idx], dict) or \
        "bio" not in relevance_scores[node_idx] or "topo" not in relevance_scores[node_idx]:
            print(f"⚠️ Relevance score for {gene} (node /{node_idx}) is not in expected format.")
            continue

        # Get relevance vectors and reduce
        node_idx = name_to_index[gene]
        gene_score = scores[node_idx]
        print(f"{gene} → Node {node_idx} | Predicted score = {gene_score:.4f}")

        neighbors = neighbors_dict.get(gene, [])
        neighbor_indices = [name_to_index[n] for n in neighbors if n in name_to_index]

        if node_idx not in relevance_scores:
            print(f"⚠️ No relevance found for {gene} (node {node_idx})")
            continue

        plot_saliency_for_gene(
            gene=gene,
            relevance_scores=relevance_scores,
            node_idx=node_idx,
            save_dir=save_dir,
            args=args,
            bio_feat_names=bio_feat_names,
            topo_feat_names=topo_feat_names
        )'''

    ###########################################################################################################################################
    #
    # Bio vs Topo contribution comaprison
    # 
    ###########################################################################################################################################
    
    # This is the correct dictionary for group-wise aggregation
    '''groupwise_saliency = defaultdict(list)

    for node_id, group_saliency in relevance_scores.items():
        for group_name, saliency_tensor in group_saliency.items():
            groupwise_saliency[group_name].append(saliency_tensor.cpu().numpy())

    for group_name in groupwise_saliency:
        groupwise_saliency[group_name] = np.stack(groupwise_saliency[group_name], axis=0)  # shape: [N, F]'''

    bio_scores = relevance_matrix_bio#groupwise_saliency["bio"]
    topo_scores = relevance_matrix_topo#groupwise_saliency["topo"]
    save_dir = os.path.join(output_dir, "gnnexplainer/")
    os.makedirs(save_dir, exist_ok=True)

    if len(bio_scores) > 0 and len(topo_scores) > 0:
        bio_means = np.mean(bio_scores, axis=0)
        topo_means = np.mean(topo_scores, axis=0)

        plot_bio_topo_saliency(
            bio_means,
            topo_means,
            title="Average Feature Relevance for Confirmed Genes (Bio vs Topo)",
            save_path=os.path.join(save_dir, f"{args.model_type}_{args.net_type}_bio_topo_comparison_lineplot_square_{args.num_epochs}.png")
        )

        plot_bio_topo_saliency_cuberoot(
            bio_means,
            topo_means,
            title="Average Feature Relevance for Confirmed Genes (Bio vs Topo)",
            save_path=os.path.join(save_dir, f"{args.model_type}_{args.net_type}_bio_topo_comparison_cuberoot_{args.num_epochs}.png")
        )
